{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LabChain","text":""},{"location":"#a-modular-and-extensible-labchain-for-ml-experimentation","title":"A Modular and Extensible LabChain for ML Experimentation","text":"<p>LabChain is a lightweight, flexible foundation for building machine learning experimentation systems that are structured, extensible, and reusable. Unlike rigid monolithic platforms, LabChain lets you craft your own workflows by composing modular components or reusing existing ones.</p> <p>Its architecture is grounded in principles of modularity, composability, and transparency, enabling you to tailor each part of your workflow to fit your project's needs.</p> <p>Warning</p> <p>Framework3 is under active development. While we strive for stability, caution is advised when using it in production environments.</p>"},{"location":"#what-is-labchain","title":"What is LabChain?","text":"<p>LabChain is not a closed pipeline, nor a library of pre-trained models. It\u2019s a collection of base interfaces and pluggable components that empower you to:</p> <ul> <li>Design and combine filters, metrics, optimizers, and storers.</li> <li>Build reusable pipelines \u2014 sequential or parallel.</li> <li>Execute optimization or evaluation processes, even in distributed settings.</li> <li>Structure your experiments with clarity and complete control.</li> </ul> <p>Think of LabChain as a toolbox of interoperable building blocks \u2014 not a black-box solution.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83e\udde9 Modular architecture: easily combine and reuse components.</li> <li>\ud83d\udd0c Extensible plugin system: add new filters, metrics, or storers without modifying the core.</li> <li>\ud83e\uddea Composable pipelines: design sequential, parallel, or hybrid flows.</li> <li>\ud83e\udde0 Built-in optimization and evaluation: test different configurations and track performance.</li> <li>\ud83e\uddf5 Distributed processing: supports parallel execution and MapReduce-style pipelines.</li> </ul>"},{"location":"#design-philosophy","title":"Design Philosophy","text":"<ol> <li>Composition over configuration: each piece is designed to work seamlessly with others.</li> <li>Frictionless extensibility: implement your own components by subclassing base interfaces.</li> <li>Full transparency: inspect every step, metric, and decision in the workflow.</li> </ol>"},{"location":"#conceptual-diagram","title":"Conceptual Diagram","text":"<pre><code>graph TD\n    subgraph Core\n        A[BaseFilter]\n        B[BasePipeline]\n        C[BaseOptimizer]\n        D[BaseSplitter]\n        E[BaseMetric]\n        F[BaseStorer]\n        A --&gt; B\n        B --&gt; C\n        B --&gt; D\n        B --&gt; E\n        B --&gt; F\n    end\n\n    subgraph Plugins\n        G[F3Pipeline] --&gt; B\n        H[KFoldSplitter] --&gt; D\n        I[OptunaOptimizer] --&gt; C\n        J[LocalStorer] --&gt; F\n        K[KnnFilter] --&gt; A\n        L[F1Metric] --&gt; E\n        M[ParallelPipeline] --&gt; B\n        N[RemoteStorer] --&gt; F\n    end</code></pre>"},{"location":"#repository-structure","title":"Repository Structure","text":"<p>LabChain is structured to be easy to navigate, extend, and maintain:</p> <ul> <li><code>base/</code>: Fundamental interfaces and abstractions.</li> <li><code>plugins/</code>: Filters, pipelines, metrics, optimizers, storers, and more.</li> <li><code>container/</code>: Dependency injection system.</li> <li><code>examples/</code>: Real-world use cases and tutorials.</li> <li><code>tests/</code>: Unit and integration tests.</li> <li><code>docs/</code>: Full documentation, including this page.</li> </ul>"},{"location":"#ready-to-get-started","title":"\ud83d\ude80 Ready to Get Started?","text":"<ul> <li>\ud83d\udcda Quickstart Guide: quick_start/index.md</li> <li>\ud83e\uddec Full API Reference: api/index.md</li> <li>\ud83d\udd0d Examples and Recipes: examples/</li> </ul>"},{"location":"#contribute","title":"\ud83e\udd1d Contribute","text":"<p>Interested in contributing? Check out the Contribution Guidelines.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>LabChain is licensed under AGPL-3.0 \u2014 View License</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [manuel.couto.pintos@usc.es]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/\u00bc/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Framework3","text":"<p>Thank you for your interest in contributing to Framework3! This guide outlines the process for contributing to our project.</p>"},{"location":"CONTRIBUTING/#quick-start","title":"Quick Start","text":"<ol> <li>Fork and clone the repository</li> <li>Set up the development environment (Installation Guide)</li> <li>Create a branch for your changes</li> <li>Make your changes, following our guidelines</li> <li>Submit a pull request</li> </ol>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and adhere to our Code of Conduct. Report any unacceptable behavior to [manuel.couto.pintos@usc.es].</p>"},{"location":"CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":""},{"location":"CONTRIBUTING/#reporting-bugs-and-suggesting-enhancements","title":"Reporting Bugs and Suggesting Enhancements","text":"<p>Use the issue tracker to report bugs or suggest enhancements. Provide detailed descriptions and, if possible, steps to reproduce or examples.</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<ol> <li>Ensure all dependencies are properly managed</li> <li>Update documentation as necessary</li> <li>Include comprehensive tests</li> <li>Obtain sign-off from two developers before merging</li> </ol>"},{"location":"CONTRIBUTING/#development-guidelines","title":"Development Guidelines","text":""},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use Black, isort, and flake8 for formatting and linting</li> <li>Write clear, concise docstrings in Google style</li> </ul>"},{"location":"CONTRIBUTING/#git-workflow","title":"Git Workflow","text":"<ul> <li>We use a modified GitFlow:<ul> <li><code>main</code>: latest stable release</li> <li><code>develop</code>: integration branch</li> <li>Feature branches: <code>feature/description</code></li> <li>Hotfix branches: <code>hotfix/description</code></li> <li>Release branches: <code>release/vX.Y.Z</code></li> </ul> </li> </ul> <p>Commit messages should be clear, use present tense and imperative mood.</p>"},{"location":"CONTRIBUTING/#solid-principles","title":"SOLID Principles","text":"<ul> <li>Adhere to SOLID principles:<ol> <li>Single Responsibility</li> <li>Open/Closed</li> <li>Liskov Substitution</li> <li>Interface Segregation</li> <li>Dependency Inversion</li> </ol> </li> </ul>"},{"location":"CONTRIBUTING/#cicd","title":"CI/CD","text":"<p>Our GitHub Actions pipeline includes linting, testing, documentation building, and deployment. Ensure your changes pass all CI checks.</p>"},{"location":"CONTRIBUTING/#documentation-and-testing","title":"Documentation and Testing","text":"<ul> <li>Write docstrings for all public code elements</li> <li>Update API documentation and tutorials as needed</li> <li>Aim for 80% code coverage</li> <li>Include unit and integration tests</li> </ul>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Push to your fork</li> <li>Create a pull request to the <code>develop</code> branch</li> <li>Describe the changes and reference relevant issues</li> <li>Ensure all checks pass and obtain two reviews</li> </ol> <p>Thank you for contributing to Framework3!</p>"},{"location":"api/","title":"LabChain API Documentation","text":"<p>Welcome to the API documentation for LabChain. This comprehensive guide details the modules, classes, and functions that form the backbone of LabChain, enabling you to build, extend, and customize ML experimentation workflows efficiently.</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Base Classes</li> <li>Container &amp; Dependency Injection</li> <li>Plugins</li> <li>Pipelines</li> <li>Filters</li> <li>Metrics</li> <li>Optimizers</li> <li>Splitters</li> <li>Storage</li> <li>Utilities</li> <li>Using the API</li> </ul>"},{"location":"api/#base-classes","title":"Base Classes","text":"<p>The foundation of LabChain is built on these abstract base classes:</p> <ul> <li>Types - Core data structures and type definitions.</li> <li>Classes - Abstract base class for all components.</li> <li>Pipeline - Base class for creating pipelines.</li> <li>Filter - Abstract class for all filter implementations.</li> <li>Metric - Base class for metric implementations.</li> <li>Optimizer - Abstract base for optimization algorithms.</li> <li>Splitter - Base class for data splitting strategies.</li> <li>Factory - Factory classes for component creation.</li> <li>Storage - Abstract base for storage implementations.</li> </ul>"},{"location":"api/#container-dependency-injection","title":"Container &amp; Dependency Injection","text":"<p>The core of LabChain's component management:</p> <ul> <li>Container - Main class for dependency injection and component management.</li> <li>Overload - Utilities for method overloading in the container.</li> </ul>"},{"location":"api/#plugins","title":"Plugins","text":""},{"location":"api/#pipelines","title":"Pipelines","text":"<p>Pipelines orchestrate the data flow through various processing steps:</p> <ul> <li>Parallel Pipelines</li> <li>MonoPipeline - For parallel processing of independent tasks.</li> <li>HPCPipeline - Optimized for high-performance computing environments.</li> <li>Sequential Pipeline</li> <li>F3Pipeline - The basic sequential pipeline.</li> </ul>"},{"location":"api/#filters","title":"Filters","text":"<p>Modular processing units that can be composed together within pipelines:</p> <ul> <li>Classification Filters</li> <li>Clustering Filters</li> <li>Regression Filters</li> <li>Transformation Filters</li> <li>Text Processing Filters</li> <li>Cache Filters</li> <li>CachedFilter</li> <li>Grid Search Filters</li> <li>GridSearchCVFilter</li> </ul>"},{"location":"api/#metrics","title":"Metrics","text":"<p>Metrics evaluate model performance across various tasks:</p> <ul> <li>Classification Metrics</li> <li>Clustering Metrics</li> <li>Coherence Metrics</li> </ul>"},{"location":"api/#optimizers","title":"Optimizers","text":"<p>Optimizers help fine-tune hyperparameters for optimal performance:</p> <ul> <li>SklearnOptimizer</li> <li>OptunaOptimizer</li> <li>WandbOptimizer</li> <li>GridOptimizer</li> </ul>"},{"location":"api/#splitters","title":"Splitters","text":"<p>Splitters divide the dataset for cross-validation and evaluation:</p> <ul> <li>KFoldSplitter</li> <li>KFoldSplitter</li> </ul>"},{"location":"api/#storage","title":"Storage","text":"<p>Storage plugins for data persistence:</p> <ul> <li>Local Storage</li> <li>S3 Storage</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<p>Additional utility functions and helpers that support the framework:</p> <ul> <li>PySpark Utilities</li> <li>Weights &amp; Biases Integration</li> <li>Typeguard for Notebooks</li> <li>Scikit-learn Estimator Utilities</li> <li>General Utilities</li> </ul>"},{"location":"api/#using-the-api","title":"Using the API","text":"<p>To utilize any component of LabChain, import it from the respective module and register it with the Container if necessary. For example:</p> <pre><code>from framework3.container import Container\nfrom framework3.base import BaseFilter, BasePipeline, BaseMetric\n\n@Container.bind()\nclass MyFilter(BaseFilter):\n    # Custom filter implementation\n\n@Container.bind()\nclass MyPipeline(BasePipeline):\n    # Custom pipeline implementation\n\n@Container.bind()\nclass MyMetric(BaseMetric):\n    # Custom metric implementation\n\n# Retrieve components\nmy_filter = Container.ff[\"MyFilter\"]()\nmy_pipeline = Container.pf[\"MyPipeline\"]()\nmy_metric = Container.mf[\"MyMetric\"]()\n</code></pre>"},{"location":"api/base/base_dataset_manager/","title":"Base dataset manager","text":""},{"location":"api/base/base_dataset_manager/#framework3.base.base_dataset_manager","title":"<code>framework3.base.base_dataset_manager</code>","text":""},{"location":"api/base/base_dataset_manager/#framework3.base.base_dataset_manager.BaseDatasetManager","title":"<code>BaseDatasetManager</code>","text":"<p>               Bases: <code>BasePlugin</code>, <code>ABC</code></p> Source code in <code>framework3/base/base_dataset_manager.py</code> <pre><code>class BaseDatasetManager(BasePlugin, ABC):\n    @abstractmethod\n    def list(self) -&gt; List[str]:\n        \"\"\"\n        List all available datasets.\n\n        Returns:\n            List[str]: A list of dataset names.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def save(self, name: str, data: VData) -&gt; None:\n        \"\"\"\n        Save a dataset.\n\n        Args:\n            name (str): Unique name for the dataset.\n            data (XYData): The data to be saved.\n\n        Raises:\n            ValueError: If a dataset with the given name already exists.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def update(self, name: str, data: VData) -&gt; None:\n        \"\"\"\n        Update a dataset.\n\n        Args:\n            name (str): Unique name for the dataset.\n            data (XYData): The data to be saved.\n\n        Raises:\n            ValueError: If a dataset with the given name doesn't exists.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def load(self, name: str) -&gt; XYData:\n        \"\"\"\n        Load a dataset.\n\n        Args:\n            name (str): Name of the dataset to load.\n\n        Returns:\n            XYData: The loaded dataset.\n\n        Raises:\n            ValueError: If the dataset does not exist.\n        \"\"\"\n        ...\n\n    def delete(self, name: str) -&gt; None:\n        \"\"\"\n        Delete a dataset.\n\n        Args:\n            name (str): Name of the dataset to delete.\n\n        Raises:\n            ValueError: If the dataset does not exist.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/base/base_dataset_manager/#framework3.base.base_dataset_manager.BaseDatasetManager.delete","title":"<code>delete(name)</code>","text":"<p>Delete a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset to delete.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataset does not exist.</p> Source code in <code>framework3/base/base_dataset_manager.py</code> <pre><code>def delete(self, name: str) -&gt; None:\n    \"\"\"\n    Delete a dataset.\n\n    Args:\n        name (str): Name of the dataset to delete.\n\n    Raises:\n        ValueError: If the dataset does not exist.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_dataset_manager/#framework3.base.base_dataset_manager.BaseDatasetManager.list","title":"<code>list()</code>  <code>abstractmethod</code>","text":"<p>List all available datasets.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of dataset names.</p> Source code in <code>framework3/base/base_dataset_manager.py</code> <pre><code>@abstractmethod\ndef list(self) -&gt; List[str]:\n    \"\"\"\n    List all available datasets.\n\n    Returns:\n        List[str]: A list of dataset names.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_dataset_manager/#framework3.base.base_dataset_manager.BaseDatasetManager.load","title":"<code>load(name)</code>  <code>abstractmethod</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset to load.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The loaded dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataset does not exist.</p> Source code in <code>framework3/base/base_dataset_manager.py</code> <pre><code>@abstractmethod\ndef load(self, name: str) -&gt; XYData:\n    \"\"\"\n    Load a dataset.\n\n    Args:\n        name (str): Name of the dataset to load.\n\n    Returns:\n        XYData: The loaded dataset.\n\n    Raises:\n        ValueError: If the dataset does not exist.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_dataset_manager/#framework3.base.base_dataset_manager.BaseDatasetManager.save","title":"<code>save(name, data)</code>  <code>abstractmethod</code>","text":"<p>Save a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the dataset.</p> required <code>data</code> <code>XYData</code> <p>The data to be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a dataset with the given name already exists.</p> Source code in <code>framework3/base/base_dataset_manager.py</code> <pre><code>@abstractmethod\ndef save(self, name: str, data: VData) -&gt; None:\n    \"\"\"\n    Save a dataset.\n\n    Args:\n        name (str): Unique name for the dataset.\n        data (XYData): The data to be saved.\n\n    Raises:\n        ValueError: If a dataset with the given name already exists.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_dataset_manager/#framework3.base.base_dataset_manager.BaseDatasetManager.update","title":"<code>update(name, data)</code>  <code>abstractmethod</code>","text":"<p>Update a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the dataset.</p> required <code>data</code> <code>XYData</code> <p>The data to be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a dataset with the given name doesn't exists.</p> Source code in <code>framework3/base/base_dataset_manager.py</code> <pre><code>@abstractmethod\ndef update(self, name: str, data: VData) -&gt; None:\n    \"\"\"\n    Update a dataset.\n\n    Args:\n        name (str): Unique name for the dataset.\n        data (XYData): The data to be saved.\n\n    Raises:\n        ValueError: If a dataset with the given name doesn't exists.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_factory/","title":"Factory","text":""},{"location":"api/base/base_factory/#framework3.base.base_factory","title":"<code>framework3.base.base_factory</code>","text":""},{"location":"api/base/base_factory/#framework3.base.base_factory.__all__","title":"<code>__all__ = ['BaseFactory']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory","title":"<code>BaseFactory</code>","text":"<p>               Bases: <code>Generic[TypePlugable]</code></p> <p>A generic factory class for managing and creating pluggable components.</p> <p>This class provides a flexible way to register, retrieve, and manage different types of components (plugins) in the framework.</p> Key Features <ul> <li>Dynamic registration and retrieval of components</li> <li>Support for attribute-style and dictionary-style access</li> <li>Iteration over registered components</li> <li>Rich printing of available components</li> </ul> Usage <p>To create a new factory for a specific type of component, inherit from this class and specify the type of components it will manage. For example:</p> <pre><code>from framework3.base.base_factory import BaseFactory\nfrom framework3.base.base_plugin import BasePlugin\n\nclass MyComponentFactory(BaseFactory[BasePlugin]):\n    pass\n\nfactory = MyComponentFactory()\nfactory['ComponentA'] = ComponentA\nfactory['ComponentB'] = ComponentB\n\ncomponent_a = factory['ComponentA']()\ncomponent_b = factory['ComponentB']()\n</code></pre> <p>Attributes:</p> Name Type Description <code>_foundry</code> <code>Dict[str, Type[TypePlugable]]</code> <p>Internal dictionary to store registered components.</p> <p>Methods:</p> Name Description <code>__getattr__</code> <p>str) -&gt; Type[TypePlugable]: Retrieve a component by attribute access.</p> <code>__setattr__</code> <p>str, value: Type[TypePlugable]) -&gt; None: Set a component by attribute assignment.</p> <code>__setitem__</code> <p>str, value: Type[TypePlugable]) -&gt; None: Set a component using dictionary-like syntax.</p> <code>__getitem__</code> <p>str, default: Type[TypePlugable] | None = None) -&gt; Type[TypePlugable]: Retrieve a component using dictionary-like syntax.</p> <code>__iter__</code> <p>Provide an iterator over the registered components.</p> <code>__contains__</code> <p>str) -&gt; bool: Check if a component is registered in the factory.</p> <code>get</code> <p>str, default: Type[TypePlugable] | None = None) -&gt; Type[TypePlugable]: Retrieve a component by name.</p> <code>print_available_components</code> <p>Print a list of all available components in the factory.</p> Note <p>This class uses Generic[TypePlugable] to allow type hinting for the specific type of components managed by the factory.</p> Source code in <code>framework3/base/base_factory.py</code> <pre><code>class BaseFactory(Generic[TypePlugable]):\n    \"\"\"\n    A generic factory class for managing and creating pluggable components.\n\n    This class provides a flexible way to register, retrieve, and manage\n    different types of components (plugins) in the framework.\n\n    Key Features:\n        - Dynamic registration and retrieval of components\n        - Support for attribute-style and dictionary-style access\n        - Iteration over registered components\n        - Rich printing of available components\n\n    Usage:\n        To create a new factory for a specific type of component, inherit from this class\n        and specify the type of components it will manage. For example:\n\n        ```python\n        from framework3.base.base_factory import BaseFactory\n        from framework3.base.base_plugin import BasePlugin\n\n        class MyComponentFactory(BaseFactory[BasePlugin]):\n            pass\n\n        factory = MyComponentFactory()\n        factory['ComponentA'] = ComponentA\n        factory['ComponentB'] = ComponentB\n\n        component_a = factory['ComponentA']()\n        component_b = factory['ComponentB']()\n        ```\n\n    Attributes:\n        _foundry (Dict[str, Type[TypePlugable]]): Internal dictionary to store registered components.\n\n    Methods:\n        __getattr__(name: str) -&gt; Type[TypePlugable]:\n            Retrieve a component by attribute access.\n\n        __setattr__(name: str, value: Type[TypePlugable]) -&gt; None:\n            Set a component by attribute assignment.\n\n        __setitem__(name: str, value: Type[TypePlugable]) -&gt; None:\n            Set a component using dictionary-like syntax.\n\n        __getitem__(name: str, default: Type[TypePlugable] | None = None) -&gt; Type[TypePlugable]:\n            Retrieve a component using dictionary-like syntax.\n\n        __iter__() -&gt; Iterator[Tuple[str, Type[TypePlugable]]]:\n            Provide an iterator over the registered components.\n\n        __contains__(item: str) -&gt; bool:\n            Check if a component is registered in the factory.\n\n        get(name: str, default: Type[TypePlugable] | None = None) -&gt; Type[TypePlugable]:\n            Retrieve a component by name.\n\n        print_available_components() -&gt; None:\n            Print a list of all available components in the factory.\n\n    Note:\n        This class uses Generic[TypePlugable] to allow type hinting for the specific\n        type of components managed by the factory.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the BaseFactory with an empty dictionary to store components.\n        \"\"\"\n        self._foundry: Dict[str, Type[TypePlugable]] = {}\n\n    def __getattr__(self, name: str) -&gt; Type[TypePlugable]:\n        \"\"\"\n        Retrieve a component by attribute access.\n\n        This method allows components to be accessed as if they were attributes\n        of the factory instance.\n\n        Args:\n            name (str): The name of the component to retrieve.\n\n        Returns:\n            Type[TypePlugable]: The requested component class.\n\n        Raises:\n            AttributeError: If the component is not found in the factory.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            component_class = factory.ComponentA\n            ```\n        \"\"\"\n        if name in self._foundry:\n            return self._foundry[name]\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setattr__(self, name: str, value: Type[TypePlugable]) -&gt; None:\n        \"\"\"\n        Set a component by attribute assignment.\n\n        This method allows components to be registered as if they were attributes\n        of the factory instance.\n\n        Args:\n            name (str): The name to assign to the component.\n            value (Type[TypePlugable]): The component class to register.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            factory.ComponentA = ComponentA\n            ```\n        \"\"\"\n        if name == \"_foundry\":\n            super().__setattr__(name, value)\n        else:\n            self._foundry[name] = value\n\n    def __setitem__(self, name: str, value: Type[TypePlugable]) -&gt; None:\n        \"\"\"\n        Set a component using dictionary-like syntax.\n\n        This method allows components to be registered using dictionary-style\n        item assignment.\n\n        Args:\n            name (str): The name to assign to the component.\n            value (Type[TypePlugable]): The component class to register.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            factory['ComponentA'] = ComponentA\n            ```\n        \"\"\"\n        if name == \"_foundry\":\n            super().__setattr__(name, value)\n        else:\n            self._foundry[name] = value\n\n    def __getitem__(\n        self, name: str, default: Type[TypePlugable] | None = None\n    ) -&gt; Type[TypePlugable]:\n        \"\"\"\n        Retrieve a component using dictionary-like syntax.\n\n        This method allows components to be accessed using dictionary-style\n        item retrieval.\n\n        Args:\n            name (str): The name of the component to retrieve.\n            default (Type[TypePlugable] | None, optional): Default value if component is not found.\n\n        Returns:\n            Type[TypePlugable]: The requested component class or the default value.\n\n        Raises:\n            AttributeError: If the component is not found and no default is provided.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            component_class = factory['ComponentA']\n            ```\n        \"\"\"\n        if name in self._foundry:\n            return self._foundry[name]\n        else:\n            if default is None:\n                raise AttributeError(\n                    f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n                )\n            return default\n\n    def __iter__(self) -&gt; Iterator[Tuple[str, Type[TypePlugable]]]:\n        \"\"\"\n        Provide an iterator over the registered components.\n\n        This method allows iteration over the (name, component) pairs in the factory.\n\n        Returns:\n            Iterator[Tuple[str, Type[TypePlugable]]]: An iterator of (name, component) pairs.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            for name, component_class in factory:\n                print(f\"{name}: {component_class}\")\n            ```\n        \"\"\"\n        return iter(self._foundry.items())\n\n    def __contains__(self, item: str) -&gt; bool:\n        \"\"\"\n        Check if a component is registered in the factory.\n\n        This method allows the use of the 'in' operator to check for component existence.\n\n        Args:\n            item (str): The name of the component to check.\n\n        Returns:\n            bool: True if the component is registered, False otherwise.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            if 'ComponentA' in factory:\n                print(\"ComponentA is available\")\n            ```\n        \"\"\"\n        return item in self._foundry\n\n    def get(\n        self, name: str, default: Type[TypePlugable] | None = None\n    ) -&gt; Type[TypePlugable]:\n        \"\"\"\n        Retrieve a component by name.\n\n        This method provides a way to safely retrieve components with an optional default value.\n\n        Args:\n            name (str): The name of the component to retrieve.\n            default (Type[TypePlugable] | None, optional): Default value if component is not found.\n\n        Returns:\n            Type[TypePlugable]: The requested component class or the default value.\n\n        Raises:\n            AttributeError: If the component is not found and no default is provided.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            component_class = factory.get('ComponentA', DefaultComponent)\n            ```\n        \"\"\"\n        if name in self._foundry:\n            return self._foundry[name]\n        else:\n            if default is None:\n                raise AttributeError(\n                    f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n                )\n            return default\n\n    def print_available_components(self):\n        \"\"\"\n        Print a list of all available components in the factory.\n\n        This method uses rich formatting to display the components in a visually appealing way.\n\n        Example:\n            ```python\n            factory = MyComponentFactory()\n            factory.print_available_components()\n            ```\n        \"\"\"\n        rprint(f\"[bold]Available {self.__class__.__name__[:-7]}s:[/bold]\")\n        for name, binding in self._foundry.items():\n            rprint(f\"  - [green]{name}[/green]: {binding}\")\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Check if a component is registered in the factory.</p> <p>This method allows the use of the 'in' operator to check for component existence.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>The name of the component to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the component is registered, False otherwise.</p> Example <pre><code>factory = MyComponentFactory()\nif 'ComponentA' in factory:\n    print(\"ComponentA is available\")\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"\n    Check if a component is registered in the factory.\n\n    This method allows the use of the 'in' operator to check for component existence.\n\n    Args:\n        item (str): The name of the component to check.\n\n    Returns:\n        bool: True if the component is registered, False otherwise.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        if 'ComponentA' in factory:\n            print(\"ComponentA is available\")\n        ```\n    \"\"\"\n    return item in self._foundry\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Retrieve a component by attribute access.</p> <p>This method allows components to be accessed as if they were attributes of the factory instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the component to retrieve.</p> required <p>Returns:</p> Type Description <code>Type[TypePlugable]</code> <p>Type[TypePlugable]: The requested component class.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the component is not found in the factory.</p> Example <pre><code>factory = MyComponentFactory()\ncomponent_class = factory.ComponentA\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Type[TypePlugable]:\n    \"\"\"\n    Retrieve a component by attribute access.\n\n    This method allows components to be accessed as if they were attributes\n    of the factory instance.\n\n    Args:\n        name (str): The name of the component to retrieve.\n\n    Returns:\n        Type[TypePlugable]: The requested component class.\n\n    Raises:\n        AttributeError: If the component is not found in the factory.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        component_class = factory.ComponentA\n        ```\n    \"\"\"\n    if name in self._foundry:\n        return self._foundry[name]\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.__getitem__","title":"<code>__getitem__(name, default=None)</code>","text":"<p>Retrieve a component using dictionary-like syntax.</p> <p>This method allows components to be accessed using dictionary-style item retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the component to retrieve.</p> required <code>default</code> <code>Type[TypePlugable] | None</code> <p>Default value if component is not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[TypePlugable]</code> <p>Type[TypePlugable]: The requested component class or the default value.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the component is not found and no default is provided.</p> Example <pre><code>factory = MyComponentFactory()\ncomponent_class = factory['ComponentA']\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def __getitem__(\n    self, name: str, default: Type[TypePlugable] | None = None\n) -&gt; Type[TypePlugable]:\n    \"\"\"\n    Retrieve a component using dictionary-like syntax.\n\n    This method allows components to be accessed using dictionary-style\n    item retrieval.\n\n    Args:\n        name (str): The name of the component to retrieve.\n        default (Type[TypePlugable] | None, optional): Default value if component is not found.\n\n    Returns:\n        Type[TypePlugable]: The requested component class or the default value.\n\n    Raises:\n        AttributeError: If the component is not found and no default is provided.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        component_class = factory['ComponentA']\n        ```\n    \"\"\"\n    if name in self._foundry:\n        return self._foundry[name]\n    else:\n        if default is None:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n        return default\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the BaseFactory with an empty dictionary to store components.</p> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the BaseFactory with an empty dictionary to store components.\n    \"\"\"\n    self._foundry: Dict[str, Type[TypePlugable]] = {}\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.__iter__","title":"<code>__iter__()</code>","text":"<p>Provide an iterator over the registered components.</p> <p>This method allows iteration over the (name, component) pairs in the factory.</p> <p>Returns:</p> Type Description <code>Iterator[Tuple[str, Type[TypePlugable]]]</code> <p>Iterator[Tuple[str, Type[TypePlugable]]]: An iterator of (name, component) pairs.</p> Example <pre><code>factory = MyComponentFactory()\nfor name, component_class in factory:\n    print(f\"{name}: {component_class}\")\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Tuple[str, Type[TypePlugable]]]:\n    \"\"\"\n    Provide an iterator over the registered components.\n\n    This method allows iteration over the (name, component) pairs in the factory.\n\n    Returns:\n        Iterator[Tuple[str, Type[TypePlugable]]]: An iterator of (name, component) pairs.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        for name, component_class in factory:\n            print(f\"{name}: {component_class}\")\n        ```\n    \"\"\"\n    return iter(self._foundry.items())\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Set a component by attribute assignment.</p> <p>This method allows components to be registered as if they were attributes of the factory instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to assign to the component.</p> required <code>value</code> <code>Type[TypePlugable]</code> <p>The component class to register.</p> required Example <pre><code>factory = MyComponentFactory()\nfactory.ComponentA = ComponentA\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def __setattr__(self, name: str, value: Type[TypePlugable]) -&gt; None:\n    \"\"\"\n    Set a component by attribute assignment.\n\n    This method allows components to be registered as if they were attributes\n    of the factory instance.\n\n    Args:\n        name (str): The name to assign to the component.\n        value (Type[TypePlugable]): The component class to register.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        factory.ComponentA = ComponentA\n        ```\n    \"\"\"\n    if name == \"_foundry\":\n        super().__setattr__(name, value)\n    else:\n        self._foundry[name] = value\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.__setitem__","title":"<code>__setitem__(name, value)</code>","text":"<p>Set a component using dictionary-like syntax.</p> <p>This method allows components to be registered using dictionary-style item assignment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to assign to the component.</p> required <code>value</code> <code>Type[TypePlugable]</code> <p>The component class to register.</p> required Example <pre><code>factory = MyComponentFactory()\nfactory['ComponentA'] = ComponentA\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def __setitem__(self, name: str, value: Type[TypePlugable]) -&gt; None:\n    \"\"\"\n    Set a component using dictionary-like syntax.\n\n    This method allows components to be registered using dictionary-style\n    item assignment.\n\n    Args:\n        name (str): The name to assign to the component.\n        value (Type[TypePlugable]): The component class to register.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        factory['ComponentA'] = ComponentA\n        ```\n    \"\"\"\n    if name == \"_foundry\":\n        super().__setattr__(name, value)\n    else:\n        self._foundry[name] = value\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.get","title":"<code>get(name, default=None)</code>","text":"<p>Retrieve a component by name.</p> <p>This method provides a way to safely retrieve components with an optional default value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the component to retrieve.</p> required <code>default</code> <code>Type[TypePlugable] | None</code> <p>Default value if component is not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[TypePlugable]</code> <p>Type[TypePlugable]: The requested component class or the default value.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the component is not found and no default is provided.</p> Example <pre><code>factory = MyComponentFactory()\ncomponent_class = factory.get('ComponentA', DefaultComponent)\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def get(\n    self, name: str, default: Type[TypePlugable] | None = None\n) -&gt; Type[TypePlugable]:\n    \"\"\"\n    Retrieve a component by name.\n\n    This method provides a way to safely retrieve components with an optional default value.\n\n    Args:\n        name (str): The name of the component to retrieve.\n        default (Type[TypePlugable] | None, optional): Default value if component is not found.\n\n    Returns:\n        Type[TypePlugable]: The requested component class or the default value.\n\n    Raises:\n        AttributeError: If the component is not found and no default is provided.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        component_class = factory.get('ComponentA', DefaultComponent)\n        ```\n    \"\"\"\n    if name in self._foundry:\n        return self._foundry[name]\n    else:\n        if default is None:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n            )\n        return default\n</code></pre>"},{"location":"api/base/base_factory/#framework3.base.base_factory.BaseFactory.print_available_components","title":"<code>print_available_components()</code>","text":"<p>Print a list of all available components in the factory.</p> <p>This method uses rich formatting to display the components in a visually appealing way.</p> Example <pre><code>factory = MyComponentFactory()\nfactory.print_available_components()\n</code></pre> Source code in <code>framework3/base/base_factory.py</code> <pre><code>def print_available_components(self):\n    \"\"\"\n    Print a list of all available components in the factory.\n\n    This method uses rich formatting to display the components in a visually appealing way.\n\n    Example:\n        ```python\n        factory = MyComponentFactory()\n        factory.print_available_components()\n        ```\n    \"\"\"\n    rprint(f\"[bold]Available {self.__class__.__name__[:-7]}s:[/bold]\")\n    for name, binding in self._foundry.items():\n        rprint(f\"  - [green]{name}[/green]: {binding}\")\n</code></pre>"},{"location":"api/base/base_filter/","title":"Filter","text":""},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter","title":"<code>framework3.base.base_clases.BaseFilter</code>","text":"<p>               Bases: <code>BasePlugin</code></p> <p>Base class for filter components in the framework.</p> <p>This abstract class extends BasePlugin and provides a structure for implementing filter operations, including fit and predict methods. It serves as the foundation for all filter types in the framework, ensuring consistent behavior and interfaces for machine learning operations.</p> Key Features <ul> <li>Implements fit and predict methods for machine learning operations</li> <li>Provides caching mechanisms for model and data storage</li> <li>Supports verbose output for debugging and monitoring</li> <li>Implements equality and hashing methods for filter comparison</li> <li>Supports serialization and deserialization of filter instances</li> </ul> Usage <p>To create a new filter type, inherit from this class and implement the required methods. For example:</p> <pre><code>class MyCustomFilter(BaseFilter):\n    def __init__(self, n_components: int = 2):\n        super().__init__(n_components=n_components)\n        self.model = None\n\n    def fit(self, x: XYData, y: Optional[XYData] = None) -&gt; None:\n        self._print_acction(\"Fitting MyCustomFilter\")\n        # Implement fitting logic here\n        data = x.value\n        self.model = np.linalg.svd(data - np.mean(data, axis=0), full_matrices=False)\n\n    def predict(self, x: XYData) -&gt; XYData:\n        self._print_acction(\"Predicting with MyCustomFilter\")\n        if self.model is None:\n            raise ValueError(\"Model not fitted yet.\")\n        # Implement prediction logic here\n        data = x.value\n        U, s, Vt = self.model\n        transformed = np.dot(data - np.mean(data, axis=0), Vt.T[:, :self.n_components])\n        return XYData(_value=transformed, _hash=x._hash, _path=self._m_path)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_verbose</code> <code>bool</code> <p>Controls the verbosity of output.</p> <code>_m_hash</code> <code>str</code> <p>Hash of the current model.</p> <code>_m_str</code> <code>str</code> <p>String representation of the current model.</p> <code>_m_path</code> <code>str</code> <p>Path to the current model.</p> <code>_original_fit</code> <code>method</code> <p>Reference to the original fit method.</p> <code>_original_predict</code> <code>method</code> <p>Reference to the original predict method.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the filter instance, setting up attributes and method wrappers.</p> <code>fit</code> <p>XYData, y: Optional[XYData]) -&gt; Optional[float]: Fits the filter to the input data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Makes predictions using the fitted filter.</p> <code>verbose</code> <p>bool) -&gt; None: Sets the verbosity level for output.</p> <code>init</code> <p>Initializes filter-specific attributes.</p> <code>_get_model_key</code> <p>str) -&gt; Tuple[str, str]: Generates a unique key for the model.</p> <code>_get_data_key</code> <p>str, data_hash: str) -&gt; Tuple[str, str]: Generates a unique key for the data.</p> <code>grid</code> <p>Dict[str, List[Any] | Tuple[Any, Any]]) -&gt; BaseFilter: Sets up grid search parameters.</p> <code>unwrap</code> <p>Returns the base filter without any wrappers.</p> Note <p>This is an abstract base class. Concrete implementations should override the fit and predict methods to provide specific functionality.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>class BaseFilter(BasePlugin):\n    \"\"\"\n    Base class for filter components in the framework.\n\n    This abstract class extends BasePlugin and provides a structure for implementing\n    filter operations, including fit and predict methods. It serves as the foundation\n    for all filter types in the framework, ensuring consistent behavior and interfaces\n    for machine learning operations.\n\n    Key Features:\n        - Implements fit and predict methods for machine learning operations\n        - Provides caching mechanisms for model and data storage\n        - Supports verbose output for debugging and monitoring\n        - Implements equality and hashing methods for filter comparison\n        - Supports serialization and deserialization of filter instances\n\n    Usage:\n        To create a new filter type, inherit from this class and implement\n        the required methods. For example:\n\n        ```python\n        class MyCustomFilter(BaseFilter):\n            def __init__(self, n_components: int = 2):\n                super().__init__(n_components=n_components)\n                self.model = None\n\n            def fit(self, x: XYData, y: Optional[XYData] = None) -&gt; None:\n                self._print_acction(\"Fitting MyCustomFilter\")\n                # Implement fitting logic here\n                data = x.value\n                self.model = np.linalg.svd(data - np.mean(data, axis=0), full_matrices=False)\n\n            def predict(self, x: XYData) -&gt; XYData:\n                self._print_acction(\"Predicting with MyCustomFilter\")\n                if self.model is None:\n                    raise ValueError(\"Model not fitted yet.\")\n                # Implement prediction logic here\n                data = x.value\n                U, s, Vt = self.model\n                transformed = np.dot(data - np.mean(data, axis=0), Vt.T[:, :self.n_components])\n                return XYData(_value=transformed, _hash=x._hash, _path=self._m_path)\n        ```\n\n    Attributes:\n        _verbose (bool): Controls the verbosity of output.\n        _m_hash (str): Hash of the current model.\n        _m_str (str): String representation of the current model.\n        _m_path (str): Path to the current model.\n        _original_fit (method): Reference to the original fit method.\n        _original_predict (method): Reference to the original predict method.\n\n    Methods:\n        __init__(verbose=True, *args, **kwargs):\n            Initializes the filter instance, setting up attributes and method wrappers.\n\n        fit(x: XYData, y: Optional[XYData]) -&gt; Optional[float]:\n            Fits the filter to the input data.\n\n        predict(x: XYData) -&gt; XYData:\n            Makes predictions using the fitted filter.\n\n        verbose(value: bool) -&gt; None:\n            Sets the verbosity level for output.\n\n        init() -&gt; None:\n            Initializes filter-specific attributes.\n\n        _get_model_key(data_hash: str) -&gt; Tuple[str, str]:\n            Generates a unique key for the model.\n\n        _get_data_key(model_str: str, data_hash: str) -&gt; Tuple[str, str]:\n            Generates a unique key for the data.\n\n        grid(grid: Dict[str, List[Any] | Tuple[Any, Any]]) -&gt; BaseFilter:\n            Sets up grid search parameters.\n\n        unwrap() -&gt; BaseFilter:\n            Returns the base filter without any wrappers.\n\n    Note:\n        This is an abstract base class. Concrete implementations should override\n        the fit and predict methods to provide specific functionality.\n    \"\"\"\n\n    def _print_acction(self, action: str) -&gt; None:\n        \"\"\"\n        Print an action message with formatting.\n\n        This method is used for verbose output to indicate the current action being performed.\n\n        Args:\n            action (str): The action message to be printed.\n\n        Returns:\n            None\n        \"\"\"\n        s_str = \"_\" * 100\n        s_str += f\"\\n{action}...\\n\"\n        s_str += \"*\" * 100\n\n        if self._verbose:\n            rprint(s_str)\n\n    def verbose(self, value: bool) -&gt; None:\n        \"\"\"\n        Set the verbosity of the filter.\n\n        Args:\n            value (bool): If True, enables verbose output; if False, disables it.\n\n        Returns:\n            None\n        \"\"\"\n        self._verbose = value\n\n    def __init__(self, verbose=True, *args: Any, **kwargs: Any):\n        \"\"\"\n        Initialize the BaseFilter instance.\n\n        This method sets up attributes for storing model-related information and wraps\n        the fit and predict methods with pre-processing steps.\n\n        Args:\n            verbose (bool, optional): If True, enables verbose output. Defaults to True.\n            *args (Any): Variable length argument list.\n            **kwargs (Any): Arbitrary keyword arguments.\n        \"\"\"\n        self._verbose = verbose\n        self._original_fit = self.fit\n        self._original_predict = self.predict\n\n        # Replace fit and predict methods\n        if hasattr(self, \"fit\"):\n            self.__setattr__(\"fit\", self._pre_fit_wrapp)\n        if hasattr(self, \"predict\"):\n            self.__setattr__(\"predict\", self._pre_predict_wrapp)\n\n        super().__init__(*args, **kwargs)\n\n        self._m_hash: str\n        self._m_str: str\n        self._m_path: str\n\n    def init(self):\n        \"\"\"\n        Initialize filter-specific attributes.\n\n        This method sets up the model hash, string representation, and path.\n\n        \"\"\"\n        m_hash, m_str = self._get_model_key(data_hash=\" , \")\n\n        self._m_hash: str = m_hash\n        self._m_str: str = m_str\n        self._m_path: str = f\"{self._get_model_name()}/{m_hash}\"\n\n    def __eq__(self, other: object) -&gt; bool | NotImplementedType:\n        \"\"\"\n        Check equality between this filter and another object.\n\n        Two filters are considered equal if they are of the same type and have the same public attributes.\n\n        Args:\n            other (object): The object to compare with this filter.\n\n        Returns:\n            bool: True if the objects are equal, False otherwise.\n        \"\"\"\n        if not isinstance(other, BaseFilter):\n            return NotImplemented\n        return (\n            type(self) is type(other)\n            and self._public_attributes == other._public_attributes\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"\n        Generate a hash value for this filter.\n\n        The hash is based on the filter's type and its public attributes.\n\n        Returns:\n            int: The hash value of the filter.\n        \"\"\"\n        return hash((type(self), frozenset(self._public_attributes.items())))\n\n    def _pre_fit(self, x: XYData, y: Optional[XYData]) -&gt; Tuple[str, str, str]:\n        \"\"\"\n        Perform pre-processing steps before fitting the model.\n\n        This method generates and sets the model hash, path, and string representation.\n\n        Args:\n            x (XYData): The input data.\n            y (Optional[XYData]): The target data, if applicable.\n\n        Returns:\n            Tuple[str, str, str]: A tuple containing the model hash, path, and string representation.\n        \"\"\"\n        m_hash, m_str = self._get_model_key(\n            data_hash=f'{x._hash}, {y._hash if y is not None else \"\"}'\n        )\n        m_path = f\"{self._get_model_name()}/{m_hash}\"\n\n        self._m_hash = m_hash\n        self._m_path = m_path\n        self._m_str = m_str\n        return m_hash, m_path, m_str\n\n    def _pre_predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Perform pre-processing steps before making predictions.\n\n        This method generates a new XYData object with updated hash and path.\n\n        Args:\n            x (XYData): The input data for prediction.\n\n        Returns:\n            XYData: A new XYData object with updated hash and path.\n\n        Raises:\n            ValueError: If the model has not been trained or loaded.\n        \"\"\"\n        try:\n            d_hash, _ = self._get_data_key(self._m_str, x._hash)\n\n            new_x = XYData(\n                _hash=d_hash,\n                _value=x._value,\n                _path=f\"{self._get_model_name()}/{self._m_hash}\",\n            )\n\n            return new_x\n\n        except Exception:\n            raise ValueError(\"Trainable filter model not trained or loaded\")\n\n    def _pre_fit_wrapp(self, x: XYData, y: Optional[XYData]) -&gt; Optional[float]:\n        \"\"\"\n        Wrapper method for the fit function.\n\n        This method performs pre-processing steps before calling the original fit method.\n\n        Args:\n            x (XYData): The input data.\n            y (Optional[XYData]): The target data, if applicable.\n\n        Returns:\n            Optional[float]: The result of the original fit method.\n        \"\"\"\n        self._pre_fit(x, y)\n        return self._original_fit(x, y)\n\n    def _pre_predict_wrapp(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Wrapper method for the predict function.\n\n        This method performs pre-processing steps before calling the original predict method.\n\n        Args:\n            x (XYData): The input data for prediction.\n\n        Returns:\n            XYData: The prediction results with updated hash and path.\n        \"\"\"\n        new_x = self._pre_predict(x)\n        return XYData(\n            _hash=new_x._hash,\n            _path=new_x._path,\n            _value=self._original_predict(x)._value,\n        )\n\n    def __getstate__(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Prepare the object for pickling.\n\n        This method ensures that the original fit and predict methods are stored for serialization.\n\n        Returns:\n            Dict[str, Any]: The object's state dictionary.\n        \"\"\"\n        state = super().__getstate__()\n        # Ensure we're storing the original methods for serialization\n        state[\"fit\"] = self._original_fit\n        state[\"predict\"] = self._original_predict\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]):\n        \"\"\"\n        Restore the object from its pickled state.\n\n        This method restores the wrapper methods after deserialization.\n\n        Args:\n            state (Dict[str, Any]): The pickled state of the object.\n        \"\"\"\n        super().__setstate__(state)\n        # Restore the wrapper methods after deserialization\n        self.__dict__[\"fit\"] = self._pre_fit_wrapp\n        self.__dict__[\"predict\"] = self._pre_predict_wrapp\n\n    def fit(self, x: XYData, y: Optional[XYData]) -&gt; Optional[float]:\n        \"\"\"\n        Method for fitting the filter to the data.\n\n        This method should be overridden by subclasses to implement specific fitting logic.\n\n        Args:\n            x (XYData): The input data.\n            y (Optional[XYData]): The target data, if applicable.\n\n        Returns:\n            Optional[float]: An optional float value, typically used for metrics or loss.\n\n        Raises:\n            NotTrainableFilterError: If the filter does not support fitting.\n        \"\"\"\n        self.init()\n        raise NotTrainableFilterError(\"This filter does not support fitting.\")\n\n    @abstractmethod\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Abstract method for making predictions using the filter.\n\n        This method must be implemented by subclasses to provide specific prediction logic.\n\n        Args:\n            x (XYData): The input data.\n\n        Returns:\n            XYData: The prediction results.\n        \"\"\"\n        ...\n\n    def _get_model_name(self) -&gt; str:\n        \"\"\"\n        Get the name of the model.\n\n        Returns:\n            str: The name of the model (class name).\n        \"\"\"\n        return self.__class__.__name__\n\n    def _get_model_key(self, data_hash: str) -&gt; Tuple[str, str]:\n        \"\"\"\n        Generate a unique key for the model based on its parameters and input data.\n\n        Args:\n            data_hash (str): A hash representing the input data.\n\n        Returns:\n            Tuple[str, str]: A tuple containing the model hash and a string representation.\n        \"\"\"\n        model_str = f\"&lt;{self.item_dump(exclude=set('extra_params'))}&gt;({data_hash})\"\n        model_hashcode = hashlib.sha1(model_str.encode(\"utf-8\")).hexdigest()\n        return model_hashcode, model_str\n\n    def _get_data_key(self, model_str: str, data_hash: str) -&gt; Tuple[str, str]:\n        \"\"\"\n        Generate a unique key for the data based on the model and input data.\n\n        Args:\n            model_str (str): A string representation of the model.\n            data_hash (str): A hash representing the input data.\n\n        Returns:\n            Tuple[str, str]: A tuple containing the data hash and a string representation.\n        \"\"\"\n        data_str = f\"{model_str}.predict({data_hash})\"\n        data_hashcode = hashlib.sha1(data_str.encode(\"utf-8\")).hexdigest()\n        return data_hashcode, data_str\n\n    def grid(self, grid: Dict[str, List[Any] | Tuple[Any, Any]]) -&gt; BaseFilter:\n        \"\"\"\n        Set up grid search parameters for the filter.\n\n        This method allows defining a grid of hyperparameters for optimization.\n\n        Args:\n            grid (Dict[str, List[Any] | Tuple[Any, Any]]): A dictionary where keys are parameter names\n                and values are lists or tuples of possible values.\n\n        Returns:\n            BaseFilter: The filter instance with grid search parameters set.\n        \"\"\"\n        self._grid = grid\n        return self\n\n    def unwrap(self) -&gt; BaseFilter:\n        \"\"\"\n        Return the base filter without any wrappers.\n\n        This method is useful when you need to access the original filter without any\n        additional layers or modifications added by wrappers.\n\n        Returns:\n            BaseFilter: The unwrapped base filter.\n        \"\"\"\n        return self\n\n    @staticmethod\n    def clear_memory():\n        import gc\n\n        gc.collect()\n        try:\n            import torch\n\n            torch.cuda.empty_cache()\n        except ImportError:\n            pass\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality between this filter and another object.</p> <p>Two filters are considered equal if they are of the same type and have the same public attributes.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>The object to compare with this filter.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool | NotImplementedType</code> <p>True if the objects are equal, False otherwise.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool | NotImplementedType:\n    \"\"\"\n    Check equality between this filter and another object.\n\n    Two filters are considered equal if they are of the same type and have the same public attributes.\n\n    Args:\n        other (object): The object to compare with this filter.\n\n    Returns:\n        bool: True if the objects are equal, False otherwise.\n    \"\"\"\n    if not isinstance(other, BaseFilter):\n        return NotImplemented\n    return (\n        type(self) is type(other)\n        and self._public_attributes == other._public_attributes\n    )\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Prepare the object for pickling.</p> <p>This method ensures that the original fit and predict methods are stored for serialization.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The object's state dictionary.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __getstate__(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Prepare the object for pickling.\n\n    This method ensures that the original fit and predict methods are stored for serialization.\n\n    Returns:\n        Dict[str, Any]: The object's state dictionary.\n    \"\"\"\n    state = super().__getstate__()\n    # Ensure we're storing the original methods for serialization\n    state[\"fit\"] = self._original_fit\n    state[\"predict\"] = self._original_predict\n    return state\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.__hash__","title":"<code>__hash__()</code>","text":"<p>Generate a hash value for this filter.</p> <p>The hash is based on the filter's type and its public attributes.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The hash value of the filter.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"\n    Generate a hash value for this filter.\n\n    The hash is based on the filter's type and its public attributes.\n\n    Returns:\n        int: The hash value of the filter.\n    \"\"\"\n    return hash((type(self), frozenset(self._public_attributes.items())))\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.__init__","title":"<code>__init__(verbose=True, *args, **kwargs)</code>","text":"<p>Initialize the BaseFilter instance.</p> <p>This method sets up attributes for storing model-related information and wraps the fit and predict methods with pre-processing steps.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, enables verbose output. Defaults to True.</p> <code>True</code> <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __init__(self, verbose=True, *args: Any, **kwargs: Any):\n    \"\"\"\n    Initialize the BaseFilter instance.\n\n    This method sets up attributes for storing model-related information and wraps\n    the fit and predict methods with pre-processing steps.\n\n    Args:\n        verbose (bool, optional): If True, enables verbose output. Defaults to True.\n        *args (Any): Variable length argument list.\n        **kwargs (Any): Arbitrary keyword arguments.\n    \"\"\"\n    self._verbose = verbose\n    self._original_fit = self.fit\n    self._original_predict = self.predict\n\n    # Replace fit and predict methods\n    if hasattr(self, \"fit\"):\n        self.__setattr__(\"fit\", self._pre_fit_wrapp)\n    if hasattr(self, \"predict\"):\n        self.__setattr__(\"predict\", self._pre_predict_wrapp)\n\n    super().__init__(*args, **kwargs)\n\n    self._m_hash: str\n    self._m_str: str\n    self._m_path: str\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore the object from its pickled state.</p> <p>This method restores the wrapper methods after deserialization.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>The pickled state of the object.</p> required Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __setstate__(self, state: Dict[str, Any]):\n    \"\"\"\n    Restore the object from its pickled state.\n\n    This method restores the wrapper methods after deserialization.\n\n    Args:\n        state (Dict[str, Any]): The pickled state of the object.\n    \"\"\"\n    super().__setstate__(state)\n    # Restore the wrapper methods after deserialization\n    self.__dict__[\"fit\"] = self._pre_fit_wrapp\n    self.__dict__[\"predict\"] = self._pre_predict_wrapp\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.clear_memory","title":"<code>clear_memory()</code>  <code>staticmethod</code>","text":"Source code in <code>framework3/base/base_clases.py</code> <pre><code>@staticmethod\ndef clear_memory():\n    import gc\n\n    gc.collect()\n    try:\n        import torch\n\n        torch.cuda.empty_cache()\n    except ImportError:\n        pass\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.fit","title":"<code>fit(x, y)</code>","text":"<p>Method for fitting the filter to the data.</p> <p>This method should be overridden by subclasses to implement specific fitting logic.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target data, if applicable.</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: An optional float value, typically used for metrics or loss.</p> <p>Raises:</p> Type Description <code>NotTrainableFilterError</code> <p>If the filter does not support fitting.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def fit(self, x: XYData, y: Optional[XYData]) -&gt; Optional[float]:\n    \"\"\"\n    Method for fitting the filter to the data.\n\n    This method should be overridden by subclasses to implement specific fitting logic.\n\n    Args:\n        x (XYData): The input data.\n        y (Optional[XYData]): The target data, if applicable.\n\n    Returns:\n        Optional[float]: An optional float value, typically used for metrics or loss.\n\n    Raises:\n        NotTrainableFilterError: If the filter does not support fitting.\n    \"\"\"\n    self.init()\n    raise NotTrainableFilterError(\"This filter does not support fitting.\")\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.grid","title":"<code>grid(grid)</code>","text":"<p>Set up grid search parameters for the filter.</p> <p>This method allows defining a grid of hyperparameters for optimization.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>Dict[str, List[Any] | Tuple[Any, Any]]</code> <p>A dictionary where keys are parameter names and values are lists or tuples of possible values.</p> required <p>Returns:</p> Name Type Description <code>BaseFilter</code> <code>BaseFilter</code> <p>The filter instance with grid search parameters set.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def grid(self, grid: Dict[str, List[Any] | Tuple[Any, Any]]) -&gt; BaseFilter:\n    \"\"\"\n    Set up grid search parameters for the filter.\n\n    This method allows defining a grid of hyperparameters for optimization.\n\n    Args:\n        grid (Dict[str, List[Any] | Tuple[Any, Any]]): A dictionary where keys are parameter names\n            and values are lists or tuples of possible values.\n\n    Returns:\n        BaseFilter: The filter instance with grid search parameters set.\n    \"\"\"\n    self._grid = grid\n    return self\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.init","title":"<code>init()</code>","text":"<p>Initialize filter-specific attributes.</p> <p>This method sets up the model hash, string representation, and path.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def init(self):\n    \"\"\"\n    Initialize filter-specific attributes.\n\n    This method sets up the model hash, string representation, and path.\n\n    \"\"\"\n    m_hash, m_str = self._get_model_key(data_hash=\" , \")\n\n    self._m_hash: str = m_hash\n    self._m_str: str = m_str\n    self._m_path: str = f\"{self._get_model_name()}/{m_hash}\"\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.predict","title":"<code>predict(x)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for making predictions using the filter.</p> <p>This method must be implemented by subclasses to provide specific prediction logic.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The prediction results.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>@abstractmethod\ndef predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Abstract method for making predictions using the filter.\n\n    This method must be implemented by subclasses to provide specific prediction logic.\n\n    Args:\n        x (XYData): The input data.\n\n    Returns:\n        XYData: The prediction results.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the base filter without any wrappers.</p> <p>This method is useful when you need to access the original filter without any additional layers or modifications added by wrappers.</p> <p>Returns:</p> Name Type Description <code>BaseFilter</code> <code>BaseFilter</code> <p>The unwrapped base filter.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def unwrap(self) -&gt; BaseFilter:\n    \"\"\"\n    Return the base filter without any wrappers.\n\n    This method is useful when you need to access the original filter without any\n    additional layers or modifications added by wrappers.\n\n    Returns:\n        BaseFilter: The unwrapped base filter.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/base/base_filter/#framework3.base.base_clases.BaseFilter.verbose","title":"<code>verbose(value)</code>","text":"<p>Set the verbosity of the filter.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>If True, enables verbose output; if False, disables it.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def verbose(self, value: bool) -&gt; None:\n    \"\"\"\n    Set the verbosity of the filter.\n\n    Args:\n        value (bool): If True, enables verbose output; if False, disables it.\n\n    Returns:\n        None\n    \"\"\"\n    self._verbose = value\n</code></pre>"},{"location":"api/base/base_map_reduce/","title":"Base map reduce","text":""},{"location":"api/base/base_map_reduce/#framework3.base.base_map_reduce","title":"<code>framework3.base.base_map_reduce</code>","text":""},{"location":"api/base/base_map_reduce/#framework3.base.base_map_reduce.MapReduceStrategy","title":"<code>MapReduceStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for implementing Map-Reduce strategies.</p> <p>This class defines the interface for Map-Reduce operations, providing a structure for implementing various distributed computing strategies.</p> Key Features <ul> <li>Abstract methods for map and reduce operations</li> <li>Support for custom map and reduce functions</li> <li>Method to stop the Map-Reduce process</li> </ul> Usage <p>To create a new Map-Reduce strategy, inherit from this class and implement all abstract methods. For example:</p> <pre><code>class SimpleMapReduce(MapReduceStrategy):\n    def __init__(self):\n        self.intermediate = []\n        self.result = None\n\n    def map(self, data: list, map_function: Callable) -&gt; None:\n        self.intermediate = [map_function(item) for item in data]\n\n    def reduce(self, reduce_function: Callable) -&gt; Any:\n        self.result = reduce_function(self.intermediate)\n        return self.result\n\n    def stop(self) -&gt; None:\n        self.intermediate = []\n        self.result = None\n\n# Usage\nmr = SimpleMapReduce()\ndata = [1, 2, 3, 4, 5]\nmr.map(data, lambda x: x * 2)\nresult = mr.reduce(sum)\nprint(result)  # Output: 30\nmr.stop()\n</code></pre> <p>Methods:</p> Name Description <code>map</code> <p>Any, map_function: Callable) -&gt; Any: Abstract method to perform the map operation.</p> <code>reduce</code> <p>Callable) -&gt; Any: Abstract method to perform the reduce operation.</p> <code>stop</code> <p>Abstract method to stop the Map-Reduce process.</p> Note <p>This is an abstract base class. Concrete implementations should override all abstract methods to provide specific Map-Reduce functionality.</p> Source code in <code>framework3/base/base_map_reduce.py</code> <pre><code>class MapReduceStrategy(ABC):\n    \"\"\"\n    Abstract base class for implementing Map-Reduce strategies.\n\n    This class defines the interface for Map-Reduce operations, providing a structure\n    for implementing various distributed computing strategies.\n\n    Key Features:\n        - Abstract methods for map and reduce operations\n        - Support for custom map and reduce functions\n        - Method to stop the Map-Reduce process\n\n    Usage:\n        To create a new Map-Reduce strategy, inherit from this class and implement\n        all abstract methods. For example:\n\n        ```python\n        class SimpleMapReduce(MapReduceStrategy):\n            def __init__(self):\n                self.intermediate = []\n                self.result = None\n\n            def map(self, data: list, map_function: Callable) -&gt; None:\n                self.intermediate = [map_function(item) for item in data]\n\n            def reduce(self, reduce_function: Callable) -&gt; Any:\n                self.result = reduce_function(self.intermediate)\n                return self.result\n\n            def stop(self) -&gt; None:\n                self.intermediate = []\n                self.result = None\n\n        # Usage\n        mr = SimpleMapReduce()\n        data = [1, 2, 3, 4, 5]\n        mr.map(data, lambda x: x * 2)\n        result = mr.reduce(sum)\n        print(result)  # Output: 30\n        mr.stop()\n        ```\n\n    Methods:\n        map(data: Any, map_function: Callable) -&gt; Any:\n            Abstract method to perform the map operation.\n        reduce(reduce_function: Callable) -&gt; Any:\n            Abstract method to perform the reduce operation.\n        stop() -&gt; None:\n            Abstract method to stop the Map-Reduce process.\n\n    Note:\n        This is an abstract base class. Concrete implementations should override\n        all abstract methods to provide specific Map-Reduce functionality.\n    \"\"\"\n\n    @abstractmethod\n    def map(self, data: Any, map_function: Callable) -&gt; Any:\n        \"\"\"\n        Perform the map operation on the input data.\n\n        This method should be implemented to apply the map_function to each element\n        of the input data, potentially in a distributed manner.\n\n        Args:\n            data (Any): The input data to be processed.\n            map_function (Callable): The function to be applied to each data element.\n\n        Returns:\n            Any: The result of the map operation, which could be a collection of\n                 intermediate key-value pairs or any other suitable format.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def map(self, data: list, map_function: Callable) -&gt; list:\n                return [map_function(item) for item in data]\n            ```\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reduce(self, reduce_function: Callable) -&gt; Any:\n        \"\"\"\n        Perform the reduce operation on the mapped data.\n\n        This method should be implemented to apply the reduce_function to the\n        intermediate results produced by the map operation.\n\n        Args:\n            reduce_function (Callable): The function to be used for reducing the mapped data.\n\n        Returns:\n            Any: The final result of the Map-Reduce operation.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def reduce(self, reduce_function: Callable) -&gt; Any:\n                return reduce_function(self.intermediate_results)\n            ```\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def stop(self) -&gt; None:\n        \"\"\"\n        Stop the Map-Reduce process and perform any necessary cleanup.\n\n        This method should be implemented to halt the Map-Reduce operation,\n        release resources, and reset the state if needed.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def stop(self) -&gt; None:\n                self.intermediate_results = []\n                self.final_result = None\n                # Additional cleanup code...\n            ```\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/base/base_map_reduce/#framework3.base.base_map_reduce.MapReduceStrategy.map","title":"<code>map(data, map_function)</code>  <code>abstractmethod</code>","text":"<p>Perform the map operation on the input data.</p> <p>This method should be implemented to apply the map_function to each element of the input data, potentially in a distributed manner.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data to be processed.</p> required <code>map_function</code> <code>Callable</code> <p>The function to be applied to each data element.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the map operation, which could be a collection of  intermediate key-value pairs or any other suitable format.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def map(self, data: list, map_function: Callable) -&gt; list:\n    return [map_function(item) for item in data]\n</code></pre> Source code in <code>framework3/base/base_map_reduce.py</code> <pre><code>@abstractmethod\ndef map(self, data: Any, map_function: Callable) -&gt; Any:\n    \"\"\"\n    Perform the map operation on the input data.\n\n    This method should be implemented to apply the map_function to each element\n    of the input data, potentially in a distributed manner.\n\n    Args:\n        data (Any): The input data to be processed.\n        map_function (Callable): The function to be applied to each data element.\n\n    Returns:\n        Any: The result of the map operation, which could be a collection of\n             intermediate key-value pairs or any other suitable format.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def map(self, data: list, map_function: Callable) -&gt; list:\n            return [map_function(item) for item in data]\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/base_map_reduce/#framework3.base.base_map_reduce.MapReduceStrategy.reduce","title":"<code>reduce(reduce_function)</code>  <code>abstractmethod</code>","text":"<p>Perform the reduce operation on the mapped data.</p> <p>This method should be implemented to apply the reduce_function to the intermediate results produced by the map operation.</p> <p>Parameters:</p> Name Type Description Default <code>reduce_function</code> <code>Callable</code> <p>The function to be used for reducing the mapped data.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The final result of the Map-Reduce operation.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def reduce(self, reduce_function: Callable) -&gt; Any:\n    return reduce_function(self.intermediate_results)\n</code></pre> Source code in <code>framework3/base/base_map_reduce.py</code> <pre><code>@abstractmethod\ndef reduce(self, reduce_function: Callable) -&gt; Any:\n    \"\"\"\n    Perform the reduce operation on the mapped data.\n\n    This method should be implemented to apply the reduce_function to the\n    intermediate results produced by the map operation.\n\n    Args:\n        reduce_function (Callable): The function to be used for reducing the mapped data.\n\n    Returns:\n        Any: The final result of the Map-Reduce operation.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def reduce(self, reduce_function: Callable) -&gt; Any:\n            return reduce_function(self.intermediate_results)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/base_map_reduce/#framework3.base.base_map_reduce.MapReduceStrategy.stop","title":"<code>stop()</code>  <code>abstractmethod</code>","text":"<p>Stop the Map-Reduce process and perform any necessary cleanup.</p> <p>This method should be implemented to halt the Map-Reduce operation, release resources, and reset the state if needed.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def stop(self) -&gt; None:\n    self.intermediate_results = []\n    self.final_result = None\n    # Additional cleanup code...\n</code></pre> Source code in <code>framework3/base/base_map_reduce.py</code> <pre><code>@abstractmethod\ndef stop(self) -&gt; None:\n    \"\"\"\n    Stop the Map-Reduce process and perform any necessary cleanup.\n\n    This method should be implemented to halt the Map-Reduce operation,\n    release resources, and reset the state if needed.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def stop(self) -&gt; None:\n            self.intermediate_results = []\n            self.final_result = None\n            # Additional cleanup code...\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_metric/","title":"Metric","text":""},{"location":"api/base/base_metric/#framework3.base.base_clases.BaseMetric","title":"<code>framework3.base.base_clases.BaseMetric</code>","text":"<p>               Bases: <code>BasePlugin</code></p> <p>Base class for implementing metric calculations in the framework.</p> <p>This abstract class defines the interface for metric evaluation and provides a structure for implementing various performance metrics. It extends BasePlugin to inherit core functionality for attribute management and serialization.</p> Key Features <ul> <li>Abstract evaluate method for implementing specific metric calculations</li> <li>higher_better attribute to indicate if higher metric values are better</li> <li>Inherits BasePlugin functionality for attribute management and serialization</li> </ul> Usage <p>To create a new metric, inherit from this class and implement the evaluate method:</p> <pre><code>from framework3.base.base_clases import BaseMetric\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\nclass MeanSquaredError(BaseMetric):\n    higher_better = False\n\n    def evaluate(self, x_data: XYData, y_true: XYData, y_pred: XYData) -&gt; float:\n        return np.mean((y_true.value - y_pred.value) ** 2)\n</code></pre> <p>Attributes:</p> Name Type Description <code>higher_better</code> <code>bool</code> <p>Indicates whether higher values of the metric are better.                   Defaults to True.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Float | np.ndarray: Abstract method to be implemented by subclasses for specific metric calculations.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>class BaseMetric(BasePlugin):\n    \"\"\"\n    Base class for implementing metric calculations in the framework.\n\n    This abstract class defines the interface for metric evaluation and provides\n    a structure for implementing various performance metrics. It extends BasePlugin\n    to inherit core functionality for attribute management and serialization.\n\n    Key Features:\n        - Abstract evaluate method for implementing specific metric calculations\n        - higher_better attribute to indicate if higher metric values are better\n        - Inherits BasePlugin functionality for attribute management and serialization\n\n    Usage:\n        To create a new metric, inherit from this class and implement the evaluate method:\n\n        ```python\n        from framework3.base.base_clases import BaseMetric\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        class MeanSquaredError(BaseMetric):\n            higher_better = False\n\n            def evaluate(self, x_data: XYData, y_true: XYData, y_pred: XYData) -&gt; float:\n                return np.mean((y_true.value - y_pred.value) ** 2)\n        ```\n\n    Attributes:\n        higher_better (bool): Indicates whether higher values of the metric are better.\n                              Defaults to True.\n\n    Methods:\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Float | np.ndarray:\n            Abstract method to be implemented by subclasses for specific metric calculations.\n    \"\"\"\n\n    higher_better: bool = True\n\n    @abstractmethod\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Evaluate the metric based on the provided data.\n\n        This abstract method should be implemented by subclasses to calculate\n        the specific metric. It provides a standardized interface for all metrics\n        in the framework.\n\n        Args:\n            x_data (XYData): The input data used for the prediction.\n            y_true (XYData | None): The ground truth or actual values. Can be None for some metrics.\n            y_pred (XYData): The predicted values.\n\n        Returns:\n            Float | np.ndarray: The calculated metric value. This can be a single float\n                                or a numpy array, depending on the specific metric implementation.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Note:\n            Subclasses must override this method to provide the specific metric calculation logic.\n        \"\"\"\n\n        ...\n</code></pre>"},{"location":"api/base/base_metric/#framework3.base.base_clases.BaseMetric.higher_better","title":"<code>higher_better = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/base/base_metric/#framework3.base.base_clases.BaseMetric.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the metric based on the provided data.</p> <p>This abstract method should be implemented by subclasses to calculate the specific metric. It provides a standardized interface for all metrics in the framework.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data used for the prediction.</p> required <code>y_true</code> <code>XYData | None</code> <p>The ground truth or actual values. Can be None for some metrics.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted values.</p> required <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The calculated metric value. This can be a single float                 or a numpy array, depending on the specific metric implementation.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Note <p>Subclasses must override this method to provide the specific metric calculation logic.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>@abstractmethod\ndef evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Evaluate the metric based on the provided data.\n\n    This abstract method should be implemented by subclasses to calculate\n    the specific metric. It provides a standardized interface for all metrics\n    in the framework.\n\n    Args:\n        x_data (XYData): The input data used for the prediction.\n        y_true (XYData | None): The ground truth or actual values. Can be None for some metrics.\n        y_pred (XYData): The predicted values.\n\n    Returns:\n        Float | np.ndarray: The calculated metric value. This can be a single float\n                            or a numpy array, depending on the specific metric implementation.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Note:\n        Subclasses must override this method to provide the specific metric calculation logic.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"api/base/base_optimizer/","title":"Optimizer","text":""},{"location":"api/base/base_optimizer/#framework3.base.base_optimizer","title":"<code>framework3.base.base_optimizer</code>","text":""},{"location":"api/base/base_optimizer/#framework3.base.base_optimizer.__all__","title":"<code>__all__ = ['BaseOptimizer']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/base_optimizer/#framework3.base.base_optimizer.BaseOptimizer","title":"<code>BaseOptimizer</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>Base class for optimizer components in the framework.</p> <p>This abstract class extends BaseFilter and defines the interface for optimizer operations. It provides a structure for implementing optimization strategies for pipelines.</p> Key Features <ul> <li>Abstract methods for starting optimization process and optimizing pipelines</li> <li>Support for verbose output control</li> </ul> Usage <p>To create a new optimizer type, inherit from this class and implement the required methods. For example:</p> <pre><code>class MyCustomOptimizer(BaseOptimizer):\n    def __init__(self, optimization_params):\n        super().__init__()\n        self.optimization_params = optimization_params\n\n    def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n        # Implement optimization start logic\n        pass\n\n    def optimize(self, pipeline: BaseFilter) -&gt; None:\n        # Implement pipeline optimization logic\n        pass\n</code></pre> <p>Attributes:</p> Name Type Description <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be optimized.</p> <p>Methods:</p> Name Description <code>start</code> <p>XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]: Abstract method to start the optimization process.</p> <code>optimize</code> <p>BaseFilter) -&gt; None: Abstract method to optimize the given pipeline.</p> <code>verbose</code> <p>bool) -&gt; None: Sets the verbosity level for the optimizer and its pipeline.</p> Note <p>This is an abstract base class. Concrete implementations should override the start and optimize methods to provide specific optimization functionality.</p> Source code in <code>framework3/base/base_optimizer.py</code> <pre><code>class BaseOptimizer(BaseFilter):\n    \"\"\"\n    Base class for optimizer components in the framework.\n\n    This abstract class extends BaseFilter and defines the interface for optimizer operations.\n    It provides a structure for implementing optimization strategies for pipelines.\n\n    Key Features:\n        - Abstract methods for starting optimization process and optimizing pipelines\n        - Support for verbose output control\n\n    Usage:\n        To create a new optimizer type, inherit from this class and implement\n        the required methods. For example:\n\n        ```python\n        class MyCustomOptimizer(BaseOptimizer):\n            def __init__(self, optimization_params):\n                super().__init__()\n                self.optimization_params = optimization_params\n\n            def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n                # Implement optimization start logic\n                pass\n\n            def optimize(self, pipeline: BaseFilter) -&gt; None:\n                # Implement pipeline optimization logic\n                pass\n        ```\n\n    Attributes:\n        pipeline (BaseFilter): The pipeline to be optimized.\n\n    Methods:\n        start(x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n            Abstract method to start the optimization process.\n\n        optimize(pipeline: BaseFilter) -&gt; None:\n            Abstract method to optimize the given pipeline.\n\n        verbose(value: bool) -&gt; None:\n            Sets the verbosity level for the optimizer and its pipeline.\n\n    Note:\n        This is an abstract base class. Concrete implementations should override\n        the start and optimize methods to provide specific optimization functionality.\n    \"\"\"\n\n    @abstractmethod\n    def start(\n        self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n    ) -&gt; Optional[XYData]:\n        \"\"\"\n        Start the optimization process.\n\n        This abstract method should be implemented by subclasses to define\n        the specific logic for initiating the optimization process.\n\n        Args:\n            x (XYData): The primary input data.\n            y (Optional[XYData]): Optional target data.\n            X_ (Optional[XYData]): Optional additional input data.\n\n        Returns:\n            Optional[XYData]: The processed data, if any.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            class MyOptimizer(BaseOptimizer):\n                def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n                    # Optimization start logic here\n                    return processed_data\n            ```\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def optimize(self, pipeline: BaseFilter) -&gt; None:\n        \"\"\"\n        Optimize the given pipeline.\n\n        This abstract method should be implemented by subclasses to define\n        the specific logic for optimizing a pipeline.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be optimized.\n\n        Returns:\n            None\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            class MyOptimizer(BaseOptimizer):\n                def optimize(self, pipeline: BaseFilter) -&gt; None:\n                    # Pipeline optimization logic here\n                    pass\n            ```\n        \"\"\"\n        ...\n\n    def verbose(self, value: bool) -&gt; None:\n        \"\"\"\n        Set the verbosity of the optimizer and its pipeline.\n\n        This method controls the verbosity of both the optimizer itself and its associated pipeline.\n\n        Args:\n            value (bool): If True, enables verbose output; if False, disables it.\n\n        Returns:\n            None\n\n        Example:\n            ```python\n            optimizer = MyOptimizer()\n            optimizer.verbose(True)  # Enable verbose output\n            ```\n        \"\"\"\n        self._verbose = value\n        self.pipeline.verbose(value)\n</code></pre>"},{"location":"api/base/base_optimizer/#framework3.base.base_optimizer.BaseOptimizer.optimize","title":"<code>optimize(pipeline)</code>  <code>abstractmethod</code>","text":"<p>Optimize the given pipeline.</p> <p>This abstract method should be implemented by subclasses to define the specific logic for optimizing a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be optimized.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>class MyOptimizer(BaseOptimizer):\n    def optimize(self, pipeline: BaseFilter) -&gt; None:\n        # Pipeline optimization logic here\n        pass\n</code></pre> Source code in <code>framework3/base/base_optimizer.py</code> <pre><code>@abstractmethod\ndef optimize(self, pipeline: BaseFilter) -&gt; None:\n    \"\"\"\n    Optimize the given pipeline.\n\n    This abstract method should be implemented by subclasses to define\n    the specific logic for optimizing a pipeline.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be optimized.\n\n    Returns:\n        None\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        class MyOptimizer(BaseOptimizer):\n            def optimize(self, pipeline: BaseFilter) -&gt; None:\n                # Pipeline optimization logic here\n                pass\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_optimizer/#framework3.base.base_optimizer.BaseOptimizer.start","title":"<code>start(x, y, X_)</code>  <code>abstractmethod</code>","text":"<p>Start the optimization process.</p> <p>This abstract method should be implemented by subclasses to define the specific logic for initiating the optimization process.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The primary input data.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Optional target data.</p> required <code>X_</code> <code>Optional[XYData]</code> <p>Optional additional input data.</p> required <p>Returns:</p> Type Description <code>Optional[XYData]</code> <p>Optional[XYData]: The processed data, if any.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>class MyOptimizer(BaseOptimizer):\n    def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n        # Optimization start logic here\n        return processed_data\n</code></pre> Source code in <code>framework3/base/base_optimizer.py</code> <pre><code>@abstractmethod\ndef start(\n    self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n) -&gt; Optional[XYData]:\n    \"\"\"\n    Start the optimization process.\n\n    This abstract method should be implemented by subclasses to define\n    the specific logic for initiating the optimization process.\n\n    Args:\n        x (XYData): The primary input data.\n        y (Optional[XYData]): Optional target data.\n        X_ (Optional[XYData]): Optional additional input data.\n\n    Returns:\n        Optional[XYData]: The processed data, if any.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        class MyOptimizer(BaseOptimizer):\n            def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n                # Optimization start logic here\n                return processed_data\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_optimizer/#framework3.base.base_optimizer.BaseOptimizer.verbose","title":"<code>verbose(value)</code>","text":"<p>Set the verbosity of the optimizer and its pipeline.</p> <p>This method controls the verbosity of both the optimizer itself and its associated pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>If True, enables verbose output; if False, disables it.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <pre><code>optimizer = MyOptimizer()\noptimizer.verbose(True)  # Enable verbose output\n</code></pre> Source code in <code>framework3/base/base_optimizer.py</code> <pre><code>def verbose(self, value: bool) -&gt; None:\n    \"\"\"\n    Set the verbosity of the optimizer and its pipeline.\n\n    This method controls the verbosity of both the optimizer itself and its associated pipeline.\n\n    Args:\n        value (bool): If True, enables verbose output; if False, disables it.\n\n    Returns:\n        None\n\n    Example:\n        ```python\n        optimizer = MyOptimizer()\n        optimizer.verbose(True)  # Enable verbose output\n        ```\n    \"\"\"\n    self._verbose = value\n    self.pipeline.verbose(value)\n</code></pre>"},{"location":"api/base/base_pipelines/","title":"Pipeline","text":""},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines","title":"<code>framework3.base.base_pipelines</code>","text":""},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.__all__","title":"<code>__all__ = ['BasePipeline', 'SequentialPipeline', 'ParallelPipeline']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline","title":"<code>BasePipeline</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>Base class for pipeline structures in the framework.</p> <p>This abstract class extends BaseFilter and defines the interface for pipeline operations. It provides a structure for implementing complex data flows and combinations of filters.</p> Key Features <ul> <li>Abstract methods for starting pipeline processing and evaluation</li> <li>Support for verbose output control</li> <li>Methods for initializing filters, getting filter types, and applying optimizers and splitters</li> <li>Access to inner filters of the pipeline</li> </ul> Usage <p>To create a new pipeline type, inherit from this class and implement the required methods. For example:</p> <pre><code>class MyCustomPipeline(BasePipeline):\n    def __init__(self, filters: List[BaseFilter]):\n        super().__init__(filters=filters)\n\n    def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n        # Implement pipeline start logic\n        pass\n\n    def evaluate(self, x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n        # Implement evaluation logic\n        pass\n</code></pre> <p>Attributes:</p> Name Type Description <code>filters</code> <code>List[BaseFilter]</code> <p>List of filters in the pipeline.</p> <p>Methods:</p> Name Description <code>start</code> <p>XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]: Abstract method to start the pipeline processing.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Abstract method to evaluate the pipeline's performance.</p> <code>verbose</code> <p>bool) -&gt; None: Sets the verbosity level for the pipeline and its filters.</p> <code>init</code> <p>Initializes the pipeline and its filters.</p> <code>get_types</code> <p>Returns the types of filters in the pipeline.</p> <code>optimizer</code> <p>BaseOptimizer) -&gt; BaseOptimizer: Applies an optimizer to the pipeline.</p> <code>splitter</code> <p>BaseSplitter) -&gt; BaseSplitter: Applies a splitter to the pipeline.</p> <code>inner</code> <p>Returns the inner filters of the pipeline.</p> Note <p>This is an abstract base class. Concrete implementations should override the start and evaluate methods to provide specific pipeline functionality.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>class BasePipeline(BaseFilter):\n    \"\"\"\n    Base class for pipeline structures in the framework.\n\n    This abstract class extends BaseFilter and defines the interface for pipeline operations.\n    It provides a structure for implementing complex data flows and combinations of filters.\n\n    Key Features:\n        - Abstract methods for starting pipeline processing and evaluation\n        - Support for verbose output control\n        - Methods for initializing filters, getting filter types, and applying optimizers and splitters\n        - Access to inner filters of the pipeline\n\n    Usage:\n        To create a new pipeline type, inherit from this class and implement\n        the required methods. For example:\n\n        ```python\n        class MyCustomPipeline(BasePipeline):\n            def __init__(self, filters: List[BaseFilter]):\n                super().__init__(filters=filters)\n\n            def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n                # Implement pipeline start logic\n                pass\n\n            def evaluate(self, x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n                # Implement evaluation logic\n                pass\n        ```\n\n    Attributes:\n        filters (List[BaseFilter]): List of filters in the pipeline.\n\n    Methods:\n        start(x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n            Abstract method to start the pipeline processing.\n\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Abstract method to evaluate the pipeline's performance.\n\n        verbose(value: bool) -&gt; None:\n            Sets the verbosity level for the pipeline and its filters.\n\n        init(*args, **kwargs) -&gt; None:\n            Initializes the pipeline and its filters.\n\n        get_types() -&gt; List[Type[BaseFilter]]:\n            Returns the types of filters in the pipeline.\n\n        optimizer(optimizer: BaseOptimizer) -&gt; BaseOptimizer:\n            Applies an optimizer to the pipeline.\n\n        splitter(splitter: BaseSplitter) -&gt; BaseSplitter:\n            Applies a splitter to the pipeline.\n\n        inner() -&gt; BaseFilter | List[BaseFilter] | None:\n            Returns the inner filters of the pipeline.\n\n    Note:\n        This is an abstract base class. Concrete implementations should override\n        the start and evaluate methods to provide specific pipeline functionality.\n    \"\"\"\n\n    @abstractmethod\n    def start(\n        self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n    ) -&gt; Optional[XYData]:\n        \"\"\"\n        Start the pipeline processing.\n\n        This abstract method should be implemented by subclasses to define\n        the specific logic for initiating the pipeline's data processing.\n\n        Args:\n            x (XYData): The primary input data.\n            y (Optional[XYData]): Optional target data.\n            X_ (Optional[XYData]): Optional additional input data.\n\n        Returns:\n            Optional[XYData]: The processed data, if any.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the pipeline's performance.\n\n        This abstract method should be implemented by subclasses to define\n        the specific logic for evaluating the pipeline's output.\n\n        Args:\n            x_data (XYData): The input data used for prediction.\n            y_true (XYData | None): The ground truth or actual values.\n            y_pred (XYData): The predicted values.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing evaluation metrics.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n        \"\"\"\n        ...\n\n    def verbose(self, value: bool) -&gt; None:\n        \"\"\"\n        Set the verbosity of the pipeline and its filters.\n\n        This method controls the verbosity of both the pipeline itself and all its constituent filters.\n\n        Args:\n            value (bool): If True, enables verbose output; if False, disables it.\n\n        Returns:\n            None\n        \"\"\"\n        self._verbose = value\n        for filter in self.filters:\n            filter.verbose(value)\n\n    def init(self, *args: List[Any], **kwargs: Dict[str, Any]):\n        \"\"\"\n        Initialize the pipeline and its filters.\n\n        This method initializes both the pipeline itself and all its constituent filters.\n\n        Args:\n            *args (List[Any]): Variable length argument list.\n            **kwargs (Dict[str,Any]): Arbitrary keyword arguments.\n\n        \"\"\"\n        super().init(*args, **kwargs)\n        for filter in self.filters:\n            filter.init(*args, **kwargs)\n\n    def get_types(self) -&gt; List[Type[BaseFilter]]:\n        \"\"\"\n        Get the types of filters in the pipeline.\n\n        This method returns a list of the types of all filters contained in the pipeline.\n\n        Returns:\n            List[Type[BaseFilter]]: A list of filter types in the pipeline.\n        \"\"\"\n        return list(map(lambda obj: type(obj), self.filters))\n\n    def optimizer(self, optimizer: BaseOptimizer) -&gt; BaseOptimizer:\n        \"\"\"\n        Apply an optimizer to the pipeline.\n\n        This method allows an optimizer to be applied to the entire pipeline.\n\n        Args:\n            optimizer (BaseOptimizer): The optimizer to apply to the pipeline.\n\n        Returns:\n            BaseOptimizer: The optimizer after optimization.\n        \"\"\"\n        optimizer.optimize(self)\n        return optimizer\n\n    def splitter(self, splitter: BaseSplitter) -&gt; BaseSplitter:\n        \"\"\"\n        Apply a splitter to the pipeline.\n\n        This method allows a splitter to be applied to the entire pipeline.\n\n        Args:\n            splitter (BaseSplitter): The splitter to apply to the pipeline.\n\n        Returns:\n            BaseSplitter: The splitter after splitting.\n        \"\"\"\n        splitter.split(self)\n        return splitter\n\n    def inner(self) -&gt; BaseFilter | List[BaseFilter] | None:\n        \"\"\"\n        Get the inner filters of the pipeline.\n\n        This method returns the list of filters contained within the pipeline.\n\n        Returns:\n            BaseFilter | List[BaseFilter] | None: The inner filters of the pipeline.\n        \"\"\"\n        return self.filters\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the pipeline's performance.</p> <p>This abstract method should be implemented by subclasses to define the specific logic for evaluating the pipeline's output.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data used for prediction.</p> required <code>y_true</code> <code>XYData | None</code> <p>The ground truth or actual values.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing evaluation metrics.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>@abstractmethod\ndef evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the pipeline's performance.\n\n    This abstract method should be implemented by subclasses to define\n    the specific logic for evaluating the pipeline's output.\n\n    Args:\n        x_data (XYData): The input data used for prediction.\n        y_true (XYData | None): The ground truth or actual values.\n        y_pred (XYData): The predicted values.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing evaluation metrics.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.get_types","title":"<code>get_types()</code>","text":"<p>Get the types of filters in the pipeline.</p> <p>This method returns a list of the types of all filters contained in the pipeline.</p> <p>Returns:</p> Type Description <code>List[Type[BaseFilter]]</code> <p>List[Type[BaseFilter]]: A list of filter types in the pipeline.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>def get_types(self) -&gt; List[Type[BaseFilter]]:\n    \"\"\"\n    Get the types of filters in the pipeline.\n\n    This method returns a list of the types of all filters contained in the pipeline.\n\n    Returns:\n        List[Type[BaseFilter]]: A list of filter types in the pipeline.\n    \"\"\"\n    return list(map(lambda obj: type(obj), self.filters))\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.init","title":"<code>init(*args, **kwargs)</code>","text":"<p>Initialize the pipeline and its filters.</p> <p>This method initializes both the pipeline itself and all its constituent filters.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>List[Any]</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>def init(self, *args: List[Any], **kwargs: Dict[str, Any]):\n    \"\"\"\n    Initialize the pipeline and its filters.\n\n    This method initializes both the pipeline itself and all its constituent filters.\n\n    Args:\n        *args (List[Any]): Variable length argument list.\n        **kwargs (Dict[str,Any]): Arbitrary keyword arguments.\n\n    \"\"\"\n    super().init(*args, **kwargs)\n    for filter in self.filters:\n        filter.init(*args, **kwargs)\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.inner","title":"<code>inner()</code>","text":"<p>Get the inner filters of the pipeline.</p> <p>This method returns the list of filters contained within the pipeline.</p> <p>Returns:</p> Type Description <code>BaseFilter | List[BaseFilter] | None</code> <p>BaseFilter | List[BaseFilter] | None: The inner filters of the pipeline.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>def inner(self) -&gt; BaseFilter | List[BaseFilter] | None:\n    \"\"\"\n    Get the inner filters of the pipeline.\n\n    This method returns the list of filters contained within the pipeline.\n\n    Returns:\n        BaseFilter | List[BaseFilter] | None: The inner filters of the pipeline.\n    \"\"\"\n    return self.filters\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.optimizer","title":"<code>optimizer(optimizer)</code>","text":"<p>Apply an optimizer to the pipeline.</p> <p>This method allows an optimizer to be applied to the entire pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>BaseOptimizer</code> <p>The optimizer to apply to the pipeline.</p> required <p>Returns:</p> Name Type Description <code>BaseOptimizer</code> <code>BaseOptimizer</code> <p>The optimizer after optimization.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>def optimizer(self, optimizer: BaseOptimizer) -&gt; BaseOptimizer:\n    \"\"\"\n    Apply an optimizer to the pipeline.\n\n    This method allows an optimizer to be applied to the entire pipeline.\n\n    Args:\n        optimizer (BaseOptimizer): The optimizer to apply to the pipeline.\n\n    Returns:\n        BaseOptimizer: The optimizer after optimization.\n    \"\"\"\n    optimizer.optimize(self)\n    return optimizer\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.splitter","title":"<code>splitter(splitter)</code>","text":"<p>Apply a splitter to the pipeline.</p> <p>This method allows a splitter to be applied to the entire pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>splitter</code> <code>BaseSplitter</code> <p>The splitter to apply to the pipeline.</p> required <p>Returns:</p> Name Type Description <code>BaseSplitter</code> <code>BaseSplitter</code> <p>The splitter after splitting.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>def splitter(self, splitter: BaseSplitter) -&gt; BaseSplitter:\n    \"\"\"\n    Apply a splitter to the pipeline.\n\n    This method allows a splitter to be applied to the entire pipeline.\n\n    Args:\n        splitter (BaseSplitter): The splitter to apply to the pipeline.\n\n    Returns:\n        BaseSplitter: The splitter after splitting.\n    \"\"\"\n    splitter.split(self)\n    return splitter\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.start","title":"<code>start(x, y, X_)</code>  <code>abstractmethod</code>","text":"<p>Start the pipeline processing.</p> <p>This abstract method should be implemented by subclasses to define the specific logic for initiating the pipeline's data processing.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The primary input data.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Optional target data.</p> required <code>X_</code> <code>Optional[XYData]</code> <p>Optional additional input data.</p> required <p>Returns:</p> Type Description <code>Optional[XYData]</code> <p>Optional[XYData]: The processed data, if any.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>@abstractmethod\ndef start(\n    self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n) -&gt; Optional[XYData]:\n    \"\"\"\n    Start the pipeline processing.\n\n    This abstract method should be implemented by subclasses to define\n    the specific logic for initiating the pipeline's data processing.\n\n    Args:\n        x (XYData): The primary input data.\n        y (Optional[XYData]): Optional target data.\n        X_ (Optional[XYData]): Optional additional input data.\n\n    Returns:\n        Optional[XYData]: The processed data, if any.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.BasePipeline.verbose","title":"<code>verbose(value)</code>","text":"<p>Set the verbosity of the pipeline and its filters.</p> <p>This method controls the verbosity of both the pipeline itself and all its constituent filters.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>If True, enables verbose output; if False, disables it.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>def verbose(self, value: bool) -&gt; None:\n    \"\"\"\n    Set the verbosity of the pipeline and its filters.\n\n    This method controls the verbosity of both the pipeline itself and all its constituent filters.\n\n    Args:\n        value (bool): If True, enables verbose output; if False, disables it.\n\n    Returns:\n        None\n    \"\"\"\n    self._verbose = value\n    for filter in self.filters:\n        filter.verbose(value)\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.ParallelPipeline","title":"<code>ParallelPipeline</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>A pipeline that processes filters in parallel.</p> <p>This class implements a pipeline where filters can be applied concurrently, potentially improving performance for certain types of operations.</p> Note <p>The implementation details for this class are not provided in the given code snippet. It is expected that concrete implementations will define the specific behavior for parallel processing of filters.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>class ParallelPipeline(BasePipeline):\n    \"\"\"\n    A pipeline that processes filters in parallel.\n\n    This class implements a pipeline where filters can be applied concurrently,\n    potentially improving performance for certain types of operations.\n\n    Note:\n        The implementation details for this class are not provided in the given code snippet.\n        It is expected that concrete implementations will define the specific behavior\n        for parallel processing of filters.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"api/base/base_pipelines/#framework3.base.base_pipelines.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>A pipeline that processes filters sequentially.</p> <p>This class implements a pipeline where each filter is applied in sequence, with the output of one filter becoming the input of the next.</p> Key Features <ul> <li>Sequential processing of filters</li> <li>Implements start method for initiating the pipeline</li> <li>Supports both fit and predict operations</li> </ul> Usage <pre><code>from framework3.base import SequentialPipeline, XYData\nfrom framework3.plugins.filters import StandardScaler, PCA, LogisticRegression\n\npipeline = SequentialPipeline([\n    StandardScaler(),\n    PCA(n_components=5),\n    LogisticRegression()\n])\n\nX_train = XYData.mock(np.random.rand(100, 10))\ny_train = XYData.mock(np.random.randint(0, 2, 100))\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_train)\n</code></pre> <p>Methods:</p> Name Description <code>start</code> <p>XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]: Starts the sequential processing of filters in the pipeline.</p> <code>_pre_fit</code> <p>XYData, y: Optional[XYData]) -&gt; Tuple[str, str, str]: Prepares the pipeline for fitting by initializing model attributes and pre-fitting filters.</p> <code>_pre_predict</code> <p>XYData) -&gt; XYData: Prepares the pipeline for prediction by applying pre-predict operations on all filters.</p> Note <p>This class extends BasePipeline and provides a concrete implementation for sequential processing of filters.</p> Source code in <code>framework3/base/base_pipelines.py</code> <pre><code>class SequentialPipeline(BasePipeline):\n    \"\"\"\n    A pipeline that processes filters sequentially.\n\n    This class implements a pipeline where each filter is applied in sequence,\n    with the output of one filter becoming the input of the next.\n\n    Key Features:\n        - Sequential processing of filters\n        - Implements start method for initiating the pipeline\n        - Supports both fit and predict operations\n\n    Usage:\n        ```python\n        from framework3.base import SequentialPipeline, XYData\n        from framework3.plugins.filters import StandardScaler, PCA, LogisticRegression\n\n        pipeline = SequentialPipeline([\n            StandardScaler(),\n            PCA(n_components=5),\n            LogisticRegression()\n        ])\n\n        X_train = XYData.mock(np.random.rand(100, 10))\n        y_train = XYData.mock(np.random.randint(0, 2, 100))\n\n        pipeline.fit(X_train, y_train)\n        predictions = pipeline.predict(X_train)\n        ```\n\n    Methods:\n        start(x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n            Starts the sequential processing of filters in the pipeline.\n        _pre_fit(x: XYData, y: Optional[XYData]) -&gt; Tuple[str, str, str]:\n            Prepares the pipeline for fitting by initializing model attributes and pre-fitting filters.\n        _pre_predict(x: XYData) -&gt; XYData:\n            Prepares the pipeline for prediction by applying pre-predict operations on all filters.\n\n    Note:\n        This class extends BasePipeline and provides a concrete implementation\n        for sequential processing of filters.\n    \"\"\"\n\n    def _pre_fit(self, x: XYData, y: Optional[XYData]):\n        \"\"\"\n        Prepare the pipeline for fitting.\n\n        This method initializes model attributes (hash, path, and string representation)\n        and performs pre-fit operations on all filters in the pipeline.\n\n        Args:\n            x (XYData): The input data for fitting.\n            y (Optional[XYData]): The target data for fitting, if applicable.\n\n        Returns:\n            Tuple[str, str, str]: A tuple containing the model hash, path, and string representation.\n\n        Note:\n            This method is called internally before the actual fit operation and\n            should not be called directly by users.\n        \"\"\"\n        m_hash, m_str = self._get_model_key(\n            data_hash=f'{x._hash}, {y._hash if y is not None else \"\"}'\n        )\n        m_path = f\"{self._get_model_name()}/{m_hash}\"\n\n        self._m_hash = m_hash\n        self._m_path = m_path\n        self._m_str = m_str\n\n        new_x = x\n\n        for filter in self.filters:\n            filter._pre_fit(new_x, y)\n            new_x = filter._pre_predict(new_x)\n\n        return m_hash, m_path, m_str\n\n    def _pre_predict(self, x: XYData):\n        \"\"\"\n        Prepare the pipeline for prediction.\n\n        This method checks if the pipeline has been properly fitted and then\n        applies the pre-predict operation on all filters in the pipeline sequentially.\n\n        Args:\n            x (XYData): The input data for prediction.\n\n        Returns:\n            XYData: The transformed input data after applying all filters' pre-predict operations.\n\n        Raises:\n            ValueError: If the pipeline model has not been trained or loaded.\n\n        Note:\n            This method is called internally before the actual predict operation and\n            should not be called directly by users.\n        \"\"\"\n        if not self._m_hash or not self._m_path or not self._m_str:\n            raise ValueError(\"Cached filter model not trained or loaded\")\n\n        aux_x = x\n        for filter in self.filters:\n            aux_x = filter._pre_predict(aux_x)\n        return aux_x\n</code></pre>"},{"location":"api/base/base_plugin/","title":"Plugins","text":""},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin","title":"<code>framework3.base.base_clases.BasePlugin</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all plugins in the framework.</p> <p>This abstract class provides core functionality for attribute management, serialization, and type checking. It serves as the foundation for all plugin types in the framework, ensuring consistent behavior and interfaces.</p> Key Features <ul> <li>Automatic separation of public and private attributes</li> <li>Type checking for methods using typeguard</li> <li>Inheritance of type annotations from abstract methods</li> <li>JSON serialization and deserialization</li> <li>Rich representation for debugging</li> </ul> Usage <p>To create a new plugin type, inherit from this class and implement the required methods. For example:</p> <pre><code>class MyCustomPlugin(BasePlugin):\n    def __init__(self, param1: int, param2: str):\n        super().__init__(param1=param1, param2=param2)\n\n    def my_method(self):\n        # Custom implementation\n        pass\n</code></pre> <p>Attributes:</p> Name Type Description <code>_public_attributes</code> <code>dict</code> <p>A dictionary containing all public attributes of the plugin.</p> <code>_private_attributes</code> <code>dict</code> <p>A dictionary containing all private attributes of the plugin.</p> <p>Methods:</p> Name Description <code>__new__</code> <p>Creates a new instance of the plugin and applies type checking.</p> <code>__init__</code> <p>Initializes the plugin instance, separating public and private attributes.</p> <code>model_dump</code> <p>Returns a copy of the public attributes.</p> <code>dict</code> <p>Alias for model_dump.</p> <code>json</code> <p>Returns a JSON-encodable representation of the public attributes.</p> <code>item_dump</code> <p>Returns a dictionary representation of the plugin, including its class name and parameters.</p> <code>get_extra</code> <p>Returns a copy of the private attributes.</p> <code>model_validate</code> <p>Validates and creates an instance from a dictionary.</p> Note <p>This class uses the ABC (Abstract Base Class) to define an interface that all plugins must adhere to. It also leverages Python's type hinting and the typeguard library for runtime type checking.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>class BasePlugin(ABC):\n    \"\"\"\n    Base class for all plugins in the framework.\n\n    This abstract class provides core functionality for attribute management,\n    serialization, and type checking. It serves as the foundation for all plugin\n    types in the framework, ensuring consistent behavior and interfaces.\n\n    Key Features:\n        - Automatic separation of public and private attributes\n        - Type checking for methods using typeguard\n        - Inheritance of type annotations from abstract methods\n        - JSON serialization and deserialization\n        - Rich representation for debugging\n\n    Usage:\n        To create a new plugin type, inherit from this class and implement\n        the required methods. For example:\n\n        ```python\n        class MyCustomPlugin(BasePlugin):\n            def __init__(self, param1: int, param2: str):\n                super().__init__(param1=param1, param2=param2)\n\n            def my_method(self):\n                # Custom implementation\n                pass\n        ```\n\n    Attributes:\n        _public_attributes (dict): A dictionary containing all public attributes of the plugin.\n        _private_attributes (dict): A dictionary containing all private attributes of the plugin.\n\n    Methods:\n        __new__(cls, *args, **kwargs):\n            Creates a new instance of the plugin and applies type checking.\n\n        __init__(**kwargs):\n            Initializes the plugin instance, separating public and private attributes.\n\n        model_dump(**kwargs) -&gt; dict:\n            Returns a copy of the public attributes.\n\n        dict(**kwargs) -&gt; dict:\n            Alias for model_dump.\n\n        json(**kwargs) -&gt; dict:\n            Returns a JSON-encodable representation of the public attributes.\n\n        item_dump(include=[], **kwargs) -&gt; Dict[str, Any]:\n            Returns a dictionary representation of the plugin, including its class name and parameters.\n\n        get_extra() -&gt; Dict[str, Any]:\n            Returns a copy of the private attributes.\n\n        model_validate(obj) -&gt; BasePlugin:\n            Validates and creates an instance from a dictionary.\n\n    Note:\n        This class uses the ABC (Abstract Base Class) to define an interface\n        that all plugins must adhere to. It also leverages Python's type hinting\n        and the typeguard library for runtime type checking.\n    \"\"\"\n\n    def __new__(cls: type[Self], *args: Any, **kwargs: Any) -&gt; Self:\n        \"\"\"\n        Create a new instance of the BasePlugin class.\n\n        This method applies type checking to the __init__ method and all other methods,\n        and inherits type annotations from abstract methods in parent classes.\n\n        Args:\n            *args Any: Variable length argument list.\n            **kwargs Any: Arbitrary keyword arguments.\n\n        Returns:\n            BasePlugin: A new instance of the BasePlugin class.\n        \"\"\"\n        instance = super().__new__(cls)\n\n        # Obtener la firma del m\u00e9todo __init__\n        init_signature = inspect.signature(cls.__init__)\n\n        instance.__dict__[\"_public_attributes\"] = {\n            k: v\n            for k, v in kwargs.items()\n            if not k.startswith(\"_\") and k in init_signature.parameters\n        }\n        instance.__dict__[\"_private_attributes\"] = {\n            k: v\n            for k, v in kwargs.items()\n            if k.startswith(\"_\") and k in init_signature.parameters\n        }\n\n        # Apply typechecked to the __init__ method\n        init_method = cls.__init__\n        if init_method is not object.__init__:\n            cls.__init__ = typechecked(init_method)  # type: ignore[method-assign]\n\n        # Inherit type annotations from abstract methods\n        cls.__inherit_annotations()\n\n        # Apply typechecked to all methods defined in the class\n        for attr_name, attr_value in cls.__dict__.items():\n            if inspect.isfunction(attr_value) and attr_name != \"__init__\":\n                setattr(cls, attr_name, typechecked(attr_value))\n\n        return instance\n\n    @classmethod\n    def __inherit_annotations(cls):\n        \"\"\"\n        Inherit type annotations from abstract methods in parent classes.\n\n        This method is responsible for combining type annotations from abstract methods\n        in parent classes with those in the concrete methods of the current class.\n        This ensures that type hints are properly inherited and can be used for\n        type checking and documentation purposes.\n\n        Args:\n            cls (type): The class on which this method is called.\n\n        Note:\n            This method modifies the `__annotations__` attribute of concrete methods\n            in the class, combining them with annotations from corresponding abstract\n            methods in parent classes.\n        \"\"\"\n        for base in cls.__bases__:\n            for name, method in base.__dict__.items():\n                if getattr(method, \"__isabstractmethod__\", False):\n                    if hasattr(cls, name):\n                        concrete_method = getattr(cls, name)\n                        abstract_annotations = get_type_hints(method)\n                        concrete_annotations = get_type_hints(concrete_method)\n                        combined_annotations = {\n                            **abstract_annotations,\n                            **concrete_annotations,\n                        }\n                        setattr(\n                            concrete_method, \"__annotations__\", combined_annotations\n                        )\n\n    def __init__(self, **kwargs: Any):\n        \"\"\"\n        Initialize the BasePlugin instance.\n\n        This method separates public and private attributes based on their naming.\n\n        Args:\n            **kwargs (Any): Arbitrary keyword arguments that will be stored as attributes.\n        \"\"\"\n        self.__dict__[\"_public_attributes\"] = {\n            k: v for k, v in kwargs.items() if not k.startswith(\"_\")\n        }\n        self.__dict__[\"_private_attributes\"] = {\n            k: v for k, v in kwargs.items() if k.startswith(\"_\")\n        }\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"\n        Custom attribute getter that checks both public and private attribute dictionaries.\n\n        Args:\n            name (str): The name of the attribute to retrieve.\n\n        Returns:\n            Any: The value of the requested attribute.\n\n        Raises:\n            AttributeError: If the attribute is not found in either public or private dictionaries.\n        \"\"\"\n        if name in self.__dict__.get(\"_public_attributes\", {}):\n            return self.__dict__[\"_public_attributes\"][name]\n        elif name in self.__dict__.get(\"_private_attributes\", {}):\n            return self.__dict__[\"_private_attributes\"][name]\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setattr__(self, name: str, value: Any):\n        \"\"\"\n        Custom attribute setter that separates public and private attributes.\n\n        Args:\n            name (str): The name of the attribute to set.\n            value (Any): The value to assign to the attribute.\n        \"\"\"\n        if not hasattr(self, \"_private_attributes\"):\n            # During initialization, attributes go directly to __dict__\n            super().__setattr__(name, value)\n        else:\n            if name.startswith(\"_\"):\n                self.__dict__[\"_private_attributes\"][name] = value\n            else:\n                self.__dict__[\"_public_attributes\"][name] = value\n            super().__setattr__(name, value)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the plugin, showing its class name and public attributes.\n\n        Returns:\n            str: A string representation of the plugin.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self._public_attributes})\"\n\n    def model_dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return a copy of the public attributes.\n\n        Args:\n            **kwargs (Any): Additional keyword arguments (not used in this method).\n\n        Returns:\n            Dict[str, Any]: A copy of the public attributes.\n        \"\"\"\n        return self._public_attributes.copy()\n\n    def dict(self, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"\n        Alias for model_dump.\n\n        Args:\n            **kwargs (Any): Additional keyword arguments passed to model_dump.\n\n        Returns:\n            Dict[str, Any]: A copy of the public attributes.\n        \"\"\"\n        return self.model_dump(**kwargs)\n\n    def json(self, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return a JSON-encodable representation of the public attributes.\n\n        Args:\n            **kwargs (Any): Additional keyword arguments passed to jsonable_encoder.\n\n        Returns:\n            Dict[str, Any]: A JSON-encodable representation of the public attributes.\n        \"\"\"\n        return jsonable_encoder(self._public_attributes, **kwargs)\n\n    def item_dump(self, include=[], **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return a dictionary representation of the plugin, including its class name and parameters.\n\n        Args:\n            include (list): A list of private attributes to include in the dump.\n            **kwargs (Any): Additional keyword arguments passed to jsonable_encoder.\n\n        Returns:\n            Dict[str, Any]: A dictionary representation of the plugin.\n        \"\"\"\n        included = {k: v for k, v in self._private_attributes.items() if k in include}\n        dump = {\n            \"clazz\": self.__class__.__name__,\n            \"params\": jsonable_encoder(\n                self._public_attributes,\n                custom_encoder={\n                    BasePlugin: lambda v: v.item_dump(include=include, **kwargs),\n                    type: lambda v: {\"clazz\": v.__name__},\n                    np.integer: lambda x: int(x),\n                    np.floating: lambda x: float(x),\n                },\n                **kwargs,\n            ),\n        }\n        if include != []:\n            dump.update(\n                **jsonable_encoder(\n                    included,\n                    custom_encoder={\n                        BasePlugin: lambda v: v.item_dump(include=include, **kwargs),\n                        type: lambda v: {\"clazz\": v.__name__},\n                        np.integer: lambda x: int(x),\n                        np.floating: lambda x: float(x),\n                    },\n                    **kwargs,\n                )\n            )\n\n        return dump\n\n    def get_extra(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return a copy of the private attributes.\n\n        Returns:\n            Dict[str, Any]: A copy of the private attributes.\n        \"\"\"\n        return self._private_attributes.copy()\n\n    def __getstate__(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Prepare the object for pickling.\n\n        Returns:\n            Dict[str, Any]: A copy of the object's __dict__.\n        \"\"\"\n        state = self.__dict__.copy()\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]):\n        \"\"\"\n        Restore the object from its pickled state.\n\n        Args:\n            state (Dict[str, Any]): The pickled state of the object.\n        \"\"\"\n        self.__dict__.update(state)\n\n    @classmethod\n    def model_validate(cls, obj: object) -&gt; BasePlugin:\n        \"\"\"\n        Validate and create an instance from a dictionary.\n\n        Args:\n            obj (Object): The object to validate and create an instance from.\n\n        Returns:\n            BasePlugin: An instance of the class.\n\n        Raises:\n            ValueError: If the input is not a dictionary.\n        \"\"\"\n        if isinstance(obj, dict):\n            return cls(**obj)\n        raise ValueError(f\"Cannot validate {type(obj)}\")\n\n    def __rich_repr__(self) -&gt; Generator[Any, Any, Any]:\n        \"\"\"\n        Rich representation of the plugin, used by the rich library.\n\n        Yields:\n            Generator[Any, Any, Any]: Key-value pairs of public attributes.\n        \"\"\"\n        for key, value in self._public_attributes.items():\n            yield key, value\n\n    @staticmethod\n    def build_from_dump(\n        dump_dict: Dict[str, Any], factory: BaseFactory[BasePlugin]\n    ) -&gt; BasePlugin | Type[BasePlugin]:\n        \"\"\"\n        Reconstruct a plugin instance from a dumped dictionary representation.\n\n        This method handles nested plugin structures and uses a factory to create instances.\n\n        Args:\n            dump_dict (Dict[str, Any]): The dumped dictionary representation of the plugin.\n            factory (BaseFactory[BasePlugin]): A factory for creating plugin instances.\n\n        Returns:\n            BasePlugin | Type[BasePlugin]: The reconstructed plugin instance or class.\n        \"\"\"\n        level_clazz: Type[BasePlugin] = factory[dump_dict[\"clazz\"]]\n\n        if \"params\" in dump_dict:\n            level_params: Dict[str, Any] = {}\n            for k, v in dump_dict[\"params\"].items():\n                if isinstance(v, dict):\n                    if \"clazz\" in v:\n                        level_params[k] = BasePlugin.build_from_dump(v, factory)\n                    else:\n                        level_params[k] = v\n                elif isinstance(v, list):\n                    level_params[k] = [\n                        BasePlugin.build_from_dump(i, factory) for i in v\n                    ]\n                else:\n                    level_params[k] = v\n            return level_clazz(**level_params)\n        else:\n            return level_clazz\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Custom attribute getter that checks both public and private attribute dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the attribute to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value of the requested attribute.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the attribute is not found in either public or private dictionaries.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Custom attribute getter that checks both public and private attribute dictionaries.\n\n    Args:\n        name (str): The name of the attribute to retrieve.\n\n    Returns:\n        Any: The value of the requested attribute.\n\n    Raises:\n        AttributeError: If the attribute is not found in either public or private dictionaries.\n    \"\"\"\n    if name in self.__dict__.get(\"_public_attributes\", {}):\n        return self.__dict__[\"_public_attributes\"][name]\n    elif name in self.__dict__.get(\"_private_attributes\", {}):\n        return self.__dict__[\"_private_attributes\"][name]\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Prepare the object for pickling.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A copy of the object's dict.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __getstate__(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Prepare the object for pickling.\n\n    Returns:\n        Dict[str, Any]: A copy of the object's __dict__.\n    \"\"\"\n    state = self.__dict__.copy()\n    return state\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__inherit_annotations","title":"<code>__inherit_annotations()</code>  <code>classmethod</code>","text":"<p>Inherit type annotations from abstract methods in parent classes.</p> <p>This method is responsible for combining type annotations from abstract methods in parent classes with those in the concrete methods of the current class. This ensures that type hints are properly inherited and can be used for type checking and documentation purposes.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class on which this method is called.</p> required Note <p>This method modifies the <code>__annotations__</code> attribute of concrete methods in the class, combining them with annotations from corresponding abstract methods in parent classes.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>@classmethod\ndef __inherit_annotations(cls):\n    \"\"\"\n    Inherit type annotations from abstract methods in parent classes.\n\n    This method is responsible for combining type annotations from abstract methods\n    in parent classes with those in the concrete methods of the current class.\n    This ensures that type hints are properly inherited and can be used for\n    type checking and documentation purposes.\n\n    Args:\n        cls (type): The class on which this method is called.\n\n    Note:\n        This method modifies the `__annotations__` attribute of concrete methods\n        in the class, combining them with annotations from corresponding abstract\n        methods in parent classes.\n    \"\"\"\n    for base in cls.__bases__:\n        for name, method in base.__dict__.items():\n            if getattr(method, \"__isabstractmethod__\", False):\n                if hasattr(cls, name):\n                    concrete_method = getattr(cls, name)\n                    abstract_annotations = get_type_hints(method)\n                    concrete_annotations = get_type_hints(concrete_method)\n                    combined_annotations = {\n                        **abstract_annotations,\n                        **concrete_annotations,\n                    }\n                    setattr(\n                        concrete_method, \"__annotations__\", combined_annotations\n                    )\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BasePlugin instance.</p> <p>This method separates public and private attributes based on their naming.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments that will be stored as attributes.</p> <code>{}</code> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"\n    Initialize the BasePlugin instance.\n\n    This method separates public and private attributes based on their naming.\n\n    Args:\n        **kwargs (Any): Arbitrary keyword arguments that will be stored as attributes.\n    \"\"\"\n    self.__dict__[\"_public_attributes\"] = {\n        k: v for k, v in kwargs.items() if not k.startswith(\"_\")\n    }\n    self.__dict__[\"_private_attributes\"] = {\n        k: v for k, v in kwargs.items() if k.startswith(\"_\")\n    }\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Create a new instance of the BasePlugin class.</p> <p>This method applies type checking to the init method and all other methods, and inherits type annotations from abstract methods in parent classes.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BasePlugin</code> <code>Self</code> <p>A new instance of the BasePlugin class.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __new__(cls: type[Self], *args: Any, **kwargs: Any) -&gt; Self:\n    \"\"\"\n    Create a new instance of the BasePlugin class.\n\n    This method applies type checking to the __init__ method and all other methods,\n    and inherits type annotations from abstract methods in parent classes.\n\n    Args:\n        *args Any: Variable length argument list.\n        **kwargs Any: Arbitrary keyword arguments.\n\n    Returns:\n        BasePlugin: A new instance of the BasePlugin class.\n    \"\"\"\n    instance = super().__new__(cls)\n\n    # Obtener la firma del m\u00e9todo __init__\n    init_signature = inspect.signature(cls.__init__)\n\n    instance.__dict__[\"_public_attributes\"] = {\n        k: v\n        for k, v in kwargs.items()\n        if not k.startswith(\"_\") and k in init_signature.parameters\n    }\n    instance.__dict__[\"_private_attributes\"] = {\n        k: v\n        for k, v in kwargs.items()\n        if k.startswith(\"_\") and k in init_signature.parameters\n    }\n\n    # Apply typechecked to the __init__ method\n    init_method = cls.__init__\n    if init_method is not object.__init__:\n        cls.__init__ = typechecked(init_method)  # type: ignore[method-assign]\n\n    # Inherit type annotations from abstract methods\n    cls.__inherit_annotations()\n\n    # Apply typechecked to all methods defined in the class\n    for attr_name, attr_value in cls.__dict__.items():\n        if inspect.isfunction(attr_value) and attr_name != \"__init__\":\n            setattr(cls, attr_name, typechecked(attr_value))\n\n    return instance\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the plugin, showing its class name and public attributes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the plugin.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    String representation of the plugin, showing its class name and public attributes.\n\n    Returns:\n        str: A string representation of the plugin.\n    \"\"\"\n    return f\"{self.__class__.__name__}({self._public_attributes})\"\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__rich_repr__","title":"<code>__rich_repr__()</code>","text":"<p>Rich representation of the plugin, used by the rich library.</p> <p>Yields:</p> Type Description <code>Any</code> <p>Generator[Any, Any, Any]: Key-value pairs of public attributes.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __rich_repr__(self) -&gt; Generator[Any, Any, Any]:\n    \"\"\"\n    Rich representation of the plugin, used by the rich library.\n\n    Yields:\n        Generator[Any, Any, Any]: Key-value pairs of public attributes.\n    \"\"\"\n    for key, value in self._public_attributes.items():\n        yield key, value\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Custom attribute setter that separates public and private attributes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the attribute to set.</p> required <code>value</code> <code>Any</code> <p>The value to assign to the attribute.</p> required Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __setattr__(self, name: str, value: Any):\n    \"\"\"\n    Custom attribute setter that separates public and private attributes.\n\n    Args:\n        name (str): The name of the attribute to set.\n        value (Any): The value to assign to the attribute.\n    \"\"\"\n    if not hasattr(self, \"_private_attributes\"):\n        # During initialization, attributes go directly to __dict__\n        super().__setattr__(name, value)\n    else:\n        if name.startswith(\"_\"):\n            self.__dict__[\"_private_attributes\"][name] = value\n        else:\n            self.__dict__[\"_public_attributes\"][name] = value\n        super().__setattr__(name, value)\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore the object from its pickled state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>The pickled state of the object.</p> required Source code in <code>framework3/base/base_clases.py</code> <pre><code>def __setstate__(self, state: Dict[str, Any]):\n    \"\"\"\n    Restore the object from its pickled state.\n\n    Args:\n        state (Dict[str, Any]): The pickled state of the object.\n    \"\"\"\n    self.__dict__.update(state)\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.build_from_dump","title":"<code>build_from_dump(dump_dict, factory)</code>  <code>staticmethod</code>","text":"<p>Reconstruct a plugin instance from a dumped dictionary representation.</p> <p>This method handles nested plugin structures and uses a factory to create instances.</p> <p>Parameters:</p> Name Type Description Default <code>dump_dict</code> <code>Dict[str, Any]</code> <p>The dumped dictionary representation of the plugin.</p> required <code>factory</code> <code>BaseFactory[BasePlugin]</code> <p>A factory for creating plugin instances.</p> required <p>Returns:</p> Type Description <code>BasePlugin | Type[BasePlugin]</code> <p>BasePlugin | Type[BasePlugin]: The reconstructed plugin instance or class.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>@staticmethod\ndef build_from_dump(\n    dump_dict: Dict[str, Any], factory: BaseFactory[BasePlugin]\n) -&gt; BasePlugin | Type[BasePlugin]:\n    \"\"\"\n    Reconstruct a plugin instance from a dumped dictionary representation.\n\n    This method handles nested plugin structures and uses a factory to create instances.\n\n    Args:\n        dump_dict (Dict[str, Any]): The dumped dictionary representation of the plugin.\n        factory (BaseFactory[BasePlugin]): A factory for creating plugin instances.\n\n    Returns:\n        BasePlugin | Type[BasePlugin]: The reconstructed plugin instance or class.\n    \"\"\"\n    level_clazz: Type[BasePlugin] = factory[dump_dict[\"clazz\"]]\n\n    if \"params\" in dump_dict:\n        level_params: Dict[str, Any] = {}\n        for k, v in dump_dict[\"params\"].items():\n            if isinstance(v, dict):\n                if \"clazz\" in v:\n                    level_params[k] = BasePlugin.build_from_dump(v, factory)\n                else:\n                    level_params[k] = v\n            elif isinstance(v, list):\n                level_params[k] = [\n                    BasePlugin.build_from_dump(i, factory) for i in v\n                ]\n            else:\n                level_params[k] = v\n        return level_clazz(**level_params)\n    else:\n        return level_clazz\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.dict","title":"<code>dict(**kwargs)</code>","text":"<p>Alias for model_dump.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to model_dump.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A copy of the public attributes.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def dict(self, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Alias for model_dump.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments passed to model_dump.\n\n    Returns:\n        Dict[str, Any]: A copy of the public attributes.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.get_extra","title":"<code>get_extra()</code>","text":"<p>Return a copy of the private attributes.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A copy of the private attributes.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def get_extra(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a copy of the private attributes.\n\n    Returns:\n        Dict[str, Any]: A copy of the private attributes.\n    \"\"\"\n    return self._private_attributes.copy()\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.item_dump","title":"<code>item_dump(include=[], **kwargs)</code>","text":"<p>Return a dictionary representation of the plugin, including its class name and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>include</code> <code>list</code> <p>A list of private attributes to include in the dump.</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to jsonable_encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary representation of the plugin.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def item_dump(self, include=[], **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a dictionary representation of the plugin, including its class name and parameters.\n\n    Args:\n        include (list): A list of private attributes to include in the dump.\n        **kwargs (Any): Additional keyword arguments passed to jsonable_encoder.\n\n    Returns:\n        Dict[str, Any]: A dictionary representation of the plugin.\n    \"\"\"\n    included = {k: v for k, v in self._private_attributes.items() if k in include}\n    dump = {\n        \"clazz\": self.__class__.__name__,\n        \"params\": jsonable_encoder(\n            self._public_attributes,\n            custom_encoder={\n                BasePlugin: lambda v: v.item_dump(include=include, **kwargs),\n                type: lambda v: {\"clazz\": v.__name__},\n                np.integer: lambda x: int(x),\n                np.floating: lambda x: float(x),\n            },\n            **kwargs,\n        ),\n    }\n    if include != []:\n        dump.update(\n            **jsonable_encoder(\n                included,\n                custom_encoder={\n                    BasePlugin: lambda v: v.item_dump(include=include, **kwargs),\n                    type: lambda v: {\"clazz\": v.__name__},\n                    np.integer: lambda x: int(x),\n                    np.floating: lambda x: float(x),\n                },\n                **kwargs,\n            )\n        )\n\n    return dump\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.json","title":"<code>json(**kwargs)</code>","text":"<p>Return a JSON-encodable representation of the public attributes.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to jsonable_encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A JSON-encodable representation of the public attributes.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def json(self, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a JSON-encodable representation of the public attributes.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments passed to jsonable_encoder.\n\n    Returns:\n        Dict[str, Any]: A JSON-encodable representation of the public attributes.\n    \"\"\"\n    return jsonable_encoder(self._public_attributes, **kwargs)\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.model_dump","title":"<code>model_dump(**kwargs)</code>","text":"<p>Return a copy of the public attributes.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (not used in this method).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A copy of the public attributes.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a copy of the public attributes.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments (not used in this method).\n\n    Returns:\n        Dict[str, Any]: A copy of the public attributes.\n    \"\"\"\n    return self._public_attributes.copy()\n</code></pre>"},{"location":"api/base/base_plugin/#framework3.base.base_clases.BasePlugin.model_validate","title":"<code>model_validate(obj)</code>  <code>classmethod</code>","text":"<p>Validate and create an instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The object to validate and create an instance from.</p> required <p>Returns:</p> Name Type Description <code>BasePlugin</code> <code>BasePlugin</code> <p>An instance of the class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not a dictionary.</p> Source code in <code>framework3/base/base_clases.py</code> <pre><code>@classmethod\ndef model_validate(cls, obj: object) -&gt; BasePlugin:\n    \"\"\"\n    Validate and create an instance from a dictionary.\n\n    Args:\n        obj (Object): The object to validate and create an instance from.\n\n    Returns:\n        BasePlugin: An instance of the class.\n\n    Raises:\n        ValueError: If the input is not a dictionary.\n    \"\"\"\n    if isinstance(obj, dict):\n        return cls(**obj)\n    raise ValueError(f\"Cannot validate {type(obj)}\")\n</code></pre>"},{"location":"api/base/base_splitter/","title":"Splitter","text":""},{"location":"api/base/base_splitter/#framework3.base.base_splitter","title":"<code>framework3.base.base_splitter</code>","text":""},{"location":"api/base/base_splitter/#framework3.base.base_splitter.__all__","title":"<code>__all__ = ['BaseSplitter']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/base_splitter/#framework3.base.base_splitter.BaseSplitter","title":"<code>BaseSplitter</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>Base class for splitter components in the framework.</p> <p>This abstract class extends BaseFilter and defines the interface for splitter operations. It provides a structure for implementing data splitting strategies for pipelines.</p> Key Features <ul> <li>Abstract methods for starting splitting process, logging metrics, and splitting pipelines</li> <li>Support for verbose output control</li> <li>Integration with optimizer components</li> </ul> Usage <p>To create a new splitter type, inherit from this class and implement the required methods. For example:</p> <pre><code>class MyCustomSplitter(BaseSplitter):\n    def __init__(self, splitting_params):\n        super().__init__()\n        self.splitting_params = splitting_params\n\n    def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n        # Implement splitting start logic\n        pass\n\n    def split(self, pipeline: BaseFilter) -&gt; None:\n        # Implement pipeline splitting logic\n        pass\n\n    def log_metrics(self) -&gt; None:\n        # Implement metric logging logic\n        pass\n\n    def evaluate(self, x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n        # Implement evaluation logic\n        pass\n</code></pre> <p>Attributes:</p> Name Type Description <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be split.</p> <p>Methods:</p> Name Description <code>start</code> <p>XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]: Abstract method to start the splitting process.</p> <code>log_metrics</code> <p>Abstract method to log the metrics of the pipeline.</p> <code>split</code> <p>BaseFilter) -&gt; None: Abstract method to split the given pipeline.</p> <code>verbose</code> <p>bool) -&gt; None: Sets the verbosity level for the splitter and its pipeline.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Abstract method to evaluate the metric based on the provided data.</p> <code>optimizer</code> <p>BaseOptimizer) -&gt; BaseOptimizer: Applies an optimizer to the splitter.</p> <code>unwrap</code> <p>Returns the underlying pipeline.</p> Note <p>This is an abstract base class. Concrete implementations should override the abstract methods to provide specific splitting functionality.</p> Source code in <code>framework3/base/base_splitter.py</code> <pre><code>class BaseSplitter(BaseFilter):\n    \"\"\"\n    Base class for splitter components in the framework.\n\n    This abstract class extends BaseFilter and defines the interface for splitter operations.\n    It provides a structure for implementing data splitting strategies for pipelines.\n\n    Key Features:\n        - Abstract methods for starting splitting process, logging metrics, and splitting pipelines\n        - Support for verbose output control\n        - Integration with optimizer components\n\n    Usage:\n        To create a new splitter type, inherit from this class and implement\n        the required methods. For example:\n\n        ```python\n        class MyCustomSplitter(BaseSplitter):\n            def __init__(self, splitting_params):\n                super().__init__()\n                self.splitting_params = splitting_params\n\n            def start(self, x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n                # Implement splitting start logic\n                pass\n\n            def split(self, pipeline: BaseFilter) -&gt; None:\n                # Implement pipeline splitting logic\n                pass\n\n            def log_metrics(self) -&gt; None:\n                # Implement metric logging logic\n                pass\n\n            def evaluate(self, x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n                # Implement evaluation logic\n                pass\n        ```\n\n    Attributes:\n        pipeline (BaseFilter): The pipeline to be split.\n\n    Methods:\n        start(x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n            Abstract method to start the splitting process.\n\n        log_metrics() -&gt; None:\n            Abstract method to log the metrics of the pipeline.\n\n        split(pipeline: BaseFilter) -&gt; None:\n            Abstract method to split the given pipeline.\n\n        verbose(value: bool) -&gt; None:\n            Sets the verbosity level for the splitter and its pipeline.\n\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Abstract method to evaluate the metric based on the provided data.\n\n        optimizer(optimizer: BaseOptimizer) -&gt; BaseOptimizer:\n            Applies an optimizer to the splitter.\n\n        unwrap() -&gt; BaseFilter:\n            Returns the underlying pipeline.\n\n    Note:\n        This is an abstract base class. Concrete implementations should override\n        the abstract methods to provide specific splitting functionality.\n    \"\"\"\n\n    @abstractmethod\n    def start(\n        self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n    ) -&gt; Optional[XYData]:\n        \"\"\"\n        Start the splitting process.\n\n        This abstract method should be implemented by subclasses to define\n        the specific logic for initiating the splitting process.\n\n        Args:\n            x (XYData): The primary input data.\n            y (Optional[XYData]): Optional target data.\n            X_ (Optional[XYData]): Optional additional input data.\n\n        Returns:\n            Optional[XYData]: The processed data, if any.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def split(self, pipeline: BaseFilter) -&gt; None:\n        \"\"\"\n        Split the given pipeline.\n\n        This abstract method should be implemented by subclasses to define\n        the specific logic for splitting a pipeline.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be split.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n        \"\"\"\n        ...\n\n    def verbose(self, value: bool) -&gt; None:\n        \"\"\"\n        Set the verbosity of the splitter and its pipeline.\n\n        This method controls the verbosity of both the splitter itself and its associated pipeline.\n\n        Args:\n            value (bool): If True, enables verbose output; if False, disables it.\n        \"\"\"\n        self._verbose = value\n        self.pipeline.verbose(value)\n\n    @abstractmethod\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the metric based on the provided data.\n\n        This abstract method should be implemented by subclasses to calculate the specific metric.\n\n        Args:\n            x_data (XYData): The input data used for the prediction.\n            y_true (XYData | None): The ground truth or actual values.\n            y_pred (XYData): The predicted values.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the calculated metric values.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n        \"\"\"\n        ...\n\n    def _pre_fit_wrapp(self, x: XYData, y: Optional[XYData]) -&gt; float | None:\n        \"\"\"\n        Wrapper method for pre-fitting.\n\n        This method calls the original fit method of the pipeline.\n\n        Args:\n            x (XYData): The input data for fitting.\n            y (Optional[XYData]): The target data for fitting, if applicable.\n\n        Returns:\n            float | None: The result of the original fit method.\n        \"\"\"\n        return self._original_fit(x, y)\n\n    def _pre_predict_wrapp(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Wrapper method for pre-prediction.\n\n        This method calls the original predict method of the pipeline.\n\n        Args:\n            x (XYData): The input data for prediction.\n\n        Returns:\n            XYData: The result of the original predict method.\n        \"\"\"\n        return self._original_predict(x)\n\n    def optimizer(self, optimizer: BaseOptimizer) -&gt; BaseOptimizer:\n        \"\"\"\n        Apply an optimizer to the splitter.\n\n        This method allows an optimizer to be applied to the entire splitter.\n\n        Args:\n            optimizer (BaseOptimizer): The optimizer to apply to the splitter.\n\n        Returns:\n            BaseOptimizer: The optimizer after optimization.\n        \"\"\"\n        optimizer.optimize(self)\n        return optimizer\n\n    def unwrap(self) -&gt; BaseFilter:\n        \"\"\"\n        Unwrap the splitter to get the underlying pipeline.\n\n        This method returns the pipeline that the splitter is operating on.\n\n        Returns:\n            BaseFilter: The underlying pipeline.\n        \"\"\"\n        return self.pipeline\n</code></pre>"},{"location":"api/base/base_splitter/#framework3.base.base_splitter.BaseSplitter.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the metric based on the provided data.</p> <p>This abstract method should be implemented by subclasses to calculate the specific metric.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data used for the prediction.</p> required <code>y_true</code> <code>XYData | None</code> <p>The ground truth or actual values.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the calculated metric values.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Source code in <code>framework3/base/base_splitter.py</code> <pre><code>@abstractmethod\ndef evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the metric based on the provided data.\n\n    This abstract method should be implemented by subclasses to calculate the specific metric.\n\n    Args:\n        x_data (XYData): The input data used for the prediction.\n        y_true (XYData | None): The ground truth or actual values.\n        y_pred (XYData): The predicted values.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the calculated metric values.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_splitter/#framework3.base.base_splitter.BaseSplitter.optimizer","title":"<code>optimizer(optimizer)</code>","text":"<p>Apply an optimizer to the splitter.</p> <p>This method allows an optimizer to be applied to the entire splitter.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>BaseOptimizer</code> <p>The optimizer to apply to the splitter.</p> required <p>Returns:</p> Name Type Description <code>BaseOptimizer</code> <code>BaseOptimizer</code> <p>The optimizer after optimization.</p> Source code in <code>framework3/base/base_splitter.py</code> <pre><code>def optimizer(self, optimizer: BaseOptimizer) -&gt; BaseOptimizer:\n    \"\"\"\n    Apply an optimizer to the splitter.\n\n    This method allows an optimizer to be applied to the entire splitter.\n\n    Args:\n        optimizer (BaseOptimizer): The optimizer to apply to the splitter.\n\n    Returns:\n        BaseOptimizer: The optimizer after optimization.\n    \"\"\"\n    optimizer.optimize(self)\n    return optimizer\n</code></pre>"},{"location":"api/base/base_splitter/#framework3.base.base_splitter.BaseSplitter.split","title":"<code>split(pipeline)</code>  <code>abstractmethod</code>","text":"<p>Split the given pipeline.</p> <p>This abstract method should be implemented by subclasses to define the specific logic for splitting a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be split.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Source code in <code>framework3/base/base_splitter.py</code> <pre><code>@abstractmethod\ndef split(self, pipeline: BaseFilter) -&gt; None:\n    \"\"\"\n    Split the given pipeline.\n\n    This abstract method should be implemented by subclasses to define\n    the specific logic for splitting a pipeline.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be split.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_splitter/#framework3.base.base_splitter.BaseSplitter.start","title":"<code>start(x, y, X_)</code>  <code>abstractmethod</code>","text":"<p>Start the splitting process.</p> <p>This abstract method should be implemented by subclasses to define the specific logic for initiating the splitting process.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The primary input data.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Optional target data.</p> required <code>X_</code> <code>Optional[XYData]</code> <p>Optional additional input data.</p> required <p>Returns:</p> Type Description <code>Optional[XYData]</code> <p>Optional[XYData]: The processed data, if any.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Source code in <code>framework3/base/base_splitter.py</code> <pre><code>@abstractmethod\ndef start(\n    self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n) -&gt; Optional[XYData]:\n    \"\"\"\n    Start the splitting process.\n\n    This abstract method should be implemented by subclasses to define\n    the specific logic for initiating the splitting process.\n\n    Args:\n        x (XYData): The primary input data.\n        y (Optional[XYData]): Optional target data.\n        X_ (Optional[XYData]): Optional additional input data.\n\n    Returns:\n        Optional[XYData]: The processed data, if any.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_splitter/#framework3.base.base_splitter.BaseSplitter.unwrap","title":"<code>unwrap()</code>","text":"<p>Unwrap the splitter to get the underlying pipeline.</p> <p>This method returns the pipeline that the splitter is operating on.</p> <p>Returns:</p> Name Type Description <code>BaseFilter</code> <code>BaseFilter</code> <p>The underlying pipeline.</p> Source code in <code>framework3/base/base_splitter.py</code> <pre><code>def unwrap(self) -&gt; BaseFilter:\n    \"\"\"\n    Unwrap the splitter to get the underlying pipeline.\n\n    This method returns the pipeline that the splitter is operating on.\n\n    Returns:\n        BaseFilter: The underlying pipeline.\n    \"\"\"\n    return self.pipeline\n</code></pre>"},{"location":"api/base/base_splitter/#framework3.base.base_splitter.BaseSplitter.verbose","title":"<code>verbose(value)</code>","text":"<p>Set the verbosity of the splitter and its pipeline.</p> <p>This method controls the verbosity of both the splitter itself and its associated pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>If True, enables verbose output; if False, disables it.</p> required Source code in <code>framework3/base/base_splitter.py</code> <pre><code>def verbose(self, value: bool) -&gt; None:\n    \"\"\"\n    Set the verbosity of the splitter and its pipeline.\n\n    This method controls the verbosity of both the splitter itself and its associated pipeline.\n\n    Args:\n        value (bool): If True, enables verbose output; if False, disables it.\n    \"\"\"\n    self._verbose = value\n    self.pipeline.verbose(value)\n</code></pre>"},{"location":"api/base/base_storage/","title":"Storage","text":""},{"location":"api/base/base_storage/#framework3.base.base_storage","title":"<code>framework3.base.base_storage</code>","text":""},{"location":"api/base/base_storage/#framework3.base.base_storage.__all__","title":"<code>__all__ = ['BaseStorage', 'BaseSingleton']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseSingleton","title":"<code>BaseSingleton</code>","text":"<p>A base class for implementing the Singleton pattern.</p> <p>This class ensures that only one instance of each derived class is created.</p> Key Features <ul> <li>Implements the Singleton design pattern</li> <li>Allows derived classes to have only one instance</li> </ul> Usage <p>To create a Singleton class, inherit from BaseSingleton:</p> <pre><code>class MySingleton(BaseSingleton):\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n\n# Usage\ninstance1 = MySingleton()\ninstance2 = MySingleton()\nassert instance1 is instance2  # True\n</code></pre> <p>Attributes:</p> Name Type Description <code>_instances</code> <code>Dict[Type[BaseSingleton], Any]</code> <p>A class-level dictionary to store instances.</p> <p>Methods:</p> Name Description <code>__new__</code> <p>Type[BaseSingleton], args: Any, *kwargs: Any) -&gt; BaseStorage: Creates a new instance or returns the existing one.</p> Note <p>This class should be used as a base class for any class that needs to implement the Singleton pattern.</p> Source code in <code>framework3/base/base_storage.py</code> <pre><code>class BaseSingleton:\n    \"\"\"\n    A base class for implementing the Singleton pattern.\n\n    This class ensures that only one instance of each derived class is created.\n\n    Key Features:\n        - Implements the Singleton design pattern\n        - Allows derived classes to have only one instance\n\n    Usage:\n        To create a Singleton class, inherit from BaseSingleton:\n\n        ```python\n        class MySingleton(BaseSingleton):\n            def __init__(self):\n                self.value = 0\n\n            def increment(self):\n                self.value += 1\n\n        # Usage\n        instance1 = MySingleton()\n        instance2 = MySingleton()\n        assert instance1 is instance2  # True\n        ```\n\n    Attributes:\n        _instances (Dict[Type[BaseSingleton], Any]): A class-level dictionary to store instances.\n\n    Methods:\n        __new__(cls: Type[BaseSingleton], *args: Any, **kwargs: Any) -&gt; BaseStorage:\n            Creates a new instance or returns the existing one.\n\n    Note:\n        This class should be used as a base class for any class that needs to implement\n        the Singleton pattern.\n    \"\"\"\n\n    _instances: Dict[Type[BaseSingleton], Any] = {}\n    _verbose: bool = True\n\n    def __new__(cls: Type[BaseSingleton], *args: Any, **kwargs: Any) -&gt; BaseStorage:\n        \"\"\"\n        Create a new instance of the class if it doesn't exist, otherwise return the existing instance.\n\n        This method implements the core logic of the Singleton pattern.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            BaseStorage: The single instance of the class.\n\n        Note:\n            This method is called before __init__ when creating a new instance of the class.\n        \"\"\"\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__new__(cls)  # type: ignore\n        return cls._instances[cls]\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseSingleton.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Create a new instance of the class if it doesn't exist, otherwise return the existing instance.</p> <p>This method implements the core logic of the Singleton pattern.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseStorage</code> <code>BaseStorage</code> <p>The single instance of the class.</p> Note <p>This method is called before init when creating a new instance of the class.</p> Source code in <code>framework3/base/base_storage.py</code> <pre><code>def __new__(cls: Type[BaseSingleton], *args: Any, **kwargs: Any) -&gt; BaseStorage:\n    \"\"\"\n    Create a new instance of the class if it doesn't exist, otherwise return the existing instance.\n\n    This method implements the core logic of the Singleton pattern.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        BaseStorage: The single instance of the class.\n\n    Note:\n        This method is called before __init__ when creating a new instance of the class.\n    \"\"\"\n    if cls not in cls._instances:\n        cls._instances[cls] = super().__new__(cls)  # type: ignore\n    return cls._instances[cls]\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage","title":"<code>BaseStorage</code>","text":"<p>               Bases: <code>BasePlugin</code>, <code>BaseSingleton</code></p> <p>An abstract base class for storage operations.</p> <p>This class defines the interface for storage-related operations and inherits from BasePlugin for plugin functionality and BaseSingleton for single instance behavior.</p> Key Features <ul> <li>Abstract methods for common storage operations</li> <li>Singleton behavior ensures only one instance per storage type</li> <li>Inherits plugin functionality from BasePlugin</li> </ul> Usage <p>To create a new storage type, inherit from BaseStorage and implement all abstract methods:</p> <pre><code>class MyCustomStorage(BaseStorage):\n    def __init__(self, root_path: str):\n        self.root_path = root_path\n\n    def get_root_path(self) -&gt; str:\n        return self.root_path\n\n    def upload_file(self, file, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None:\n        # Implement file upload logic\n        ...\n\n    # Implement other abstract methods\n    ...\n\n# Usage\nstorage = MyCustomStorage(\"/path/to/storage\")\nstorage.upload_file(file_object, \"example.txt\", \"documents\")\n</code></pre> <p>Methods:</p> Name Description <code>get_root_path</code> <p>Abstract method to get the root path of the storage.</p> <code>upload_file</code> <p>object, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None: Abstract method to upload a file to the storage.</p> <code>download_file</code> <p>str, context: str) -&gt; Any: Abstract method to download a file from the storage.</p> <code>list_stored_files</code> <p>str) -&gt; List[Any]: Abstract method to list all files stored in a specific context.</p> <code>get_file_by_hashcode</code> <p>str, context: str) -&gt; Any: Abstract method to retrieve a file by its hashcode.</p> <code>check_if_exists</code> <p>str, context: str) -&gt; bool: Abstract method to check if a file exists in the storage.</p> <code>delete_file</code> <p>str, context: str): Abstract method to delete a file from the storage.</p> Note <p>This is an abstract base class. Concrete implementations should override all abstract methods to provide specific storage functionality.</p> Source code in <code>framework3/base/base_storage.py</code> <pre><code>class BaseStorage(BasePlugin, BaseSingleton):\n    \"\"\"\n    An abstract base class for storage operations.\n\n    This class defines the interface for storage-related operations and inherits\n    from BasePlugin for plugin functionality and BaseSingleton for single instance behavior.\n\n    Key Features:\n        - Abstract methods for common storage operations\n        - Singleton behavior ensures only one instance per storage type\n        - Inherits plugin functionality from BasePlugin\n\n    Usage:\n        To create a new storage type, inherit from BaseStorage and implement all abstract methods:\n\n        ```python\n        class MyCustomStorage(BaseStorage):\n            def __init__(self, root_path: str):\n                self.root_path = root_path\n\n            def get_root_path(self) -&gt; str:\n                return self.root_path\n\n            def upload_file(self, file, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None:\n                # Implement file upload logic\n                ...\n\n            # Implement other abstract methods\n            ...\n\n        # Usage\n        storage = MyCustomStorage(\"/path/to/storage\")\n        storage.upload_file(file_object, \"example.txt\", \"documents\")\n        ```\n\n    Methods:\n        get_root_path() -&gt; str:\n            Abstract method to get the root path of the storage.\n        upload_file(file: object, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None:\n            Abstract method to upload a file to the storage.\n        download_file(hashcode: str, context: str) -&gt; Any:\n            Abstract method to download a file from the storage.\n        list_stored_files(context: str) -&gt; List[Any]:\n            Abstract method to list all files stored in a specific context.\n        get_file_by_hashcode(hashcode: str, context: str) -&gt; Any:\n            Abstract method to retrieve a file by its hashcode.\n        check_if_exists(hashcode: str, context: str) -&gt; bool:\n            Abstract method to check if a file exists in the storage.\n        delete_file(hashcode: str, context: str):\n            Abstract method to delete a file from the storage.\n\n    Note:\n        This is an abstract base class. Concrete implementations should override\n        all abstract methods to provide specific storage functionality.\n    \"\"\"\n\n    @abstractmethod\n    def get_root_path(self) -&gt; str:\n        \"\"\"\n        Get the root path of the storage.\n\n        This method should be implemented to return the base directory or path\n        where the storage system keeps its files.\n\n        Returns:\n            str: The root path of the storage.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def get_root_path(self) -&gt; str:\n                return \"/var/data/storage\"\n            ```\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def upload_file(\n        self, file: object, file_name: str, context: str, direct_stream: bool = False\n    ) -&gt; str | None:\n        \"\"\"\n        Upload a file to the storage.\n\n        This method should be implemented to handle file uploads to the storage system.\n\n        Args:\n            file (object): The file object to upload.\n            file_name (str): The name of the file.\n            context (str): The context or directory for the file.\n            direct_stream (bool, optional): Whether to use direct streaming. Defaults to False.\n\n        Returns:\n            str | None: The identifier of the uploaded file, or None if upload failed.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def upload_file(self, file: object, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None:\n                path = os.path.join(self.get_root_path(), context, file_name)\n                with open(path, 'wb') as f:\n                    f.write(file.read())\n                return file_name\n            ```\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def download_file(self, hashcode: str, context: str) -&gt; Any:\n        \"\"\"\n        Download a file from the storage.\n\n        This method should be implemented to retrieve files from the storage system.\n\n        Args:\n            hashcode (str): The identifier of the file to download.\n            context (str): The context or directory of the file.\n\n        Returns:\n            Any: The downloaded file object.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def download_file(self, hashcode: str, context: str) -&gt; Any:\n                path = os.path.join(self.get_root_path(), context, hashcode)\n                with open(path, 'rb') as f:\n                    return f.read()\n            ```\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def list_stored_files(self, context: str) -&gt; List[Any]:\n        \"\"\"\n        List all files stored in a specific context.\n\n        This method should be implemented to return a list of files in a given context.\n\n        Args:\n            context (str): The context or directory to list files from.\n\n        Returns:\n            List[Any]: A list of file objects or file information.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def list_stored_files(self, context: str) -&gt; List[Any]:\n                path = os.path.join(self.get_root_path(), context)\n                return os.listdir(path)\n            ```\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; Any:\n        \"\"\"\n        Retrieve a file by its hashcode.\n\n        This method should be implemented to fetch a specific file using its identifier.\n\n        Args:\n            hashcode (str): The identifier of the file.\n            context (str): The context or directory of the file.\n\n        Returns:\n            Any: The file object or file information.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; Any:\n                return self.download_file(hashcode, context)\n            ```\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the storage.\n\n        This method should be implemented to verify the existence of a file.\n\n        Args:\n            hashcode (str): The identifier of the file.\n            context (str): The context or directory of the file.\n\n        Returns:\n            bool: True if the file exists, False otherwise.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n                path = os.path.join(self.get_root_path(), context, hashcode)\n                return os.path.exists(path)\n            ```\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def delete_file(self, hashcode: str, context: str):\n        \"\"\"\n        Delete a file from the storage.\n\n        This method should be implemented to remove a file from the storage system.\n\n        Args:\n            hashcode (str): The identifier of the file to delete.\n            context (str): The context or directory of the file.\n\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n\n        Example:\n            ```python\n            def delete_file(self, hashcode: str, context: str):\n                path = os.path.join(self.get_root_path(), context, hashcode)\n                if os.path.exists(path):\n                    os.remove(path)\n                else:\n                    raise FileNotFoundError(f\"File {hashcode} not found in {context}\")\n            ```\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage.check_if_exists","title":"<code>check_if_exists(hashcode, context)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the storage.</p> <p>This method should be implemented to verify the existence of a file.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The identifier of the file.</p> required <code>context</code> <code>str</code> <p>The context or directory of the file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file exists, False otherwise.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n    path = os.path.join(self.get_root_path(), context, hashcode)\n    return os.path.exists(path)\n</code></pre> Source code in <code>framework3/base/base_storage.py</code> <pre><code>@abstractmethod\ndef check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the storage.\n\n    This method should be implemented to verify the existence of a file.\n\n    Args:\n        hashcode (str): The identifier of the file.\n        context (str): The context or directory of the file.\n\n    Returns:\n        bool: True if the file exists, False otherwise.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n            path = os.path.join(self.get_root_path(), context, hashcode)\n            return os.path.exists(path)\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage.delete_file","title":"<code>delete_file(hashcode, context)</code>  <code>abstractmethod</code>","text":"<p>Delete a file from the storage.</p> <p>This method should be implemented to remove a file from the storage system.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The identifier of the file to delete.</p> required <code>context</code> <code>str</code> <p>The context or directory of the file.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def delete_file(self, hashcode: str, context: str):\n    path = os.path.join(self.get_root_path(), context, hashcode)\n    if os.path.exists(path):\n        os.remove(path)\n    else:\n        raise FileNotFoundError(f\"File {hashcode} not found in {context}\")\n</code></pre> Source code in <code>framework3/base/base_storage.py</code> <pre><code>@abstractmethod\ndef delete_file(self, hashcode: str, context: str):\n    \"\"\"\n    Delete a file from the storage.\n\n    This method should be implemented to remove a file from the storage system.\n\n    Args:\n        hashcode (str): The identifier of the file to delete.\n        context (str): The context or directory of the file.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def delete_file(self, hashcode: str, context: str):\n            path = os.path.join(self.get_root_path(), context, hashcode)\n            if os.path.exists(path):\n                os.remove(path)\n            else:\n                raise FileNotFoundError(f\"File {hashcode} not found in {context}\")\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage.download_file","title":"<code>download_file(hashcode, context)</code>  <code>abstractmethod</code>","text":"<p>Download a file from the storage.</p> <p>This method should be implemented to retrieve files from the storage system.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The identifier of the file to download.</p> required <code>context</code> <code>str</code> <p>The context or directory of the file.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The downloaded file object.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def download_file(self, hashcode: str, context: str) -&gt; Any:\n    path = os.path.join(self.get_root_path(), context, hashcode)\n    with open(path, 'rb') as f:\n        return f.read()\n</code></pre> Source code in <code>framework3/base/base_storage.py</code> <pre><code>@abstractmethod\ndef download_file(self, hashcode: str, context: str) -&gt; Any:\n    \"\"\"\n    Download a file from the storage.\n\n    This method should be implemented to retrieve files from the storage system.\n\n    Args:\n        hashcode (str): The identifier of the file to download.\n        context (str): The context or directory of the file.\n\n    Returns:\n        Any: The downloaded file object.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def download_file(self, hashcode: str, context: str) -&gt; Any:\n            path = os.path.join(self.get_root_path(), context, hashcode)\n            with open(path, 'rb') as f:\n                return f.read()\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage.get_file_by_hashcode","title":"<code>get_file_by_hashcode(hashcode, context)</code>  <code>abstractmethod</code>","text":"<p>Retrieve a file by its hashcode.</p> <p>This method should be implemented to fetch a specific file using its identifier.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The identifier of the file.</p> required <code>context</code> <code>str</code> <p>The context or directory of the file.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The file object or file information.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; Any:\n    return self.download_file(hashcode, context)\n</code></pre> Source code in <code>framework3/base/base_storage.py</code> <pre><code>@abstractmethod\ndef get_file_by_hashcode(self, hashcode: str, context: str) -&gt; Any:\n    \"\"\"\n    Retrieve a file by its hashcode.\n\n    This method should be implemented to fetch a specific file using its identifier.\n\n    Args:\n        hashcode (str): The identifier of the file.\n        context (str): The context or directory of the file.\n\n    Returns:\n        Any: The file object or file information.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; Any:\n            return self.download_file(hashcode, context)\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage.get_root_path","title":"<code>get_root_path()</code>  <code>abstractmethod</code>","text":"<p>Get the root path of the storage.</p> <p>This method should be implemented to return the base directory or path where the storage system keeps its files.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The root path of the storage.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def get_root_path(self) -&gt; str:\n    return \"/var/data/storage\"\n</code></pre> Source code in <code>framework3/base/base_storage.py</code> <pre><code>@abstractmethod\ndef get_root_path(self) -&gt; str:\n    \"\"\"\n    Get the root path of the storage.\n\n    This method should be implemented to return the base directory or path\n    where the storage system keeps its files.\n\n    Returns:\n        str: The root path of the storage.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def get_root_path(self) -&gt; str:\n            return \"/var/data/storage\"\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage.list_stored_files","title":"<code>list_stored_files(context)</code>  <code>abstractmethod</code>","text":"<p>List all files stored in a specific context.</p> <p>This method should be implemented to return a list of files in a given context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>The context or directory to list files from.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: A list of file objects or file information.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def list_stored_files(self, context: str) -&gt; List[Any]:\n    path = os.path.join(self.get_root_path(), context)\n    return os.listdir(path)\n</code></pre> Source code in <code>framework3/base/base_storage.py</code> <pre><code>@abstractmethod\ndef list_stored_files(self, context: str) -&gt; List[Any]:\n    \"\"\"\n    List all files stored in a specific context.\n\n    This method should be implemented to return a list of files in a given context.\n\n    Args:\n        context (str): The context or directory to list files from.\n\n    Returns:\n        List[Any]: A list of file objects or file information.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def list_stored_files(self, context: str) -&gt; List[Any]:\n            path = os.path.join(self.get_root_path(), context)\n            return os.listdir(path)\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_storage/#framework3.base.base_storage.BaseStorage.upload_file","title":"<code>upload_file(file, file_name, context, direct_stream=False)</code>  <code>abstractmethod</code>","text":"<p>Upload a file to the storage.</p> <p>This method should be implemented to handle file uploads to the storage system.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>object</code> <p>The file object to upload.</p> required <code>file_name</code> <code>str</code> <p>The name of the file.</p> required <code>context</code> <code>str</code> <p>The context or directory for the file.</p> required <code>direct_stream</code> <code>bool</code> <p>Whether to use direct streaming. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The identifier of the uploaded file, or None if upload failed.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not implement this method.</p> Example <pre><code>def upload_file(self, file: object, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None:\n    path = os.path.join(self.get_root_path(), context, file_name)\n    with open(path, 'wb') as f:\n        f.write(file.read())\n    return file_name\n</code></pre> Source code in <code>framework3/base/base_storage.py</code> <pre><code>@abstractmethod\ndef upload_file(\n    self, file: object, file_name: str, context: str, direct_stream: bool = False\n) -&gt; str | None:\n    \"\"\"\n    Upload a file to the storage.\n\n    This method should be implemented to handle file uploads to the storage system.\n\n    Args:\n        file (object): The file object to upload.\n        file_name (str): The name of the file.\n        context (str): The context or directory for the file.\n        direct_stream (bool, optional): Whether to use direct streaming. Defaults to False.\n\n    Returns:\n        str | None: The identifier of the uploaded file, or None if upload failed.\n\n    Raises:\n        NotImplementedError: If the subclass does not implement this method.\n\n    Example:\n        ```python\n        def upload_file(self, file: object, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None:\n            path = os.path.join(self.get_root_path(), context, file_name)\n            with open(path, 'wb') as f:\n                f.write(file.read())\n            return file_name\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/base/base_types/","title":"Types","text":""},{"location":"api/base/base_types/#framework3.base.base_types","title":"<code>framework3.base.base_types</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.Float","title":"<code>Float = float | np.float16 | np.float32 | np.float64</code>  <code>module-attribute</code>","text":"<p>Type alias for float values, including numpy float types.</p>"},{"location":"api/base/base_types/#framework3.base.base_types.IncEx","title":"<code>IncEx = 'set[int] | set[str] | dict[int, Any] | dict[str, Any] | None'</code>  <code>module-attribute</code>","text":"<p>Type alias for inclusion/exclusion specifications in data processing.</p>"},{"location":"api/base/base_types/#framework3.base.base_types.SkVData","title":"<code>SkVData = np.ndarray | pd.DataFrame | spmatrix | csr_matrix</code>  <code>module-attribute</code>","text":"<p>Type alias for scikit-learn compatible data structures.</p>"},{"location":"api/base/base_types/#framework3.base.base_types.TxyData","title":"<code>TxyData = TypeVar('TxyData', SkVData, VData)</code>  <code>module-attribute</code>","text":"<p>Type variable constrained to SkVData or VData for use in XYData.</p>"},{"location":"api/base/base_types/#framework3.base.base_types.TypePlugable","title":"<code>TypePlugable = TypeVar('TypePlugable')</code>  <code>module-attribute</code>","text":"<p>Generic type variable for pluggable types in the framework.</p>"},{"location":"api/base/base_types/#framework3.base.base_types.VData","title":"<code>VData = np.ndarray | pd.DataFrame | spmatrix | list | torch.Tensor</code>  <code>module-attribute</code>","text":"<p>Type alias for various data structures used in the framework.</p>"},{"location":"api/base/base_types/#framework3.base.base_types.__all__","title":"<code>__all__ = ['XYData', 'VData', 'SkVData', 'IncEx', 'TypePlugable']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.JsonEncoderkwargs","title":"<code>JsonEncoderkwargs</code>","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>framework3/base/base_types.py</code> <pre><code>class JsonEncoderkwargs(TypedDict, total=False):\n    exclude: IncEx | None\n    by_alias: bool\n    exclude_unset: bool\n    exclude_defaults: bool\n    exclude_none: bool\n    sqlalchemy_safe: bool\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.JsonEncoderkwargs.by_alias","title":"<code>by_alias</code>  <code>instance-attribute</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.JsonEncoderkwargs.exclude","title":"<code>exclude</code>  <code>instance-attribute</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.JsonEncoderkwargs.exclude_defaults","title":"<code>exclude_defaults</code>  <code>instance-attribute</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.JsonEncoderkwargs.exclude_none","title":"<code>exclude_none</code>  <code>instance-attribute</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.JsonEncoderkwargs.exclude_unset","title":"<code>exclude_unset</code>  <code>instance-attribute</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.JsonEncoderkwargs.sqlalchemy_safe","title":"<code>sqlalchemy_safe</code>  <code>instance-attribute</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.XYData","title":"<code>XYData</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TxyData]</code></p> <p>A dataclass representing data for machine learning tasks, typically features (X) or targets (Y).</p> <p>This class is immutable and uses slots for memory efficiency. It provides a standardized way to handle various types of data used in machine learning pipelines.</p> <p>Attributes:</p> Name Type Description <code>_hash</code> <code>str</code> <p>A unique identifier or hash for the data.</p> <code>_path</code> <code>str</code> <p>The path where the data is stored or retrieved from.</p> <code>_value</code> <code>TxyData | Callable[..., TxyData]</code> <p>The actual data or a callable that returns the data.</p> <p>Methods:</p> Name Description <code>train_test_split</code> <p>Split the data into training and testing sets.</p> <code>split</code> <p>Create a new XYData instance with specified indices.</p> <code>mock</code> <p>Create a mock XYData instance for testing or placeholder purposes.</p> <code>concat</code> <p>Concatenate a list of data along the specified axis.</p> <code>ensure_dim</code> <p>Ensure the input data has at least two dimensions.</p> <code>as_iterable</code> <p>Convert the data to an iterable form.</p> Example <pre><code>import numpy as np\nfrom framework3.base.base_types import XYData\n\n# Create a mock XYData instance with random data\nfeatures = np.random.rand(100, 5)\nlabels = np.random.randint(0, 2, 100)\n\nx_data = XYData.mock(features, hash=\"feature_data\", path=\"/data/features\")\ny_data = XYData.mock(labels, hash=\"label_data\", path=\"/data/labels\")\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = x_data.train_test_split(x_data.value, y_data.value, test_size=0.2)\n\n# Access the data\nprint(f\"Training features shape: {X_train.value.shape}\")\nprint(f\"Training labels shape: {y_train.value.shape}\")\n\n# Create a subset of the data\nsubset = x_data.split(range(50))\nprint(f\"Subset shape: {subset.value.shape}\")\n</code></pre> Note <p>This class is designed to work with various data types including numpy arrays, pandas DataFrames, scipy sparse matrices, and PyTorch tensors.</p> Source code in <code>framework3/base/base_types.py</code> <pre><code>@dataclass(slots=True)\nclass XYData(Generic[TxyData]):\n    \"\"\"\n    A dataclass representing data for machine learning tasks, typically features (X) or targets (Y).\n\n    This class is immutable and uses slots for memory efficiency. It provides a standardized\n    way to handle various types of data used in machine learning pipelines.\n\n    Attributes:\n        _hash (str): A unique identifier or hash for the data.\n        _path (str): The path where the data is stored or retrieved from.\n        _value (TxyData | Callable[..., TxyData]): The actual data or a callable that returns the data.\n\n    Methods:\n        train_test_split: Split the data into training and testing sets.\n        split: Create a new XYData instance with specified indices.\n        mock: Create a mock XYData instance for testing or placeholder purposes.\n        concat: Concatenate a list of data along the specified axis.\n        ensure_dim: Ensure the input data has at least two dimensions.\n        as_iterable: Convert the data to an iterable form.\n\n    Example:\n        ```python\n        import numpy as np\n        from framework3.base.base_types import XYData\n\n        # Create a mock XYData instance with random data\n        features = np.random.rand(100, 5)\n        labels = np.random.randint(0, 2, 100)\n\n        x_data = XYData.mock(features, hash=\"feature_data\", path=\"/data/features\")\n        y_data = XYData.mock(labels, hash=\"label_data\", path=\"/data/labels\")\n\n        # Split the data into training and testing sets\n        X_train, X_test, y_train, y_test = x_data.train_test_split(x_data.value, y_data.value, test_size=0.2)\n\n        # Access the data\n        print(f\"Training features shape: {X_train.value.shape}\")\n        print(f\"Training labels shape: {y_train.value.shape}\")\n\n        # Create a subset of the data\n        subset = x_data.split(range(50))\n        print(f\"Subset shape: {subset.value.shape}\")\n        ```\n\n    Note:\n        This class is designed to work with various data types including numpy arrays,\n        pandas DataFrames, scipy sparse matrices, and PyTorch tensors.\n    \"\"\"\n\n    _hash: str = field(init=True)\n    _path: str = field(init=True)\n    _value: TxyData | Callable[..., TxyData] = field(init=True, repr=False)\n\n    def train_test_split(\n        self, x: TxyData, y: TxyData | None, test_size: float, random_state: int = 42\n    ) -&gt; Tuple[XYData, XYData, XYData, XYData]:\n        \"\"\"\n        Split the data into training and testing sets.\n\n        This method uses sklearn's train_test_split function to divide the data\n        into training and testing sets for both features (X) and targets (Y).\n\n        Args:\n            x (TxyData): The feature data to split.\n            y (TxyData | None): The target data to split. Can be None for unsupervised learning.\n            test_size (float): The proportion of the data to include in the test split (0.0 to 1.0).\n            random_state (int, optional): Seed for the random number generator. Defaults to 42.\n\n        Returns:\n            Tuple[XYData, XYData, XYData, XYData]: A tuple containing (X_train, X_test, y_train, y_test),\n            each wrapped in an XYData instance.\n\n        Example:\n            ```python\n            data = XYData.mock(np.random.rand(100, 5))\n            labels = XYData.mock(np.random.randint(0, 2, 100))\n            X_train, X_test, y_train, y_test = data.train_test_split(data.value, labels.value, test_size=0.2)\n            ```\n        \"\"\"\n        X_train, X_test, y_train, y_test = train_test_split(\n            x, y, test_size=test_size, random_state=random_state\n        )\n\n        return (\n            XYData.mock(X_train, hash=f\"{self._hash} X train\", path=\"/dataset\"),\n            XYData.mock(X_test, hash=f\"{self._hash} X test\", path=\"/dataset\"),\n            XYData.mock(y_train, hash=f\"{self._hash} y train\", path=\"/dataset\"),\n            XYData.mock(y_test, hash=f\"{self._hash} y test\", path=\"/dataset\"),\n        )\n\n    def split(self, indices: Iterable[int]) -&gt; XYData:\n        \"\"\"\n        Split the data into a new XYData instance with the specified indices.\n\n        This method creates a new XYData instance containing only the data\n        corresponding to the provided indices.\n\n        Args:\n            indices (Iterable[int]): The indices to select from the data.\n\n        Returns:\n            XYData: A new XYData instance containing the selected data.\n\n        Example:\n            ```python\n            data = XYData.mock(np.random.rand(100, 5))\n            subset = data.split(range(50, 100))  # Select second half of the data\n            ```\n        \"\"\"\n\n        def split_data(self, indices: Iterable[int]) -&gt; VData:\n            value = self.value\n\n            match value:\n                case np.ndarray():\n                    return value[list(indices)]\n\n                case list():\n                    return [value[i] for i in indices]\n\n                case torch.Tensor():\n                    return value[list(indices)]\n\n                # mypy: disable-next-line[misc]\n                # case spmatrix():\n                #     return cast(spmatrix, csr_matrix(value)[indices])\n\n                case pd.DataFrame():\n                    return value.iloc[list(indices)]\n\n                case _:\n                    if isinstance(value, spmatrix):\n                        return cast(spmatrix, csr_matrix(value)[indices])\n                    raise TypeError(\n                        f\"Unsupported data type for splitting: {type(value)}\"\n                    )\n\n        indices_hash = hashlib.sha1(str(list(indices)).encode()).hexdigest()\n        return XYData(\n            _hash=f\"{self._hash}[{indices_hash}]\",\n            _path=self._path,\n            _value=lambda: split_data(self, indices),\n        )\n\n    @staticmethod\n    def mock(\n        value: TxyData | Callable[..., TxyData],\n        hash: str | None = None,\n        path: str | None = None,\n    ) -&gt; XYData:\n        \"\"\"\n        Create a mock XYData instance for testing or placeholder purposes.\n\n        This static method allows for easy creation of XYData instances,\n        particularly useful in testing scenarios or when placeholder data is needed.\n\n        Args:\n            value (TxyData | Callable[..., TxyData]): The data or a callable that returns the data.\n            hash (str | None, optional): A hash string for the data. Defaults to \"Mock\" if None.\n            path (str | None, optional): A path string for the data. Defaults to \"/tmp\" if None.\n\n        Returns:\n            XYData: A new XYData instance with the provided or default values.\n\n        Example:\n            ```python\n            mock_data = XYData.mock(np.random.rand(10, 5), hash=\"test_data\", path=\"/data/test\")\n            ```\n        \"\"\"\n        if hash is None:\n            hash = \"Mock\"\n\n        if path is None:\n            path = \"/tmp\"\n\n        return XYData(_hash=hash, _path=path, _value=value)\n\n    @property\n    def value(self) -&gt; TxyData:\n        \"\"\"\n        Property to access the actual data.\n\n        This property ensures that if _value is a callable, it is called to retrieve the data.\n        Otherwise, it returns the data directly.\n\n        Returns:\n            TxyData: The actual data (numpy array, pandas DataFrame, scipy sparse matrix, etc.).\n\n        Note:\n            This property may modify the _value attribute if it's initially a callable.\n        \"\"\"\n        self._value = self._value() if callable(self._value) else self._value\n        return self._value\n\n    @staticmethod\n    def concat(x: list[TxyData], axis: int = -1) -&gt; XYData:\n        \"\"\"\n        Concatenate a list of data along the specified axis.\n\n        This static method handles concatenation for various data types,\n        including sparse matrices and other array-like structures.\n\n        Args:\n            x (list[TxyData]): List of data to concatenate.\n            axis (int, optional): Axis along which to concatenate. Defaults to -1.\n\n        Returns:\n            XYData: A new XYData instance with the concatenated data.\n\n        Raises:\n            ValueError: If an invalid axis is specified for sparse matrix concatenation.\n\n        Example:\n            ```python\n            data1 = np.random.rand(10, 5)\n            data2 = np.random.rand(10, 5)\n            combined = XYData.concat([data1, data2], axis=1)\n            ```\n        \"\"\"\n        if all(isinstance(item, spmatrix) for item in x):\n            if axis == 1:\n                return XYData.mock(value=cast(spmatrix, hstack(x)))\n            elif axis == 0:\n                return XYData.mock(value=cast(spmatrix, vstack(x)))\n            raise ValueError(\"Invalid axis for concatenating sparse matrices\")\n        return concat(x, axis=axis)\n\n    @staticmethod\n    def ensure_dim(x: list | np.ndarray) -&gt; list | np.ndarray:\n        \"\"\"\n        Ensure the input data has at least two dimensions.\n\n        This static method is a wrapper around the ensure_dim function,\n        which adds a new axis to 1D arrays or lists.\n\n        Args:\n            x (list | np.ndarray): Input data to ensure dimensions.\n\n        Returns:\n            list | np.ndarray: Data with at least two dimensions.\n\n        Example:\n            ```python\n            data = [1, 2, 3, 4, 5]\n            two_dim_data = XYData.ensure_dim(data)\n            ```\n        \"\"\"\n        return ensure_dim(x)\n\n    def as_iterable(self) -&gt; Iterable:\n        \"\"\"\n        Convert the `_value` attribute to an iterable, regardless of its underlying type.\n\n        This method provides a consistent way to iterate over the data,\n        handling different data types appropriately.\n\n        Returns:\n            Iterable: An iterable version of `_value`.\n\n        Raises:\n            TypeError: If the value type is not compatible with iteration.\n\n        Example:\n            ```python\n            data = XYData.mock(np.random.rand(10, 5))\n            for item in data.as_iterable():\n                print(item)\n            ```\n        \"\"\"\n        value = self.value\n\n        # Maneja diferentes tipos de datos\n        if isinstance(value, np.ndarray):\n            return value  # Los arrays numpy ya son iterables\n        elif isinstance(value, pd.DataFrame):\n            return value.iterrows()  # Devuelve un iterable sobre las filas\n        elif isinstance(value, spmatrix):\n            return value.toarray()  # type: ignore # Convierte la matriz dispersa a un array denso\n        elif isinstance(value, torch.Tensor):\n            return value\n        else:\n            raise TypeError(f\"El tipo {type(value)} no es compatible con iteraci\u00f3n.\")\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.XYData.value","title":"<code>value</code>  <code>property</code>","text":"<p>Property to access the actual data.</p> <p>This property ensures that if _value is a callable, it is called to retrieve the data. Otherwise, it returns the data directly.</p> <p>Returns:</p> Name Type Description <code>TxyData</code> <code>TxyData</code> <p>The actual data (numpy array, pandas DataFrame, scipy sparse matrix, etc.).</p> Note <p>This property may modify the _value attribute if it's initially a callable.</p>"},{"location":"api/base/base_types/#framework3.base.base_types.XYData.__init__","title":"<code>__init__(_hash, _path, _value)</code>","text":""},{"location":"api/base/base_types/#framework3.base.base_types.XYData.as_iterable","title":"<code>as_iterable()</code>","text":"<p>Convert the <code>_value</code> attribute to an iterable, regardless of its underlying type.</p> <p>This method provides a consistent way to iterate over the data, handling different data types appropriately.</p> <p>Returns:</p> Name Type Description <code>Iterable</code> <code>Iterable</code> <p>An iterable version of <code>_value</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the value type is not compatible with iteration.</p> Example <pre><code>data = XYData.mock(np.random.rand(10, 5))\nfor item in data.as_iterable():\n    print(item)\n</code></pre> Source code in <code>framework3/base/base_types.py</code> <pre><code>def as_iterable(self) -&gt; Iterable:\n    \"\"\"\n    Convert the `_value` attribute to an iterable, regardless of its underlying type.\n\n    This method provides a consistent way to iterate over the data,\n    handling different data types appropriately.\n\n    Returns:\n        Iterable: An iterable version of `_value`.\n\n    Raises:\n        TypeError: If the value type is not compatible with iteration.\n\n    Example:\n        ```python\n        data = XYData.mock(np.random.rand(10, 5))\n        for item in data.as_iterable():\n            print(item)\n        ```\n    \"\"\"\n    value = self.value\n\n    # Maneja diferentes tipos de datos\n    if isinstance(value, np.ndarray):\n        return value  # Los arrays numpy ya son iterables\n    elif isinstance(value, pd.DataFrame):\n        return value.iterrows()  # Devuelve un iterable sobre las filas\n    elif isinstance(value, spmatrix):\n        return value.toarray()  # type: ignore # Convierte la matriz dispersa a un array denso\n    elif isinstance(value, torch.Tensor):\n        return value\n    else:\n        raise TypeError(f\"El tipo {type(value)} no es compatible con iteraci\u00f3n.\")\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.XYData.concat","title":"<code>concat(x, axis=-1)</code>  <code>staticmethod</code>","text":"<p>Concatenate a list of data along the specified axis.</p> <p>This static method handles concatenation for various data types, including sparse matrices and other array-like structures.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>list[TxyData]</code> <p>List of data to concatenate.</p> required <code>axis</code> <code>int</code> <p>Axis along which to concatenate. Defaults to -1.</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>A new XYData instance with the concatenated data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid axis is specified for sparse matrix concatenation.</p> Example <pre><code>data1 = np.random.rand(10, 5)\ndata2 = np.random.rand(10, 5)\ncombined = XYData.concat([data1, data2], axis=1)\n</code></pre> Source code in <code>framework3/base/base_types.py</code> <pre><code>@staticmethod\ndef concat(x: list[TxyData], axis: int = -1) -&gt; XYData:\n    \"\"\"\n    Concatenate a list of data along the specified axis.\n\n    This static method handles concatenation for various data types,\n    including sparse matrices and other array-like structures.\n\n    Args:\n        x (list[TxyData]): List of data to concatenate.\n        axis (int, optional): Axis along which to concatenate. Defaults to -1.\n\n    Returns:\n        XYData: A new XYData instance with the concatenated data.\n\n    Raises:\n        ValueError: If an invalid axis is specified for sparse matrix concatenation.\n\n    Example:\n        ```python\n        data1 = np.random.rand(10, 5)\n        data2 = np.random.rand(10, 5)\n        combined = XYData.concat([data1, data2], axis=1)\n        ```\n    \"\"\"\n    if all(isinstance(item, spmatrix) for item in x):\n        if axis == 1:\n            return XYData.mock(value=cast(spmatrix, hstack(x)))\n        elif axis == 0:\n            return XYData.mock(value=cast(spmatrix, vstack(x)))\n        raise ValueError(\"Invalid axis for concatenating sparse matrices\")\n    return concat(x, axis=axis)\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.XYData.ensure_dim","title":"<code>ensure_dim(x)</code>  <code>staticmethod</code>","text":"<p>Ensure the input data has at least two dimensions.</p> <p>This static method is a wrapper around the ensure_dim function, which adds a new axis to 1D arrays or lists.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>list | ndarray</code> <p>Input data to ensure dimensions.</p> required <p>Returns:</p> Type Description <code>list | ndarray</code> <p>list | np.ndarray: Data with at least two dimensions.</p> Example <pre><code>data = [1, 2, 3, 4, 5]\ntwo_dim_data = XYData.ensure_dim(data)\n</code></pre> Source code in <code>framework3/base/base_types.py</code> <pre><code>@staticmethod\ndef ensure_dim(x: list | np.ndarray) -&gt; list | np.ndarray:\n    \"\"\"\n    Ensure the input data has at least two dimensions.\n\n    This static method is a wrapper around the ensure_dim function,\n    which adds a new axis to 1D arrays or lists.\n\n    Args:\n        x (list | np.ndarray): Input data to ensure dimensions.\n\n    Returns:\n        list | np.ndarray: Data with at least two dimensions.\n\n    Example:\n        ```python\n        data = [1, 2, 3, 4, 5]\n        two_dim_data = XYData.ensure_dim(data)\n        ```\n    \"\"\"\n    return ensure_dim(x)\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.XYData.mock","title":"<code>mock(value, hash=None, path=None)</code>  <code>staticmethod</code>","text":"<p>Create a mock XYData instance for testing or placeholder purposes.</p> <p>This static method allows for easy creation of XYData instances, particularly useful in testing scenarios or when placeholder data is needed.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>TxyData | Callable[..., TxyData]</code> <p>The data or a callable that returns the data.</p> required <code>hash</code> <code>str | None</code> <p>A hash string for the data. Defaults to \"Mock\" if None.</p> <code>None</code> <code>path</code> <code>str | None</code> <p>A path string for the data. Defaults to \"/tmp\" if None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>A new XYData instance with the provided or default values.</p> Example <pre><code>mock_data = XYData.mock(np.random.rand(10, 5), hash=\"test_data\", path=\"/data/test\")\n</code></pre> Source code in <code>framework3/base/base_types.py</code> <pre><code>@staticmethod\ndef mock(\n    value: TxyData | Callable[..., TxyData],\n    hash: str | None = None,\n    path: str | None = None,\n) -&gt; XYData:\n    \"\"\"\n    Create a mock XYData instance for testing or placeholder purposes.\n\n    This static method allows for easy creation of XYData instances,\n    particularly useful in testing scenarios or when placeholder data is needed.\n\n    Args:\n        value (TxyData | Callable[..., TxyData]): The data or a callable that returns the data.\n        hash (str | None, optional): A hash string for the data. Defaults to \"Mock\" if None.\n        path (str | None, optional): A path string for the data. Defaults to \"/tmp\" if None.\n\n    Returns:\n        XYData: A new XYData instance with the provided or default values.\n\n    Example:\n        ```python\n        mock_data = XYData.mock(np.random.rand(10, 5), hash=\"test_data\", path=\"/data/test\")\n        ```\n    \"\"\"\n    if hash is None:\n        hash = \"Mock\"\n\n    if path is None:\n        path = \"/tmp\"\n\n    return XYData(_hash=hash, _path=path, _value=value)\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.XYData.split","title":"<code>split(indices)</code>","text":"<p>Split the data into a new XYData instance with the specified indices.</p> <p>This method creates a new XYData instance containing only the data corresponding to the provided indices.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable[int]</code> <p>The indices to select from the data.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>A new XYData instance containing the selected data.</p> Example <pre><code>data = XYData.mock(np.random.rand(100, 5))\nsubset = data.split(range(50, 100))  # Select second half of the data\n</code></pre> Source code in <code>framework3/base/base_types.py</code> <pre><code>def split(self, indices: Iterable[int]) -&gt; XYData:\n    \"\"\"\n    Split the data into a new XYData instance with the specified indices.\n\n    This method creates a new XYData instance containing only the data\n    corresponding to the provided indices.\n\n    Args:\n        indices (Iterable[int]): The indices to select from the data.\n\n    Returns:\n        XYData: A new XYData instance containing the selected data.\n\n    Example:\n        ```python\n        data = XYData.mock(np.random.rand(100, 5))\n        subset = data.split(range(50, 100))  # Select second half of the data\n        ```\n    \"\"\"\n\n    def split_data(self, indices: Iterable[int]) -&gt; VData:\n        value = self.value\n\n        match value:\n            case np.ndarray():\n                return value[list(indices)]\n\n            case list():\n                return [value[i] for i in indices]\n\n            case torch.Tensor():\n                return value[list(indices)]\n\n            # mypy: disable-next-line[misc]\n            # case spmatrix():\n            #     return cast(spmatrix, csr_matrix(value)[indices])\n\n            case pd.DataFrame():\n                return value.iloc[list(indices)]\n\n            case _:\n                if isinstance(value, spmatrix):\n                    return cast(spmatrix, csr_matrix(value)[indices])\n                raise TypeError(\n                    f\"Unsupported data type for splitting: {type(value)}\"\n                )\n\n    indices_hash = hashlib.sha1(str(list(indices)).encode()).hexdigest()\n    return XYData(\n        _hash=f\"{self._hash}[{indices_hash}]\",\n        _path=self._path,\n        _value=lambda: split_data(self, indices),\n    )\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.XYData.train_test_split","title":"<code>train_test_split(x, y, test_size, random_state=42)</code>","text":"<p>Split the data into training and testing sets.</p> <p>This method uses sklearn's train_test_split function to divide the data into training and testing sets for both features (X) and targets (Y).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>TxyData</code> <p>The feature data to split.</p> required <code>y</code> <code>TxyData | None</code> <p>The target data to split. Can be None for unsupervised learning.</p> required <code>test_size</code> <code>float</code> <p>The proportion of the data to include in the test split (0.0 to 1.0).</p> required <code>random_state</code> <code>int</code> <p>Seed for the random number generator. Defaults to 42.</p> <code>42</code> <p>Returns:</p> Type Description <code>XYData</code> <p>Tuple[XYData, XYData, XYData, XYData]: A tuple containing (X_train, X_test, y_train, y_test),</p> <code>XYData</code> <p>each wrapped in an XYData instance.</p> Example <pre><code>data = XYData.mock(np.random.rand(100, 5))\nlabels = XYData.mock(np.random.randint(0, 2, 100))\nX_train, X_test, y_train, y_test = data.train_test_split(data.value, labels.value, test_size=0.2)\n</code></pre> Source code in <code>framework3/base/base_types.py</code> <pre><code>def train_test_split(\n    self, x: TxyData, y: TxyData | None, test_size: float, random_state: int = 42\n) -&gt; Tuple[XYData, XYData, XYData, XYData]:\n    \"\"\"\n    Split the data into training and testing sets.\n\n    This method uses sklearn's train_test_split function to divide the data\n    into training and testing sets for both features (X) and targets (Y).\n\n    Args:\n        x (TxyData): The feature data to split.\n        y (TxyData | None): The target data to split. Can be None for unsupervised learning.\n        test_size (float): The proportion of the data to include in the test split (0.0 to 1.0).\n        random_state (int, optional): Seed for the random number generator. Defaults to 42.\n\n    Returns:\n        Tuple[XYData, XYData, XYData, XYData]: A tuple containing (X_train, X_test, y_train, y_test),\n        each wrapped in an XYData instance.\n\n    Example:\n        ```python\n        data = XYData.mock(np.random.rand(100, 5))\n        labels = XYData.mock(np.random.randint(0, 2, 100))\n        X_train, X_test, y_train, y_test = data.train_test_split(data.value, labels.value, test_size=0.2)\n        ```\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(\n        x, y, test_size=test_size, random_state=random_state\n    )\n\n    return (\n        XYData.mock(X_train, hash=f\"{self._hash} X train\", path=\"/dataset\"),\n        XYData.mock(X_test, hash=f\"{self._hash} X test\", path=\"/dataset\"),\n        XYData.mock(y_train, hash=f\"{self._hash} y train\", path=\"/dataset\"),\n        XYData.mock(y_test, hash=f\"{self._hash} y test\", path=\"/dataset\"),\n    )\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types._","title":"<code>_(x)</code>","text":"<p>Ensure that a list has at least two dimensions by converting it to a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>list</code> <p>Input list.</p> required <p>Returns:</p> Name Type Description <code>SkVData</code> <code>SkVData</code> <p>A numpy array with at least two dimensions.</p> Source code in <code>framework3/base/base_types.py</code> <pre><code>@ensure_dim.register  # type: ignore\ndef _(x: list) -&gt; SkVData:\n    \"\"\"\n    Ensure that a list has at least two dimensions by converting it to a numpy array.\n\n    Args:\n        x (list): Input list.\n\n    Returns:\n        SkVData: A numpy array with at least two dimensions.\n    \"\"\"\n    return ensure_dim(np.array(x))\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.concat","title":"<code>concat(x, axis)</code>","text":"<p>Base multimethod for concatenation. Raises an error for unsupported types.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Data to concatenate.</p> required <code>axis</code> <code>int</code> <p>Axis along which to concatenate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>Always raised as this is the base method for unsupported types.</p> Source code in <code>framework3/base/base_types.py</code> <pre><code>@multimethod\ndef concat(x: Any, axis: int) -&gt; \"XYData\":\n    \"\"\"\n    Base multimethod for concatenation. Raises an error for unsupported types.\n\n    Args:\n        x (Any): Data to concatenate.\n        axis (int): Axis along which to concatenate.\n\n    Raises:\n        TypeError: Always raised as this is the base method for unsupported types.\n    \"\"\"\n    raise TypeError(f\"Cannot concatenate this type of data, only {VData} compatible\")\n</code></pre>"},{"location":"api/base/base_types/#framework3.base.base_types.ensure_dim","title":"<code>ensure_dim(x)</code>","text":"<p>Base multimethod for ensuring dimensions. Raises an error for unsupported types.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Data to ensure dimensions for.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>Always raised as this is the base method for unsupported types.</p> Source code in <code>framework3/base/base_types.py</code> <pre><code>@multimethod\ndef ensure_dim(x: Any) -&gt; SkVData | VData:\n    \"\"\"\n    Base multimethod for ensuring dimensions. Raises an error for unsupported types.\n\n    Args:\n        x (Any): Data to ensure dimensions for.\n\n    Raises:\n        TypeError: Always raised as this is the base method for unsupported types.\n    \"\"\"\n    raise TypeError(\n        f\"Cannot concatenate this type of data, only {VData} or {SkVData} compatible\"\n    )\n</code></pre>"},{"location":"api/base/exceptions/","title":"Exceptions","text":""},{"location":"api/base/exceptions/#framework3.base.exceptions","title":"<code>framework3.base.exceptions</code>","text":""},{"location":"api/base/exceptions/#framework3.base.exceptions.NotTrainableFilterError","title":"<code>NotTrainableFilterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when attempting to fit a non-trainable filter.</p> <p>This exception is used to indicate that an attempt was made to train or fit a filter that is not designed to be trainable.</p> Key Features <ul> <li>Inherits from the built-in Exception class</li> <li>Provides a clear error message for debugging and error handling</li> </ul> Usage <p>This exception should be raised when a non-trainable filter's fit method is called or when any attempt is made to train a filter that doesn't support training.</p> Example <pre><code>class NonTrainableFilter(BaseFilter):\n    def fit(self, X, y=None):\n        raise NotTrainableFilterError(\"This filter does not support training.\")\n\n    def transform(self, X):\n        # transformation logic here\n        pass\n</code></pre> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>The error message passed when raising the exception.</p> Note <p>When catching this exception, it's often useful to log the error or inform the user that the attempted operation is not supported for the given filter.</p> Source code in <code>framework3/base/exceptions.py</code> <pre><code>class NotTrainableFilterError(Exception):\n    \"\"\"\n    Exception raised when attempting to fit a non-trainable filter.\n\n    This exception is used to indicate that an attempt was made to train or fit\n    a filter that is not designed to be trainable.\n\n    Key Features:\n        - Inherits from the built-in Exception class\n        - Provides a clear error message for debugging and error handling\n\n    Usage:\n        This exception should be raised when a non-trainable filter's fit method\n        is called or when any attempt is made to train a filter that doesn't\n        support training.\n\n    Example:\n        ```python\n        class NonTrainableFilter(BaseFilter):\n            def fit(self, X, y=None):\n                raise NotTrainableFilterError(\"This filter does not support training.\")\n\n            def transform(self, X):\n                # transformation logic here\n                pass\n        ```\n\n    Attributes:\n        message (str): The error message passed when raising the exception.\n\n    Note:\n        When catching this exception, it's often useful to log the error or inform\n        the user that the attempted operation is not supported for the given filter.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/container/container/","title":"Container","text":""},{"location":"api/container/container/#framework3.container.container","title":"<code>framework3.container.container</code>","text":""},{"location":"api/container/container/#framework3.container.container.F","title":"<code>F = TypeVar('F', bound=type)</code>  <code>module-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.__all__","title":"<code>__all__ = ['Container']</code>  <code>module-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container","title":"<code>Container</code>","text":"<p>A container class for managing various components of the framework.</p> <p>This class provides a centralized location for storing and managing different types of objects such as filters, pipelines, metrics, storage, and plugins. It uses factories to create and store these objects.</p> Key Features <ul> <li>Centralized management of framework components</li> <li>Factory-based creation and storage of objects</li> <li>Static binding method for easy registration of components</li> <li>Support for multiple component types (filters, pipelines, metrics, storage, plugins)</li> </ul> Usage <p>To use the Container, you can register components and then retrieve them as needed:</p> <pre><code>from framework3.container import Container\nfrom framework3.base import BaseFilter, BasePipeline\n\n@Container.bind()\nclass MyFilter(BaseFilter):\n    def fit(self, x, y):\n        pass\n    def predict(self, x):\n        return x\n\n@Container.bind()\nclass MyPipeline(BasePipeline):\n    def fit(self, x, y):\n        pass\n    def predict(self, x):\n        return x\n    def init(self):\n        pass\n    def start(self, x, y, X_):\n        return None\n    def log_metrics(self):\n        pass\n    def finish(self):\n        pass\n    def evaluate(self, x_data, y_true, y_pred):\n        return {}\n\n# Retrieving and using registered components\nfilter_instance = Container.ff[\"MyFilter\"]()\npipeline_instance = Container.pf[\"MyPipeline\"]()\n\nresult = pipeline_instance.run(filter_instance.process(\"hello\"))\nprint(result)\n</code></pre> <p>Attributes:</p> Name Type Description <code>storage</code> <code>BaseStorage</code> <p>An instance of BaseStorage for handling storage operations.</p> <code>ds</code> <code>BaseDatasetManager</code> <p>An instance of BaseDatasetManager for managing datasets.</p> <code>ff</code> <code>BaseFactory[BaseFilter]</code> <p>Factory for creating and storing BaseFilter objects.</p> <code>pf</code> <code>BaseFactory[BasePipeline]</code> <p>Factory for creating and storing BasePipeline objects.</p> <code>mf</code> <code>BaseFactory[BaseMetric]</code> <p>Factory for creating and storing BaseMetric objects.</p> <code>sf</code> <code>BaseFactory[BaseStorage]</code> <p>Factory for creating and storing BaseStorage objects.</p> <code>pif</code> <code>BaseFactory[BasePlugin]</code> <p>Factory for creating and storing BasePlugin objects.</p> <p>Methods:</p> Name Description <code>bind</code> <p>Optional[Any] = dict, wrapper: Optional[Any] = dict) -&gt; Callable: A decorator for binding various components to the Container.</p> Note <p>The Container class is designed to be used as a singleton, with all its methods and attributes being class-level (static) to ensure a single point of access for all components across the framework.</p> Source code in <code>framework3/container/container.py</code> <pre><code>class Container:\n    \"\"\"\n    A container class for managing various components of the framework.\n\n    This class provides a centralized location for storing and managing different types of\n    objects such as filters, pipelines, metrics, storage, and plugins. It uses factories\n    to create and store these objects.\n\n    Key Features:\n        - Centralized management of framework components\n        - Factory-based creation and storage of objects\n        - Static binding method for easy registration of components\n        - Support for multiple component types (filters, pipelines, metrics, storage, plugins)\n\n    Usage:\n        To use the Container, you can register components and then retrieve them as needed:\n\n        ```python\n        from framework3.container import Container\n        from framework3.base import BaseFilter, BasePipeline\n\n        @Container.bind()\n        class MyFilter(BaseFilter):\n            def fit(self, x, y):\n                pass\n            def predict(self, x):\n                return x\n\n        @Container.bind()\n        class MyPipeline(BasePipeline):\n            def fit(self, x, y):\n                pass\n            def predict(self, x):\n                return x\n            def init(self):\n                pass\n            def start(self, x, y, X_):\n                return None\n            def log_metrics(self):\n                pass\n            def finish(self):\n                pass\n            def evaluate(self, x_data, y_true, y_pred):\n                return {}\n\n        # Retrieving and using registered components\n        filter_instance = Container.ff[\"MyFilter\"]()\n        pipeline_instance = Container.pf[\"MyPipeline\"]()\n\n        result = pipeline_instance.run(filter_instance.process(\"hello\"))\n        print(result)\n        ```\n\n    Attributes:\n        storage (BaseStorage): An instance of BaseStorage for handling storage operations.\n        ds (BaseDatasetManager): An instance of BaseDatasetManager for managing datasets.\n        ff (BaseFactory[BaseFilter]): Factory for creating and storing BaseFilter objects.\n        pf (BaseFactory[BasePipeline]): Factory for creating and storing BasePipeline objects.\n        mf (BaseFactory[BaseMetric]): Factory for creating and storing BaseMetric objects.\n        sf (BaseFactory[BaseStorage]): Factory for creating and storing BaseStorage objects.\n        pif (BaseFactory[BasePlugin]): Factory for creating and storing BasePlugin objects.\n\n    Methods:\n        bind(manager: Optional[Any] = dict, wrapper: Optional[Any] = dict) -&gt; Callable:\n            A decorator for binding various components to the Container.\n\n    Note:\n        The Container class is designed to be used as a singleton, with all its methods\n        and attributes being class-level (static) to ensure a single point of access\n        for all components across the framework.\n    \"\"\"\n\n    storage: BaseStorage\n    ds: BaseDatasetManager\n    ff: BaseFactory[BaseFilter] = BaseFactory[BaseFilter]()\n    pf: BaseFactory[BasePipeline] = BaseFactory[BasePipeline]()\n    mf: BaseFactory[BaseMetric] = BaseFactory[BaseMetric]()\n    sf: BaseFactory[BaseStorage] = BaseFactory[BaseStorage]()\n    pif: BaseFactory[BasePlugin] = BaseFactory[BasePlugin]()\n\n    @staticmethod\n    def bind(manager: Optional[Any] = dict, wrapper: Optional[Any] = dict) -&gt; Callable:\n        \"\"\"\n        A decorator for binding various components to the Container.\n\n        This method uses function dispatching to register different types of components\n        (filters, pipelines, metrics, storage) with their respective factories in the Container.\n\n        Args:\n            manager (Optional[Any]): An optional manager for the binding process. Defaults to dict.\n            wrapper (Optional[Any]): An optional wrapper for the binding process. Defaults to dict.\n\n        Returns:\n            Callable: A decorator function that registers the decorated class with the appropriate factory.\n\n        Raises:\n            NotImplementedError: If no decorator is registered for the given function.\n\n        Example:\n            ```python\n            @Container.bind()\n            class MyCustomFilter(BaseFilter):\n                def fit(self, x, y):\n                    # Implementation\n                    pass\n\n                def predict(self, x):\n                    # Implementation\n                    return x\n\n            # The MyCustomFilter class is now registered and can be accessed via Container.ff[\"MyCustomFilter\"]\n            ```\n\n        Note:\n            This method uses the @fundispatch decorator to provide different implementations\n            based on the type of the decorated class. It automatically registers the class\n            with the appropriate factory based on its base class (BaseFilter, BasePipeline, etc.).\n        \"\"\"\n\n        @fundispatch  # type: ignore\n        def inner(func: Any):\n            \"\"\"\n            Default inner function for the bind decorator.\n\n            This function is called when no specific registration is found for the decorated class.\n\n            Args:\n                func (Any): The class being decorated.\n\n            Raises:\n                NotImplementedError: Always raised to indicate that no suitable decorator was found.\n            \"\"\"\n            raise NotImplementedError(f\"No decorator registered for {func.__name__}\")\n\n        @inner.register(BaseFilter)  # type: ignore\n        def _(func: Type[BaseFilter]) -&gt; Type[BaseFilter]:\n            \"\"\"\n            Register a BaseFilter class with the Container.\n\n            This function is called when the decorated class is a subclass of BaseFilter.\n\n            Args:\n                func (Type[BaseFilter]): The BaseFilter subclass being decorated.\n\n            Returns:\n                Type[BaseFilter]: The decorated class, now registered with the Container.\n            \"\"\"\n            Container.ff[func.__name__] = func\n            Container.pif[func.__name__] = func\n            return func\n\n        @inner.register(BasePipeline)  # type: ignore\n        def _(func: Type[BasePipeline]) -&gt; Type[BasePipeline]:\n            \"\"\"\n            Register a BasePipeline class with the Container.\n\n            This function is called when the decorated class is a subclass of BasePipeline.\n\n            Args:\n                func (Type[BasePipeline]): The BasePipeline subclass being decorated.\n\n            Returns:\n                Type[BasePipeline]: The decorated class, now registered with the Container.\n            \"\"\"\n            Container.pf[func.__name__] = func\n            Container.pif[func.__name__] = func\n            return func\n\n        @inner.register(BaseMetric)  # type: ignore\n        def _(func: Type[BaseMetric]) -&gt; Type[BaseMetric]:\n            \"\"\"\n            Register a BaseMetric class with the Container.\n\n            This function is called when the decorated class is a subclass of BaseMetric.\n\n            Args:\n                func (Type[BaseMetric]): The BaseMetric subclass being decorated.\n\n            Returns:\n                Type[BaseMetric]: The decorated class, now registered with the Container.\n            \"\"\"\n            Container.mf[func.__name__] = func\n            Container.pif[func.__name__] = func\n            return func\n\n        @inner.register(BaseStorage)  # type: ignore\n        def _(func: Type[BaseStorage]) -&gt; Type[BaseStorage]:\n            \"\"\"\n            Register a BaseStorage class with the Container.\n\n            This function is called when the decorated class is a subclass of BaseStorage.\n\n            Args:\n                func (Type[BaseStorage]): The BaseStorage subclass being decorated.\n\n            Returns:\n                Type[BaseStorage]: The decorated class, now registered with the Container.\n            \"\"\"\n            Container.sf[func.__name__] = func\n            Container.pif[func.__name__] = func\n            return func\n\n        @inner.register(BasePlugin)  # type: ignore\n        def _(func: Type[BasePlugin]) -&gt; Type[BasePlugin]:\n            \"\"\"\n            Register a BasePlugin class with the Container.\n\n            This function is called when the decorated class is a subclass of BasePlugin.\n\n            Args:\n                func (Type[BasePlugin]): The BasePlugin subclass being decorated.\n\n            Returns:\n                Type[BasePlugin]: The decorated class, now registered with the Container.\n            \"\"\"\n            Container.pif[func.__name__] = func\n            return func\n\n        return inner\n</code></pre>"},{"location":"api/container/container/#framework3.container.container.Container.ds","title":"<code>ds</code>  <code>instance-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container.ff","title":"<code>ff = BaseFactory[BaseFilter]()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container.mf","title":"<code>mf = BaseFactory[BaseMetric]()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container.pf","title":"<code>pf = BaseFactory[BasePipeline]()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container.pif","title":"<code>pif = BaseFactory[BasePlugin]()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container.sf","title":"<code>sf = BaseFactory[BaseStorage]()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container.storage","title":"<code>storage</code>  <code>instance-attribute</code>","text":""},{"location":"api/container/container/#framework3.container.container.Container.bind","title":"<code>bind(manager=dict, wrapper=dict)</code>  <code>staticmethod</code>","text":"<p>A decorator for binding various components to the Container.</p> <p>This method uses function dispatching to register different types of components (filters, pipelines, metrics, storage) with their respective factories in the Container.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>Optional[Any]</code> <p>An optional manager for the binding process. Defaults to dict.</p> <code>dict</code> <code>wrapper</code> <code>Optional[Any]</code> <p>An optional wrapper for the binding process. Defaults to dict.</p> <code>dict</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A decorator function that registers the decorated class with the appropriate factory.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If no decorator is registered for the given function.</p> Example <pre><code>@Container.bind()\nclass MyCustomFilter(BaseFilter):\n    def fit(self, x, y):\n        # Implementation\n        pass\n\n    def predict(self, x):\n        # Implementation\n        return x\n\n# The MyCustomFilter class is now registered and can be accessed via Container.ff[\"MyCustomFilter\"]\n</code></pre> Note <p>This method uses the @fundispatch decorator to provide different implementations based on the type of the decorated class. It automatically registers the class with the appropriate factory based on its base class (BaseFilter, BasePipeline, etc.).</p> Source code in <code>framework3/container/container.py</code> <pre><code>@staticmethod\ndef bind(manager: Optional[Any] = dict, wrapper: Optional[Any] = dict) -&gt; Callable:\n    \"\"\"\n    A decorator for binding various components to the Container.\n\n    This method uses function dispatching to register different types of components\n    (filters, pipelines, metrics, storage) with their respective factories in the Container.\n\n    Args:\n        manager (Optional[Any]): An optional manager for the binding process. Defaults to dict.\n        wrapper (Optional[Any]): An optional wrapper for the binding process. Defaults to dict.\n\n    Returns:\n        Callable: A decorator function that registers the decorated class with the appropriate factory.\n\n    Raises:\n        NotImplementedError: If no decorator is registered for the given function.\n\n    Example:\n        ```python\n        @Container.bind()\n        class MyCustomFilter(BaseFilter):\n            def fit(self, x, y):\n                # Implementation\n                pass\n\n            def predict(self, x):\n                # Implementation\n                return x\n\n        # The MyCustomFilter class is now registered and can be accessed via Container.ff[\"MyCustomFilter\"]\n        ```\n\n    Note:\n        This method uses the @fundispatch decorator to provide different implementations\n        based on the type of the decorated class. It automatically registers the class\n        with the appropriate factory based on its base class (BaseFilter, BasePipeline, etc.).\n    \"\"\"\n\n    @fundispatch  # type: ignore\n    def inner(func: Any):\n        \"\"\"\n        Default inner function for the bind decorator.\n\n        This function is called when no specific registration is found for the decorated class.\n\n        Args:\n            func (Any): The class being decorated.\n\n        Raises:\n            NotImplementedError: Always raised to indicate that no suitable decorator was found.\n        \"\"\"\n        raise NotImplementedError(f\"No decorator registered for {func.__name__}\")\n\n    @inner.register(BaseFilter)  # type: ignore\n    def _(func: Type[BaseFilter]) -&gt; Type[BaseFilter]:\n        \"\"\"\n        Register a BaseFilter class with the Container.\n\n        This function is called when the decorated class is a subclass of BaseFilter.\n\n        Args:\n            func (Type[BaseFilter]): The BaseFilter subclass being decorated.\n\n        Returns:\n            Type[BaseFilter]: The decorated class, now registered with the Container.\n        \"\"\"\n        Container.ff[func.__name__] = func\n        Container.pif[func.__name__] = func\n        return func\n\n    @inner.register(BasePipeline)  # type: ignore\n    def _(func: Type[BasePipeline]) -&gt; Type[BasePipeline]:\n        \"\"\"\n        Register a BasePipeline class with the Container.\n\n        This function is called when the decorated class is a subclass of BasePipeline.\n\n        Args:\n            func (Type[BasePipeline]): The BasePipeline subclass being decorated.\n\n        Returns:\n            Type[BasePipeline]: The decorated class, now registered with the Container.\n        \"\"\"\n        Container.pf[func.__name__] = func\n        Container.pif[func.__name__] = func\n        return func\n\n    @inner.register(BaseMetric)  # type: ignore\n    def _(func: Type[BaseMetric]) -&gt; Type[BaseMetric]:\n        \"\"\"\n        Register a BaseMetric class with the Container.\n\n        This function is called when the decorated class is a subclass of BaseMetric.\n\n        Args:\n            func (Type[BaseMetric]): The BaseMetric subclass being decorated.\n\n        Returns:\n            Type[BaseMetric]: The decorated class, now registered with the Container.\n        \"\"\"\n        Container.mf[func.__name__] = func\n        Container.pif[func.__name__] = func\n        return func\n\n    @inner.register(BaseStorage)  # type: ignore\n    def _(func: Type[BaseStorage]) -&gt; Type[BaseStorage]:\n        \"\"\"\n        Register a BaseStorage class with the Container.\n\n        This function is called when the decorated class is a subclass of BaseStorage.\n\n        Args:\n            func (Type[BaseStorage]): The BaseStorage subclass being decorated.\n\n        Returns:\n            Type[BaseStorage]: The decorated class, now registered with the Container.\n        \"\"\"\n        Container.sf[func.__name__] = func\n        Container.pif[func.__name__] = func\n        return func\n\n    @inner.register(BasePlugin)  # type: ignore\n    def _(func: Type[BasePlugin]) -&gt; Type[BasePlugin]:\n        \"\"\"\n        Register a BasePlugin class with the Container.\n\n        This function is called when the decorated class is a subclass of BasePlugin.\n\n        Args:\n            func (Type[BasePlugin]): The BasePlugin subclass being decorated.\n\n        Returns:\n            Type[BasePlugin]: The decorated class, now registered with the Container.\n        \"\"\"\n        Container.pif[func.__name__] = func\n        return func\n\n    return inner\n</code></pre>"},{"location":"api/container/overload/","title":"Overload","text":""},{"location":"api/container/overload/#framework3.container.overload","title":"<code>framework3.container.overload</code>","text":""},{"location":"api/container/overload/#framework3.container.overload.R","title":"<code>R = TypeVar('R')</code>  <code>module-attribute</code>","text":""},{"location":"api/container/overload/#framework3.container.overload.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"api/container/overload/#framework3.container.overload.DispatchableMethod","title":"<code>DispatchableMethod</code>","text":"<p>               Bases: <code>Protocol[R]</code></p> <p>Protocol for a dispatchable method.</p> <p>This protocol defines the interface for a method that can be dispatched based on the type of its arguments and can register new implementations.</p> Key Features <ul> <li>Defines a callable interface</li> <li>Provides a register method for new implementations</li> </ul> Usage <p>This protocol is typically used as a type hint for methods that support dynamic dispatch based on argument types.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Any, **kwargs: Any) -&gt; R: Call the method with the given arguments.</p> <code>register</code> <p>type[T], func: Callable[..., R]) -&gt; Callable[..., R]: Register a new implementation for the given class.</p> Note <p>This is a Protocol class and is not meant to be instantiated directly. It serves as a structural subtyping tool for static type checking.</p> Source code in <code>framework3/container/overload.py</code> <pre><code>class DispatchableMethod(Protocol[R]):\n    \"\"\"\n    Protocol for a dispatchable method.\n\n    This protocol defines the interface for a method that can be dispatched\n    based on the type of its arguments and can register new implementations.\n\n    Key Features:\n        - Defines a callable interface\n        - Provides a register method for new implementations\n\n    Usage:\n        This protocol is typically used as a type hint for methods that support\n        dynamic dispatch based on argument types.\n\n    Methods:\n        __call__(*args: Any, **kwargs: Any) -&gt; R:\n            Call the method with the given arguments.\n        register(cls: type[T], func: Callable[..., R]) -&gt; Callable[..., R]:\n            Register a new implementation for the given class.\n\n    Note:\n        This is a Protocol class and is not meant to be instantiated directly.\n        It serves as a structural subtyping tool for static type checking.\n    \"\"\"\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; R:\n        \"\"\"\n        Call the method with the given arguments.\n\n        This method represents the main functionality of the dispatchable method.\n        It will be called when the method is invoked and will dispatch to the\n        appropriate implementation based on the types of the arguments.\n\n        Args:\n            *args (Any): Positional arguments passed to the method.\n            **kwargs (Any): Keyword arguments passed to the method.\n\n        Returns:\n            R: The result of calling the appropriate implementation of the method.\n        \"\"\"\n        ...\n\n    def register(self, cls: type[T], func: Callable[..., R]) -&gt; Callable[..., R]:\n        \"\"\"\n        Register a new implementation for the given class.\n\n        This method allows registering new implementations for specific types.\n        When the dispatchable method is called with an instance of the registered\n        class as its argument, the corresponding implementation will be used.\n\n        Args:\n            cls (type[T]): The class for which to register the implementation.\n            func (Callable[..., R]): The function to be used as the implementation\n                for the given class.\n\n        Returns:\n            Callable[..., R]: The registered function, allowing for decorator-style usage.\n\n        Example:\n            ```python\n            @my_method.register(int)\n            def _(self, arg: int):\n                return f\"Integer implementation: {arg}\"\n            ```\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.DispatchableMethod.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Call the method with the given arguments.</p> <p>This method represents the main functionality of the dispatchable method. It will be called when the method is invoked and will dispatch to the appropriate implementation based on the types of the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments passed to the method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>R</code> <code>R</code> <p>The result of calling the appropriate implementation of the method.</p> Source code in <code>framework3/container/overload.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; R:\n    \"\"\"\n    Call the method with the given arguments.\n\n    This method represents the main functionality of the dispatchable method.\n    It will be called when the method is invoked and will dispatch to the\n    appropriate implementation based on the types of the arguments.\n\n    Args:\n        *args (Any): Positional arguments passed to the method.\n        **kwargs (Any): Keyword arguments passed to the method.\n\n    Returns:\n        R: The result of calling the appropriate implementation of the method.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.DispatchableMethod.register","title":"<code>register(cls, func)</code>","text":"<p>Register a new implementation for the given class.</p> <p>This method allows registering new implementations for specific types. When the dispatchable method is called with an instance of the registered class as its argument, the corresponding implementation will be used.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[T]</code> <p>The class for which to register the implementation.</p> required <code>func</code> <code>Callable[..., R]</code> <p>The function to be used as the implementation for the given class.</p> required <p>Returns:</p> Type Description <code>Callable[..., R]</code> <p>Callable[..., R]: The registered function, allowing for decorator-style usage.</p> Example <pre><code>@my_method.register(int)\ndef _(self, arg: int):\n    return f\"Integer implementation: {arg}\"\n</code></pre> Source code in <code>framework3/container/overload.py</code> <pre><code>def register(self, cls: type[T], func: Callable[..., R]) -&gt; Callable[..., R]:\n    \"\"\"\n    Register a new implementation for the given class.\n\n    This method allows registering new implementations for specific types.\n    When the dispatchable method is called with an instance of the registered\n    class as its argument, the corresponding implementation will be used.\n\n    Args:\n        cls (type[T]): The class for which to register the implementation.\n        func (Callable[..., R]): The function to be used as the implementation\n            for the given class.\n\n    Returns:\n        Callable[..., R]: The registered function, allowing for decorator-style usage.\n\n    Example:\n        ```python\n        @my_method.register(int)\n        def _(self, arg: int):\n            return f\"Integer implementation: {arg}\"\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.SingleDispatch","title":"<code>SingleDispatch</code>","text":"<p>               Bases: <code>Protocol[R]</code></p> <p>Protocol for a single dispatch function.</p> <p>This protocol defines the interface for a function that can be dispatched based on the type of its first argument and can register new implementations.</p> Key Features <ul> <li>Defines a callable interface</li> <li>Provides methods for registering and dispatching implementations</li> </ul> Usage <p>This protocol is typically used as a type hint for functions that support single dispatch based on the type of the first argument.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Any, **kwargs: Any) -&gt; R: Call the function with the given arguments.</p> <code>register</code> <p>type, func: Callable[..., R]) -&gt; Callable[..., R]: Register a new implementation for the given class.</p> <code>dispatch</code> <p>type) -&gt; Callable[..., R]: Return the implementation for the given class.</p> Note <p>This is a Protocol class and is not meant to be instantiated directly. It serves as a structural subtyping tool for static type checking.</p> Source code in <code>framework3/container/overload.py</code> <pre><code>class SingleDispatch(Protocol[R]):\n    \"\"\"\n    Protocol for a single dispatch function.\n\n    This protocol defines the interface for a function that can be dispatched\n    based on the type of its first argument and can register new implementations.\n\n    Key Features:\n        - Defines a callable interface\n        - Provides methods for registering and dispatching implementations\n\n    Usage:\n        This protocol is typically used as a type hint for functions that support\n        single dispatch based on the type of the first argument.\n\n    Methods:\n        __call__(*args: Any, **kwargs: Any) -&gt; R:\n            Call the function with the given arguments.\n        register(cls: type, func: Callable[..., R]) -&gt; Callable[..., R]:\n            Register a new implementation for the given class.\n        dispatch(cls: type) -&gt; Callable[..., R]:\n            Return the implementation for the given class.\n\n    Note:\n        This is a Protocol class and is not meant to be instantiated directly.\n        It serves as a structural subtyping tool for static type checking.\n    \"\"\"\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; R:\n        \"\"\"\n        Call the function with the given arguments.\n\n        This method represents the main functionality of the single dispatch function.\n        It will be called when the function is invoked and will dispatch to the\n        appropriate implementation based on the type of the first argument.\n\n        Args:\n            *args (Any): Positional arguments passed to the function.\n            **kwargs (Any): Keyword arguments passed to the function.\n\n        Returns:\n            R: The result of calling the appropriate implementation of the function.\n        \"\"\"\n        ...\n\n    def register(self, cls: type, func: Callable[..., R]) -&gt; Callable[..., R]:\n        \"\"\"\n        Register a new implementation for the given class.\n\n        This method allows registering new implementations for specific types.\n        When the single dispatch function is called with an instance of the registered\n        class as its first argument, the corresponding implementation will be used.\n\n        Args:\n            cls (type): The class for which to register the implementation.\n            func (Callable[..., R]): The function to be used as the implementation\n                for the given class.\n\n        Returns:\n            Callable[..., R]: The registered function, allowing for decorator-style usage.\n\n        Example:\n            ```python\n            @process.register(int)\n            def _(arg: int):\n                return f\"Integer implementation: {arg}\"\n            ```\n        \"\"\"\n        ...\n\n    def dispatch(self, cls: type) -&gt; Callable[..., R]:\n        \"\"\"\n        Return the implementation for the given class.\n\n        This method is used internally by the single dispatch mechanism to retrieve\n        the appropriate implementation for a given type.\n\n        Args:\n            cls (type): The class for which to retrieve the implementation.\n\n        Returns:\n            Callable[..., R]: The implementation function registered for the given class.\n\n        Note:\n            This method is typically used internally by the dispatch mechanism and\n            not called directly by users of the single dispatch function.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.SingleDispatch.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Call the function with the given arguments.</p> <p>This method represents the main functionality of the single dispatch function. It will be called when the function is invoked and will dispatch to the appropriate implementation based on the type of the first argument.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments passed to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>R</code> <code>R</code> <p>The result of calling the appropriate implementation of the function.</p> Source code in <code>framework3/container/overload.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; R:\n    \"\"\"\n    Call the function with the given arguments.\n\n    This method represents the main functionality of the single dispatch function.\n    It will be called when the function is invoked and will dispatch to the\n    appropriate implementation based on the type of the first argument.\n\n    Args:\n        *args (Any): Positional arguments passed to the function.\n        **kwargs (Any): Keyword arguments passed to the function.\n\n    Returns:\n        R: The result of calling the appropriate implementation of the function.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.SingleDispatch.dispatch","title":"<code>dispatch(cls)</code>","text":"<p>Return the implementation for the given class.</p> <p>This method is used internally by the single dispatch mechanism to retrieve the appropriate implementation for a given type.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class for which to retrieve the implementation.</p> required <p>Returns:</p> Type Description <code>Callable[..., R]</code> <p>Callable[..., R]: The implementation function registered for the given class.</p> Note <p>This method is typically used internally by the dispatch mechanism and not called directly by users of the single dispatch function.</p> Source code in <code>framework3/container/overload.py</code> <pre><code>def dispatch(self, cls: type) -&gt; Callable[..., R]:\n    \"\"\"\n    Return the implementation for the given class.\n\n    This method is used internally by the single dispatch mechanism to retrieve\n    the appropriate implementation for a given type.\n\n    Args:\n        cls (type): The class for which to retrieve the implementation.\n\n    Returns:\n        Callable[..., R]: The implementation function registered for the given class.\n\n    Note:\n        This method is typically used internally by the dispatch mechanism and\n        not called directly by users of the single dispatch function.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.SingleDispatch.register","title":"<code>register(cls, func)</code>","text":"<p>Register a new implementation for the given class.</p> <p>This method allows registering new implementations for specific types. When the single dispatch function is called with an instance of the registered class as its first argument, the corresponding implementation will be used.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class for which to register the implementation.</p> required <code>func</code> <code>Callable[..., R]</code> <p>The function to be used as the implementation for the given class.</p> required <p>Returns:</p> Type Description <code>Callable[..., R]</code> <p>Callable[..., R]: The registered function, allowing for decorator-style usage.</p> Example <pre><code>@process.register(int)\ndef _(arg: int):\n    return f\"Integer implementation: {arg}\"\n</code></pre> Source code in <code>framework3/container/overload.py</code> <pre><code>def register(self, cls: type, func: Callable[..., R]) -&gt; Callable[..., R]:\n    \"\"\"\n    Register a new implementation for the given class.\n\n    This method allows registering new implementations for specific types.\n    When the single dispatch function is called with an instance of the registered\n    class as its first argument, the corresponding implementation will be used.\n\n    Args:\n        cls (type): The class for which to register the implementation.\n        func (Callable[..., R]): The function to be used as the implementation\n            for the given class.\n\n    Returns:\n        Callable[..., R]: The registered function, allowing for decorator-style usage.\n\n    Example:\n        ```python\n        @process.register(int)\n        def _(arg: int):\n            return f\"Integer implementation: {arg}\"\n        ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.fundispatch","title":"<code>fundispatch(func)</code>","text":"<p>Decorator for creating a function dispatch.</p> <p>This decorator creates a wrapper around the given function that dispatches based on the type of the first argument.</p> Key Features <ul> <li>Creates a dispatchable function</li> <li>Dispatches based on the type of the first argument</li> <li>Allows registration of new implementations</li> </ul> Usage <p>Use this decorator on functions that need to dispatch based on the type of their first argument.</p> <pre><code>@fundispatch\ndef process(arg):\n    return \"Default implementation\"\n\n@process.register(int)\ndef _(arg: int):\n    return f\"Integer implementation: {arg}\"\n\n@process.register(str)\ndef _(arg: str):\n    return f\"String implementation: {arg}\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>SingleDispatch[R]</code> <p>The function to be wrapped.</p> required <p>Returns:</p> Type Description <code>SingleDispatch[R]</code> <p>SingleDispatch[R]: A wrapper function with dispatch capabilities.</p> Note <p>The wrapped function will dispatch based on the type of the first argument. If the first argument is a type object, it will dispatch on that type directly. Otherwise, it will dispatch on the type of the first argument.</p> Source code in <code>framework3/container/overload.py</code> <pre><code>def fundispatch(func: SingleDispatch[R]) -&gt; SingleDispatch[R]:\n    \"\"\"\n    Decorator for creating a function dispatch.\n\n    This decorator creates a wrapper around the given function that dispatches\n    based on the type of the first argument.\n\n    Key Features:\n        - Creates a dispatchable function\n        - Dispatches based on the type of the first argument\n        - Allows registration of new implementations\n\n    Usage:\n        Use this decorator on functions that need to dispatch based on the type\n        of their first argument.\n\n        ```python\n        @fundispatch\n        def process(arg):\n            return \"Default implementation\"\n\n        @process.register(int)\n        def _(arg: int):\n            return f\"Integer implementation: {arg}\"\n\n        @process.register(str)\n        def _(arg: str):\n            return f\"String implementation: {arg}\"\n        ```\n\n    Args:\n        func (SingleDispatch[R]): The function to be wrapped.\n\n    Returns:\n        SingleDispatch[R]: A wrapper function with dispatch capabilities.\n\n    Note:\n        The wrapped function will dispatch based on the type of the first argument.\n        If the first argument is a type object, it will dispatch on that type directly.\n        Otherwise, it will dispatch on the type of the first argument.\n    \"\"\"\n    dispatcher = singledispatch(func)\n\n    def wrapper(*args: Any, **kwargs: Any) -&gt; R:\n        arg_type = args[0] if isinstance(args[0], type) else type(args[0])\n        return dispatcher.dispatch(arg_type)(*args, **kwargs)\n\n    wrapper = cast(SingleDispatch[R], wrapper)\n    setattr(wrapper, \"register\", dispatcher.register)\n    setattr(wrapper, \"dispatch\", dispatcher.dispatch)\n    update_wrapper(wrapper, func)\n    return wrapper\n</code></pre>"},{"location":"api/container/overload/#framework3.container.overload.methdispatch","title":"<code>methdispatch(func)</code>","text":"<p>Decorator for creating a method dispatch.</p> <p>This decorator creates a wrapper around the given function that dispatches based on the type of the second argument (typically 'self' in method calls).</p> Key Features <ul> <li>Creates a dispatchable method</li> <li>Dispatches based on the type of the second argument</li> <li>Allows registration of new implementations</li> </ul> Usage <p>Use this decorator on methods that need to dispatch based on the type of their second argument (typically the first argument after 'self').</p> <pre><code>class MyClass:\n    @methdispatch\n    def my_method(self, arg):\n        return \"Default implementation\"\n\n    @my_method.register(int)\n    def _(self, arg: int):\n        return f\"Integer implementation: {arg}\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., R]</code> <p>The function to be wrapped.</p> required <p>Returns:</p> Type Description <code>DispatchableMethod[R]</code> <p>DispatchableMethod[R]: A wrapper function with dispatch capabilities.</p> Note <p>The wrapped method will dispatch based on the type of the second argument, which is typically the first argument after 'self' in method calls.</p> Source code in <code>framework3/container/overload.py</code> <pre><code>def methdispatch(func: Callable[..., R]) -&gt; DispatchableMethod[R]:\n    \"\"\"\n    Decorator for creating a method dispatch.\n\n    This decorator creates a wrapper around the given function that dispatches\n    based on the type of the second argument (typically 'self' in method calls).\n\n    Key Features:\n        - Creates a dispatchable method\n        - Dispatches based on the type of the second argument\n        - Allows registration of new implementations\n\n    Usage:\n        Use this decorator on methods that need to dispatch based on the type\n        of their second argument (typically the first argument after 'self').\n\n        ```python\n        class MyClass:\n            @methdispatch\n            def my_method(self, arg):\n                return \"Default implementation\"\n\n            @my_method.register(int)\n            def _(self, arg: int):\n                return f\"Integer implementation: {arg}\"\n        ```\n\n    Args:\n        func (Callable[..., R]): The function to be wrapped.\n\n    Returns:\n        DispatchableMethod[R]: A wrapper function with dispatch capabilities.\n\n    Note:\n        The wrapped method will dispatch based on the type of the second argument,\n        which is typically the first argument after 'self' in method calls.\n    \"\"\"\n    dispatcher = singledispatch(func)\n\n    def wrapper(*args, **kw):\n        return dispatcher.dispatch(args[1].__class__)(*args, **kw)\n\n    wrapper = cast(DispatchableMethod[R], wrapper)\n    setattr(wrapper, \"register\", dispatcher.register)\n    update_wrapper(wrapper, func)\n    return wrapper\n</code></pre>"},{"location":"api/plugins/filters/cache/","title":"CachedFilter","text":""},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter","title":"<code>framework3.plugins.filters.cache.cached_filter</code>","text":""},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.__all__","title":"<code>__all__ = ['Cached']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached","title":"<code>Cached</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A filter that manages the storage of models and data in a BaseStorage type.</p> <p>This class extends BaseFilter to provide caching capabilities for both the filter model and the processed data. It allows for efficient reuse of previously computed results and trained models.</p> Key Features <ul> <li>Caches both filter models and processed data</li> <li>Supports various storage backends through BaseStorage</li> <li>Allows for overwriting existing cached data</li> <li>Provides methods for managing the cache</li> </ul> Usage <p>The Cached filter can be used to wrap any BaseFilter, providing caching capabilities:</p> <pre><code>from framework3.storage import LocalStorage\nfrom framework3.container import Container\nfrom your_custom_filter import CustomFilter\n\n# Configure storage\nContainer.storage = LocalStorage(storage_path='cache')\n\n# Create a custom filter and wrap it with Cached\ncustom_filter = CustomFilter()\ncached_filter = Cached(\n    filter=custom_filter,\n    cache_data=True,\n    cache_filter=True,\n    overwrite=False\n)\n\n# Use the cached filter\nX = XYData(_hash='input_data', _path='/datasets', _value=input_data)\ny = XYData(_hash='target_data', _path='/datasets', _value=target_data)\n\ncached_filter.fit(X, y)\npredictions = cached_filter.predict(X)\n\n# Clear the cache if needed\ncached_filter.clear_cache()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>BaseFilter</code> <p>The underlying filter to be cached.</p> required <code>cache_data</code> <code>bool</code> <p>Whether to cache the processed data.</p> <code>True</code> <code>cache_filter</code> <code>bool</code> <p>Whether to cache the trained filter.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing cached data/models.</p> <code>False</code> <code>storage</code> <code>BaseStorage | None</code> <p>The storage backend for caching.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>filter</code> <code>BaseFilter</code> <p>The underlying filter being cached.</p> <code>cache_data</code> <code>bool</code> <p>Flag indicating whether to cache processed data.</p> <code>cache_filter</code> <code>bool</code> <p>Flag indicating whether to cache the trained filter.</p> <code>overwrite</code> <code>bool</code> <p>Flag indicating whether to overwrite existing cached data/models.</p> <code>_storage</code> <code>BaseStorage</code> <p>The storage backend used for caching.</p> <code>_lambda_filter</code> <code>Callable[..., BaseFilter] | None</code> <p>Lambda function for lazy loading of cached filter.</p> <p>Methods:</p> Name Description <code>init</code> <p>Initialize the cached filter.</p> <code>fit</code> <p>XYData, y: Optional[XYData]): Fit the filter to the input data, caching the model if necessary.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the filter, caching the results if necessary.</p> <code>clear_cache</code> <p>Clear the cache in the storage.</p> Note <p>The caching behavior can be customized by adjusting the cache_data, cache_filter, and overwrite parameters. The storage backend can be changed by providing a different BaseStorage implementation.</p> Source code in <code>framework3/plugins/filters/cache/cached_filter.py</code> <pre><code>@Container.bind()\nclass Cached(BaseFilter):\n    \"\"\"\n    A filter that manages the storage of models and data in a BaseStorage type.\n\n    This class extends BaseFilter to provide caching capabilities for both the filter model\n    and the processed data. It allows for efficient reuse of previously computed results\n    and trained models.\n\n    Key Features:\n        - Caches both filter models and processed data\n        - Supports various storage backends through BaseStorage\n        - Allows for overwriting existing cached data\n        - Provides methods for managing the cache\n\n    Usage:\n        The Cached filter can be used to wrap any BaseFilter, providing caching capabilities:\n\n        ```python\n        from framework3.storage import LocalStorage\n        from framework3.container import Container\n        from your_custom_filter import CustomFilter\n\n        # Configure storage\n        Container.storage = LocalStorage(storage_path='cache')\n\n        # Create a custom filter and wrap it with Cached\n        custom_filter = CustomFilter()\n        cached_filter = Cached(\n            filter=custom_filter,\n            cache_data=True,\n            cache_filter=True,\n            overwrite=False\n        )\n\n        # Use the cached filter\n        X = XYData(_hash='input_data', _path='/datasets', _value=input_data)\n        y = XYData(_hash='target_data', _path='/datasets', _value=target_data)\n\n        cached_filter.fit(X, y)\n        predictions = cached_filter.predict(X)\n\n        # Clear the cache if needed\n        cached_filter.clear_cache()\n        ```\n\n    Args:\n        filter (BaseFilter): The underlying filter to be cached.\n        cache_data (bool): Whether to cache the processed data.\n        cache_filter (bool): Whether to cache the trained filter.\n        overwrite (bool): Whether to overwrite existing cached data/models.\n        storage (BaseStorage|None): The storage backend for caching.\n\n    Attributes:\n        filter (BaseFilter): The underlying filter being cached.\n        cache_data (bool): Flag indicating whether to cache processed data.\n        cache_filter (bool): Flag indicating whether to cache the trained filter.\n        overwrite (bool): Flag indicating whether to overwrite existing cached data/models.\n        _storage (BaseStorage): The storage backend used for caching.\n        _lambda_filter (Callable[..., BaseFilter] | None): Lambda function for lazy loading of cached filter.\n\n    Methods:\n        init(): Initialize the cached filter.\n        fit(x: XYData, y: Optional[XYData]): Fit the filter to the input data, caching the model if necessary.\n        predict(x: XYData) -&gt; XYData: Make predictions using the filter, caching the results if necessary.\n        clear_cache(): Clear the cache in the storage.\n\n    Note:\n        The caching behavior can be customized by adjusting the cache_data, cache_filter, and overwrite parameters.\n        The storage backend can be changed by providing a different BaseStorage implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        filter: BaseFilter,\n        cache_data: bool = True,\n        cache_filter: bool = True,\n        overwrite: bool = False,\n        storage: BaseStorage | None = None,\n    ):\n        \"\"\"\n        Initialize a new Cached filter instance.\n\n        This constructor sets up the Cached filter with the specified parameters and\n        initializes the underlying filter and storage backend.\n\n        Args:\n            filter (BaseFilter): The underlying filter to be cached.\n            cache_data (bool, optional): Whether to cache the processed data. Defaults to True.\n            cache_filter (bool, optional): Whether to cache the trained filter. Defaults to True.\n            overwrite (bool, optional): Whether to overwrite existing cached data/models. Defaults to False.\n            storage (BaseStorage | None, optional): The storage backend for caching. If None, uses the Container's storage. Defaults to None.\n\n        Note:\n            If no storage is provided, the method will use the storage defined in the Container.\n            The _lambda_filter attribute is initialized as None and will be set later if needed.\n        \"\"\"\n        super().__init__(\n            filter=filter,\n            cache_data=cache_data,\n            cache_filter=cache_filter,\n            overwrite=overwrite,\n            storage=storage,\n        )\n        self.filter: BaseFilter = filter\n        self.cache_data = cache_data\n        self.cache_filter = cache_filter\n        self.overwrite = overwrite\n        self._storage: BaseStorage = Container.storage if storage is None else storage\n        self._lambda_filter: Callable[..., BaseFilter] | None = None\n\n    def verbose(self, value: bool):\n        super().verbose(value)\n        self._storage._verbose = value\n        self.filter.verbose(value)\n\n    def init(self) -&gt; None:\n        \"\"\"\n        Initialize the cached filter.\n\n        This method initializes both the underlying filter and the Cached filter itself.\n        \"\"\"\n        self.filter.init()\n        super().init()\n\n    def _pre_fit_wrapp(self, x: XYData, y: Optional[XYData]) -&gt; float | None:\n        \"\"\"\n        Wrapper method for the pre-fit stage.\n\n        Args:\n            x (XYData): The input data.\n            y (Optional[XYData]): The target data, if any.\n\n        Returns:\n            float | None: The result of the original fit method.\n        \"\"\"\n        return self._original_fit(x, y)\n\n    def _pre_predict_wrapp(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Wrapper method for the pre-predict stage.\n\n        Args:\n            x (XYData): The input data for prediction.\n\n        Returns:\n            XYData: The result of the original predict method.\n        \"\"\"\n        return self._original_predict(x)\n\n    def _get_model_name(self) -&gt; str:\n        \"\"\"\n        Get the name of the underlying filter's model.\n\n        Returns:\n            str: The model name.\n        \"\"\"\n        return self.filter._get_model_name()\n\n    def _get_model_key(self, data_hash: str) -&gt; Tuple[str, str]:\n        \"\"\"\n        Generate the model key based on the input data hash.\n\n        Args:\n            data_hash (str): The hash of the input data.\n\n        Returns:\n            Tuple[str, str]: A tuple containing the model hash and its string representation.\n        \"\"\"\n        return BaseFilter._get_model_key(self.filter, data_hash)\n\n    def _get_data_key(self, model_str: str, data_hash: str) -&gt; Tuple[str, str]:\n        \"\"\"\n        Generate the data key based on the model and input data hash.\n\n        Args:\n            model_str (str): The string representation of the model.\n            data_hash (str): The hash of the input data.\n\n        Returns:\n            Tuple[str, str]: A tuple containing the data hash and its string representation.\n        \"\"\"\n        return BaseFilter._get_data_key(self.filter, model_str, data_hash)\n\n    def fit(self, x: XYData, y: Optional[XYData]) -&gt; None:\n        \"\"\"\n        Fit the filter to the input data, caching the model if necessary.\n\n        This method checks if a cached model exists and uses it if available.\n        If not, it trains the model and caches it if caching is enabled.\n\n        Args:\n            x (XYData): The input data.\n            y (Optional[XYData]): The target data, if any.\n        \"\"\"\n\n        self.filter._pre_fit(x, y)\n\n        if (\n            not self._storage.check_if_exists(\n                hashcode=\"model\",\n                context=f\"{self._storage.get_root_path()}/{self.filter._m_path}\",\n            )\n            or self.overwrite\n        ):\n            if self._verbose:\n                rprint(\n                    f\"\\t - El filtro {self.filter} con hash {self.filter._m_hash} No existe, se va a entrenar.\"\n                )\n            self.filter._original_fit(x, y)\n\n            if self.cache_filter and method_is_overridden(self.filter.__class__, \"fit\"):\n                if self._verbose:\n                    rprint(f\"\\t - El filtro {self.filter} Se cachea.\")\n                self._storage.upload_file(\n                    file=pickle.dumps(self.filter),\n                    file_name=\"model\",\n                    context=f\"{self._storage.get_root_path()}/{self.filter._m_path}\",\n                )\n        else:\n            if self._verbose:\n                rprint(f\"\\t - El filtro {self.filter} Existe, se carga del storage.\")\n\n            self._lambda_filter = lambda: cast(\n                BaseFilter,\n                self._storage.download_file(\n                    \"model\", f\"{self._storage.get_root_path()}/{self.filter._m_path}\"\n                ),\n            )\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the filter, caching the results if necessary.\n\n        This method checks if cached predictions exist and uses them if available.\n        If not, it makes new predictions and caches them if caching is enabled.\n\n        Args:\n            x (XYData): The input data for prediction.\n\n        Returns:\n            XYData: The prediction results.\n        \"\"\"\n        x = self.filter._pre_predict(x)\n\n        if (\n            not self._storage.check_if_exists(\n                x._hash, context=f\"{self._storage.get_root_path()}/{x._path}\"\n            )\n            or self.overwrite\n        ):\n            if self._verbose:\n                rprint(f\"\\t - El dato {x} No existe, se va a crear.\")\n\n            if self._lambda_filter is not None:\n                if self._verbose:\n                    rprint(\n                        \"\\t - Existe un Lambda por lo que se recupera el filtro del storage.\"\n                    )\n                self.filter = self._lambda_filter()\n\n            value = XYData(\n                _hash=x._hash,\n                _path=x._path,\n                _value=self.filter._original_predict(x)._value,\n            )\n            if self.cache_data:\n                if self._verbose:\n                    rprint(f\"\\t - El dato {x} Se cachea.\")\n\n                self._storage.upload_file(\n                    file=pickle.dumps(value.value),\n                    file_name=x._hash,\n                    context=f\"{self._storage.get_root_path()}/{x._path}\",\n                )\n        else:\n            if self._verbose:\n                rprint(f\"\\t - El dato {x} Existe, se carga del storage.\")\n\n            value = XYData(\n                _hash=x._hash,\n                _path=x._path,\n                _value=lambda: cast(\n                    VData,\n                    self._storage.download_file(\n                        x._hash, f\"{self._storage.get_root_path()}/{x._path}\"\n                    ),\n                ),\n            )\n        return value\n\n    def clear_cache(self):\n        \"\"\"\n        Clear the cache in the storage.\n\n        This method should implement the logic to clear all cached data and models\n        associated with this filter from the storage backend.\n\n        Note:\n            This method is not yet implemented.\n        \"\"\"\n        # Implementa la l\u00f3gica para limpiar el cach\u00e9 en el almacenamiento\n        raise NotImplementedError(\"El m\u00e9todo clear_cache no est\u00e1 implementado.\")\n        pass\n</code></pre>"},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.cache_data","title":"<code>cache_data = cache_data</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.cache_filter","title":"<code>cache_filter = cache_filter</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.filter","title":"<code>filter = filter</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.overwrite","title":"<code>overwrite = overwrite</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.__init__","title":"<code>__init__(filter, cache_data=True, cache_filter=True, overwrite=False, storage=None)</code>","text":"<p>Initialize a new Cached filter instance.</p> <p>This constructor sets up the Cached filter with the specified parameters and initializes the underlying filter and storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>BaseFilter</code> <p>The underlying filter to be cached.</p> required <code>cache_data</code> <code>bool</code> <p>Whether to cache the processed data. Defaults to True.</p> <code>True</code> <code>cache_filter</code> <code>bool</code> <p>Whether to cache the trained filter. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing cached data/models. Defaults to False.</p> <code>False</code> <code>storage</code> <code>BaseStorage | None</code> <p>The storage backend for caching. If None, uses the Container's storage. Defaults to None.</p> <code>None</code> Note <p>If no storage is provided, the method will use the storage defined in the Container. The _lambda_filter attribute is initialized as None and will be set later if needed.</p> Source code in <code>framework3/plugins/filters/cache/cached_filter.py</code> <pre><code>def __init__(\n    self,\n    filter: BaseFilter,\n    cache_data: bool = True,\n    cache_filter: bool = True,\n    overwrite: bool = False,\n    storage: BaseStorage | None = None,\n):\n    \"\"\"\n    Initialize a new Cached filter instance.\n\n    This constructor sets up the Cached filter with the specified parameters and\n    initializes the underlying filter and storage backend.\n\n    Args:\n        filter (BaseFilter): The underlying filter to be cached.\n        cache_data (bool, optional): Whether to cache the processed data. Defaults to True.\n        cache_filter (bool, optional): Whether to cache the trained filter. Defaults to True.\n        overwrite (bool, optional): Whether to overwrite existing cached data/models. Defaults to False.\n        storage (BaseStorage | None, optional): The storage backend for caching. If None, uses the Container's storage. Defaults to None.\n\n    Note:\n        If no storage is provided, the method will use the storage defined in the Container.\n        The _lambda_filter attribute is initialized as None and will be set later if needed.\n    \"\"\"\n    super().__init__(\n        filter=filter,\n        cache_data=cache_data,\n        cache_filter=cache_filter,\n        overwrite=overwrite,\n        storage=storage,\n    )\n    self.filter: BaseFilter = filter\n    self.cache_data = cache_data\n    self.cache_filter = cache_filter\n    self.overwrite = overwrite\n    self._storage: BaseStorage = Container.storage if storage is None else storage\n    self._lambda_filter: Callable[..., BaseFilter] | None = None\n</code></pre>"},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache in the storage.</p> <p>This method should implement the logic to clear all cached data and models associated with this filter from the storage backend.</p> Note <p>This method is not yet implemented.</p> Source code in <code>framework3/plugins/filters/cache/cached_filter.py</code> <pre><code>def clear_cache(self):\n    \"\"\"\n    Clear the cache in the storage.\n\n    This method should implement the logic to clear all cached data and models\n    associated with this filter from the storage backend.\n\n    Note:\n        This method is not yet implemented.\n    \"\"\"\n    # Implementa la l\u00f3gica para limpiar el cach\u00e9 en el almacenamiento\n    raise NotImplementedError(\"El m\u00e9todo clear_cache no est\u00e1 implementado.\")\n    pass\n</code></pre>"},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.fit","title":"<code>fit(x, y)</code>","text":"<p>Fit the filter to the input data, caching the model if necessary.</p> <p>This method checks if a cached model exists and uses it if available. If not, it trains the model and caches it if caching is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target data, if any.</p> required Source code in <code>framework3/plugins/filters/cache/cached_filter.py</code> <pre><code>def fit(self, x: XYData, y: Optional[XYData]) -&gt; None:\n    \"\"\"\n    Fit the filter to the input data, caching the model if necessary.\n\n    This method checks if a cached model exists and uses it if available.\n    If not, it trains the model and caches it if caching is enabled.\n\n    Args:\n        x (XYData): The input data.\n        y (Optional[XYData]): The target data, if any.\n    \"\"\"\n\n    self.filter._pre_fit(x, y)\n\n    if (\n        not self._storage.check_if_exists(\n            hashcode=\"model\",\n            context=f\"{self._storage.get_root_path()}/{self.filter._m_path}\",\n        )\n        or self.overwrite\n    ):\n        if self._verbose:\n            rprint(\n                f\"\\t - El filtro {self.filter} con hash {self.filter._m_hash} No existe, se va a entrenar.\"\n            )\n        self.filter._original_fit(x, y)\n\n        if self.cache_filter and method_is_overridden(self.filter.__class__, \"fit\"):\n            if self._verbose:\n                rprint(f\"\\t - El filtro {self.filter} Se cachea.\")\n            self._storage.upload_file(\n                file=pickle.dumps(self.filter),\n                file_name=\"model\",\n                context=f\"{self._storage.get_root_path()}/{self.filter._m_path}\",\n            )\n    else:\n        if self._verbose:\n            rprint(f\"\\t - El filtro {self.filter} Existe, se carga del storage.\")\n\n        self._lambda_filter = lambda: cast(\n            BaseFilter,\n            self._storage.download_file(\n                \"model\", f\"{self._storage.get_root_path()}/{self.filter._m_path}\"\n            ),\n        )\n</code></pre>"},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.init","title":"<code>init()</code>","text":"<p>Initialize the cached filter.</p> <p>This method initializes both the underlying filter and the Cached filter itself.</p> Source code in <code>framework3/plugins/filters/cache/cached_filter.py</code> <pre><code>def init(self) -&gt; None:\n    \"\"\"\n    Initialize the cached filter.\n\n    This method initializes both the underlying filter and the Cached filter itself.\n    \"\"\"\n    self.filter.init()\n    super().init()\n</code></pre>"},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the filter, caching the results if necessary.</p> <p>This method checks if cached predictions exist and uses them if available. If not, it makes new predictions and caches them if caching is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data for prediction.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The prediction results.</p> Source code in <code>framework3/plugins/filters/cache/cached_filter.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the filter, caching the results if necessary.\n\n    This method checks if cached predictions exist and uses them if available.\n    If not, it makes new predictions and caches them if caching is enabled.\n\n    Args:\n        x (XYData): The input data for prediction.\n\n    Returns:\n        XYData: The prediction results.\n    \"\"\"\n    x = self.filter._pre_predict(x)\n\n    if (\n        not self._storage.check_if_exists(\n            x._hash, context=f\"{self._storage.get_root_path()}/{x._path}\"\n        )\n        or self.overwrite\n    ):\n        if self._verbose:\n            rprint(f\"\\t - El dato {x} No existe, se va a crear.\")\n\n        if self._lambda_filter is not None:\n            if self._verbose:\n                rprint(\n                    \"\\t - Existe un Lambda por lo que se recupera el filtro del storage.\"\n                )\n            self.filter = self._lambda_filter()\n\n        value = XYData(\n            _hash=x._hash,\n            _path=x._path,\n            _value=self.filter._original_predict(x)._value,\n        )\n        if self.cache_data:\n            if self._verbose:\n                rprint(f\"\\t - El dato {x} Se cachea.\")\n\n            self._storage.upload_file(\n                file=pickle.dumps(value.value),\n                file_name=x._hash,\n                context=f\"{self._storage.get_root_path()}/{x._path}\",\n            )\n    else:\n        if self._verbose:\n            rprint(f\"\\t - El dato {x} Existe, se carga del storage.\")\n\n        value = XYData(\n            _hash=x._hash,\n            _path=x._path,\n            _value=lambda: cast(\n                VData,\n                self._storage.download_file(\n                    x._hash, f\"{self._storage.get_root_path()}/{x._path}\"\n                ),\n            ),\n        )\n    return value\n</code></pre>"},{"location":"api/plugins/filters/cache/#framework3.plugins.filters.cache.cached_filter.Cached.verbose","title":"<code>verbose(value)</code>","text":"Source code in <code>framework3/plugins/filters/cache/cached_filter.py</code> <pre><code>def verbose(self, value: bool):\n    super().verbose(value)\n    self._storage._verbose = value\n    self.filter.verbose(value)\n</code></pre>"},{"location":"api/plugins/filters/classification/","title":"Classification Filters","text":""},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.ClassifierSVMPlugin","title":"<code>ClassifierSVMPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code>, <code>BasePlugin</code></p> <p>A plugin for Support Vector Machine (SVM) classification using scikit-learn's SVC.</p> <p>This plugin integrates the SVC (Support Vector Classification) implementation from scikit-learn into the framework3 ecosystem, allowing for seamless use of SVM classification in pipelines and supporting hyperparameter tuning through grid search.</p> Key Features <ul> <li>Wraps scikit-learn's SVC for use within framework3</li> <li>Supports various kernel types: linear, polynomial, RBF, and sigmoid</li> <li>Allows customization of regularization parameter (C) and kernel coefficient (gamma)</li> <li>Provides methods for fitting the model, making predictions, and generating parameter grids</li> </ul> Usage <p>The ClassifierSVMPlugin can be used to perform SVM classification on your data:</p> <pre><code>from framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\ny_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n# Create and fit the SVM classifier\nsvm_plugin = ClassifierSVMPlugin(C=1.0, kernel='rbf', gamma='scale')\nsvm_plugin.fit(X_data, y_data)\n\n# Make predictions\nX_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\npredictions = svm_plugin.predict(X_test)\nprint(predictions.value)\n\n# Generate parameter grid for hyperparameter tuning\ngrid_params = ClassifierSVMPlugin.item_grid(C=[0.1, 1, 10], kernel=['linear', 'rbf'], gamma=['scale', 'auto'])\nprint(grid_params)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_model</code> <code>SVC</code> <p>The underlying scikit-learn SVC model.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the SVM model to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the fitted SVM model.</p> <code>item_grid</code> <p>List[float], kernel: List[L], gamma: List[float | Literal['scale', 'auto']]) -&gt; Dict[str, List[Any]]: Generate a parameter grid for hyperparameter tuning.</p> Note <p>This plugin uses scikit-learn's implementation of SVM, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/classification/svm.py</code> <pre><code>@Container.bind()\nclass ClassifierSVMPlugin(BaseFilter, BasePlugin):\n    \"\"\"\n    A plugin for Support Vector Machine (SVM) classification using scikit-learn's SVC.\n\n    This plugin integrates the SVC (Support Vector Classification) implementation from scikit-learn\n    into the framework3 ecosystem, allowing for seamless use of SVM classification in pipelines\n    and supporting hyperparameter tuning through grid search.\n\n    Key Features:\n        - Wraps scikit-learn's SVC for use within framework3\n        - Supports various kernel types: linear, polynomial, RBF, and sigmoid\n        - Allows customization of regularization parameter (C) and kernel coefficient (gamma)\n        - Provides methods for fitting the model, making predictions, and generating parameter grids\n\n    Usage:\n        The ClassifierSVMPlugin can be used to perform SVM classification on your data:\n\n        ```python\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        X_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n        y_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n        # Create and fit the SVM classifier\n        svm_plugin = ClassifierSVMPlugin(C=1.0, kernel='rbf', gamma='scale')\n        svm_plugin.fit(X_data, y_data)\n\n        # Make predictions\n        X_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\n        predictions = svm_plugin.predict(X_test)\n        print(predictions.value)\n\n        # Generate parameter grid for hyperparameter tuning\n        grid_params = ClassifierSVMPlugin.item_grid(C=[0.1, 1, 10], kernel=['linear', 'rbf'], gamma=['scale', 'auto'])\n        print(grid_params)\n        ```\n\n    Attributes:\n        _model (SVC): The underlying scikit-learn SVC model.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the SVM model to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Make predictions using the fitted SVM model.\n        item_grid(C: List[float], kernel: List[L], gamma: List[float | Literal['scale', 'auto']]) -&gt; Dict[str, List[Any]]:\n            Generate a parameter grid for hyperparameter tuning.\n\n    Note:\n        This plugin uses scikit-learn's implementation of SVM, which may have its own dependencies and requirements.\n        Ensure that scikit-learn is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        C: float = 1.0,\n        gamma: float | Literal[\"scale\", \"auto\"] = \"scale\",\n        kernel: L = \"linear\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize a new ClassifierSVMPlugin instance.\n\n        This constructor sets up the ClassifierSVMPlugin with the specified parameters and\n        initializes the underlying scikit-learn SVC model.\n\n        Args:\n            C (float): Regularization parameter. Defaults to 1.0.\n            gamma (float | Literal[\"scale\", \"auto\"]): Kernel coefficient. Defaults to \"scale\".\n            kernel (L): Specifies the kernel type to be used in the algorithm.\n                        Can be 'linear', 'poly', 'rbf', or 'sigmoid'. Defaults to \"linear\".\n\n        Note:\n            The parameters are passed directly to scikit-learn's SVC.\n            Refer to scikit-learn's documentation for detailed information on these parameters.\n        \"\"\"\n        super().__init__(C=C, kernel=kernel, gamma=gamma)\n        self._model = SVC(C=C, kernel=kernel, gamma=gamma)\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the SVM model to the given data.\n\n        This method trains the SVM classifier on the provided input features and target values.\n\n        Args:\n            x (XYData): The input features for training.\n            y (Optional[XYData]): The target values for training.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: The score of the fitted model on the training data, or None if y is None.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The score is calculated using scikit-learn's score method, which computes the mean accuracy.\n        \"\"\"\n        if y is not None:\n            self._model.fit(x.value, y.value)  # type: ignore\n            return self._model.score(x.value, y.value)  # type: ignore\n        return None\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted SVM model.\n\n        This method uses the trained SVM classifier to make predictions on new input data.\n\n        Args:\n            x (XYData): The input features to predict.\n\n        Returns:\n            (XYData): The predicted values wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's predict method internally.\n            The predictions are wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._model.predict(x.value))\n\n    @staticmethod\n    def item_grid(\n        C: List[float],\n        kernel: List[L],\n        gamma: List[float] | List[Literal[\"scale\", \"auto\"]] = [\"scale\"],  # type: ignore[assignment]\n    ) -&gt; Dict[str, List[Any]]:\n        \"\"\"\n        Generate a parameter grid for hyperparameter tuning.\n\n        This static method provides a way to generate a grid of parameters for use in\n        hyperparameter optimization techniques like grid search.\n\n        Args:\n            C (List[float]): List of regularization parameter values to try.\n            kernel (List[L]): List of kernel types to try.\n            gamma (List[float] | List[Literal['scale', 'auto']]): List of gamma values to try. Defaults to [\"scale\"].\n\n        Returns:\n            Dict[str, List[Any]]: A dictionary of parameter names and their possible values.\n\n        Note:\n            The returned dictionary can be used directly with hyperparameter tuning tools\n            that accept parameter grids, such as scikit-learn's GridSearchCV.\n        \"\"\"\n        return {\n            \"ClassifierSVMPlugin__C\": C,\n            \"ClassifierSVMPlugin__kernel\": kernel,\n            \"ClassifierSVMPlugin__gamma\": gamma,\n        }\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.ClassifierSVMPlugin.__init__","title":"<code>__init__(C=1.0, gamma='scale', kernel='linear')</code>","text":"<p>Initialize a new ClassifierSVMPlugin instance.</p> <p>This constructor sets up the ClassifierSVMPlugin with the specified parameters and initializes the underlying scikit-learn SVC model.</p> <p>Parameters:</p> Name Type Description Default <code>C</code> <code>float</code> <p>Regularization parameter. Defaults to 1.0.</p> <code>1.0</code> <code>gamma</code> <code>float | Literal['scale', 'auto']</code> <p>Kernel coefficient. Defaults to \"scale\".</p> <code>'scale'</code> <code>kernel</code> <code>L</code> <p>Specifies the kernel type to be used in the algorithm.         Can be 'linear', 'poly', 'rbf', or 'sigmoid'. Defaults to \"linear\".</p> <code>'linear'</code> Note <p>The parameters are passed directly to scikit-learn's SVC. Refer to scikit-learn's documentation for detailed information on these parameters.</p> Source code in <code>framework3/plugins/filters/classification/svm.py</code> <pre><code>def __init__(\n    self,\n    C: float = 1.0,\n    gamma: float | Literal[\"scale\", \"auto\"] = \"scale\",\n    kernel: L = \"linear\",\n) -&gt; None:\n    \"\"\"\n    Initialize a new ClassifierSVMPlugin instance.\n\n    This constructor sets up the ClassifierSVMPlugin with the specified parameters and\n    initializes the underlying scikit-learn SVC model.\n\n    Args:\n        C (float): Regularization parameter. Defaults to 1.0.\n        gamma (float | Literal[\"scale\", \"auto\"]): Kernel coefficient. Defaults to \"scale\".\n        kernel (L): Specifies the kernel type to be used in the algorithm.\n                    Can be 'linear', 'poly', 'rbf', or 'sigmoid'. Defaults to \"linear\".\n\n    Note:\n        The parameters are passed directly to scikit-learn's SVC.\n        Refer to scikit-learn's documentation for detailed information on these parameters.\n    \"\"\"\n    super().__init__(C=C, kernel=kernel, gamma=gamma)\n    self._model = SVC(C=C, kernel=kernel, gamma=gamma)\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.ClassifierSVMPlugin.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the SVM model to the given data.</p> <p>This method trains the SVM classifier on the provided input features and target values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features for training.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target values for training.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The score of the fitted model on the training data, or None if y is None.</p> Note <p>This method uses scikit-learn's fit method internally. The score is calculated using scikit-learn's score method, which computes the mean accuracy.</p> Source code in <code>framework3/plugins/filters/classification/svm.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the SVM model to the given data.\n\n    This method trains the SVM classifier on the provided input features and target values.\n\n    Args:\n        x (XYData): The input features for training.\n        y (Optional[XYData]): The target values for training.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: The score of the fitted model on the training data, or None if y is None.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The score is calculated using scikit-learn's score method, which computes the mean accuracy.\n    \"\"\"\n    if y is not None:\n        self._model.fit(x.value, y.value)  # type: ignore\n        return self._model.score(x.value, y.value)  # type: ignore\n    return None\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.ClassifierSVMPlugin.item_grid","title":"<code>item_grid(C, kernel, gamma=['scale'])</code>  <code>staticmethod</code>","text":"<p>Generate a parameter grid for hyperparameter tuning.</p> <p>This static method provides a way to generate a grid of parameters for use in hyperparameter optimization techniques like grid search.</p> <p>Parameters:</p> Name Type Description Default <code>C</code> <code>List[float]</code> <p>List of regularization parameter values to try.</p> required <code>kernel</code> <code>List[L]</code> <p>List of kernel types to try.</p> required <code>gamma</code> <code>List[float] | List[Literal['scale', 'auto']]</code> <p>List of gamma values to try. Defaults to [\"scale\"].</p> <code>['scale']</code> <p>Returns:</p> Type Description <code>Dict[str, List[Any]]</code> <p>Dict[str, List[Any]]: A dictionary of parameter names and their possible values.</p> Note <p>The returned dictionary can be used directly with hyperparameter tuning tools that accept parameter grids, such as scikit-learn's GridSearchCV.</p> Source code in <code>framework3/plugins/filters/classification/svm.py</code> <pre><code>@staticmethod\ndef item_grid(\n    C: List[float],\n    kernel: List[L],\n    gamma: List[float] | List[Literal[\"scale\", \"auto\"]] = [\"scale\"],  # type: ignore[assignment]\n) -&gt; Dict[str, List[Any]]:\n    \"\"\"\n    Generate a parameter grid for hyperparameter tuning.\n\n    This static method provides a way to generate a grid of parameters for use in\n    hyperparameter optimization techniques like grid search.\n\n    Args:\n        C (List[float]): List of regularization parameter values to try.\n        kernel (List[L]): List of kernel types to try.\n        gamma (List[float] | List[Literal['scale', 'auto']]): List of gamma values to try. Defaults to [\"scale\"].\n\n    Returns:\n        Dict[str, List[Any]]: A dictionary of parameter names and their possible values.\n\n    Note:\n        The returned dictionary can be used directly with hyperparameter tuning tools\n        that accept parameter grids, such as scikit-learn's GridSearchCV.\n    \"\"\"\n    return {\n        \"ClassifierSVMPlugin__C\": C,\n        \"ClassifierSVMPlugin__kernel\": kernel,\n        \"ClassifierSVMPlugin__gamma\": gamma,\n    }\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.ClassifierSVMPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted SVM model.</p> <p>This method uses the trained SVM classifier to make predictions on new input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to predict.</p> required <p>Returns:</p> Type Description <code>XYData</code> <p>The predicted values wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's predict method internally. The predictions are wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/classification/svm.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted SVM model.\n\n    This method uses the trained SVM classifier to make predictions on new input data.\n\n    Args:\n        x (XYData): The input features to predict.\n\n    Returns:\n        (XYData): The predicted values wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's predict method internally.\n        The predictions are wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._model.predict(x.value))\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.KnnFilter","title":"<code>KnnFilter</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A wrapper for scikit-learn's KNeighborsClassifier using the framework3 BaseFilter interface.</p> <p>This filter implements the K-Nearest Neighbors algorithm for classification within the framework3 ecosystem.</p> Key Features <ul> <li>Integrates scikit-learn's KNeighborsClassifier with framework3</li> <li>Supports various KNN parameters like number of neighbors, weights, and distance metrics</li> <li>Provides methods for fitting the model and making predictions</li> <li>Includes a static method for generating parameter grids for hyperparameter tuning</li> </ul> Usage <p>The KnnFilter can be used to perform K-Nearest Neighbors classification on your data:</p> <pre><code>from framework3.plugins.filters.classification.knn import KnnFilter\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\ny_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n# Create and fit the KNN filter\nknn = KnnFilter(n_neighbors=3, weights='uniform')\nknn.fit(X_data, y_data)\n\n# Make predictions\nX_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\npredictions = knn.predict(X_test)\nprint(predictions.value)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_clf</code> <code>KNeighborsClassifier</code> <p>The underlying scikit-learn KNN classifier.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the KNN model to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the fitted KNN model.</p> <code>item_grid</code> <p>Generate a parameter grid for hyperparameter tuning.</p> Note <p>This filter uses scikit-learn's implementation of KNN, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/classification/knn.py</code> <pre><code>@Container.bind()\nclass KnnFilter(BaseFilter):\n    \"\"\"\n    A wrapper for scikit-learn's KNeighborsClassifier using the framework3 BaseFilter interface.\n\n    This filter implements the K-Nearest Neighbors algorithm for classification within the framework3 ecosystem.\n\n    Key Features:\n        - Integrates scikit-learn's KNeighborsClassifier with framework3\n        - Supports various KNN parameters like number of neighbors, weights, and distance metrics\n        - Provides methods for fitting the model and making predictions\n        - Includes a static method for generating parameter grids for hyperparameter tuning\n\n    Usage:\n        The KnnFilter can be used to perform K-Nearest Neighbors classification on your data:\n\n        ```python\n        from framework3.plugins.filters.classification.knn import KnnFilter\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        X_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n        y_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n        # Create and fit the KNN filter\n        knn = KnnFilter(n_neighbors=3, weights='uniform')\n        knn.fit(X_data, y_data)\n\n        # Make predictions\n        X_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\n        predictions = knn.predict(X_test)\n        print(predictions.value)\n        ```\n\n    Attributes:\n        _clf (KNeighborsClassifier): The underlying scikit-learn KNN classifier.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the KNN model to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Make predictions using the fitted KNN model.\n        item_grid(**kwargs) -&gt; tuple[type[BaseFilter], Dict[str, List[Any]]]:\n            Generate a parameter grid for hyperparameter tuning.\n\n    Note:\n        This filter uses scikit-learn's implementation of KNN, which may have its own dependencies and requirements.\n        Ensure that scikit-learn is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_neighbors: int = 5,\n        weights: Literal[\"uniform\", \"distance\"] = \"uniform\",\n        algorithm: Literal[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"] = \"auto\",\n        leaf_size: int = 30,\n        p: int = 2,\n        metric: str = \"minkowski\",\n        metric_params: Optional[Dict[str, Any]] = None,\n        n_jobs: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize a new KnnFilter instance.\n\n        This constructor sets up the KnnFilter with the specified parameters and\n        initializes the underlying scikit-learn KNeighborsClassifier.\n\n        Args:\n            n_neighbors (int): Number of neighbors to use for knn. Defaults to 5.\n            weights (Literal[\"uniform\", \"distance\"]): Weight function used in prediction. Defaults to \"uniform\".\n            algorithm (Literal[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]): Algorithm used to compute nearest neighbors. Defaults to \"auto\".\n            leaf_size (int): Leaf size passed to BallTree or KDTree. Defaults to 30.\n            p (int): Power parameter for the Minkowski metric. Defaults to 2 (Euclidean distance).\n            metric (str): The distance metric to use for the tree. Defaults to \"minkowski\".\n            metric_params (Optional[Dict[str, Any]]): Additional keyword arguments for the metric function. Defaults to None.\n            n_jobs (Optional[int]): The number of parallel jobs to run for neighbors search. Defaults to None.\n\n        Note:\n            The parameters are passed directly to scikit-learn's KNeighborsClassifier.\n            Refer to scikit-learn's documentation for detailed information on these parameters.\n        \"\"\"\n        super().__init__(\n            n_neighbors=n_neighbors,\n            weights=weights,\n            algorithm=algorithm,\n            leaf_size=leaf_size,\n            p=p,\n            metric=metric,\n            metric_params=metric_params,\n            n_jobs=n_jobs,\n        )\n        self._clf = KNeighborsClassifier(\n            n_neighbors=n_neighbors,\n            weights=weights,\n            algorithm=algorithm,\n            leaf_size=leaf_size,\n            p=p,\n            metric=metric,\n            metric_params=metric_params,\n            n_jobs=n_jobs,\n        )\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the KNN model to the given data.\n\n        This method trains the KNN classifier on the provided input features and target values.\n\n        Args:\n            x (XYData): The input features for training.\n            y (Optional[XYData]): The target values for training.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: The score of the fitted model on the training data.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The score is calculated using scikit-learn's score method, which computes the mean accuracy.\n        \"\"\"\n        self._clf.fit(x.value, y.value)  # type: ignore\n        return self._clf.score(x.value, y.value)  # type: ignore\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted KNN model.\n\n        This method uses the trained KNN classifier to make predictions on new input data.\n\n        Args:\n            x (XYData): The input features to predict.\n\n        Returns:\n            XYData: The predicted values wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's predict method internally.\n            The predictions are wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        predictions = self._clf.predict(x.value)\n        return XYData.mock(predictions)\n\n    @staticmethod\n    def item_grid(\n        **kwargs: Dict[str, List[Any]],\n    ) -&gt; tuple[type[BaseFilter], Dict[str, List[Any]]]:\n        \"\"\"\n        Generate a parameter grid for hyperparameter tuning.\n\n        This static method provides a way to generate a grid of parameters for use in\n        hyperparameter optimization techniques like grid search.\n\n        Args:\n            **kwargs (Dict[str, List[Any]]): Keyword arguments to override default parameter ranges.\n\n        Returns:\n            tuple[type[BaseFilter], Dict[str, List[Any]]]: A tuple containing the KnnFilter class\n            and a dictionary of parameter names and their possible values.\n\n        Note:\n            The returned dictionary can be used directly with hyperparameter tuning tools\n            that accept parameter grids, such as scikit-learn's GridSearchCV.\n        \"\"\"\n\n        return KnnFilter, kwargs  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.KnnFilter.__init__","title":"<code>__init__(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)</code>","text":"<p>Initialize a new KnnFilter instance.</p> <p>This constructor sets up the KnnFilter with the specified parameters and initializes the underlying scikit-learn KNeighborsClassifier.</p> <p>Parameters:</p> Name Type Description Default <code>n_neighbors</code> <code>int</code> <p>Number of neighbors to use for knn. Defaults to 5.</p> <code>5</code> <code>weights</code> <code>Literal['uniform', 'distance']</code> <p>Weight function used in prediction. Defaults to \"uniform\".</p> <code>'uniform'</code> <code>algorithm</code> <code>Literal['auto', 'ball_tree', 'kd_tree', 'brute']</code> <p>Algorithm used to compute nearest neighbors. Defaults to \"auto\".</p> <code>'auto'</code> <code>leaf_size</code> <code>int</code> <p>Leaf size passed to BallTree or KDTree. Defaults to 30.</p> <code>30</code> <code>p</code> <code>int</code> <p>Power parameter for the Minkowski metric. Defaults to 2 (Euclidean distance).</p> <code>2</code> <code>metric</code> <code>str</code> <p>The distance metric to use for the tree. Defaults to \"minkowski\".</p> <code>'minkowski'</code> <code>metric_params</code> <code>Optional[Dict[str, Any]]</code> <p>Additional keyword arguments for the metric function. Defaults to None.</p> <code>None</code> <code>n_jobs</code> <code>Optional[int]</code> <p>The number of parallel jobs to run for neighbors search. Defaults to None.</p> <code>None</code> Note <p>The parameters are passed directly to scikit-learn's KNeighborsClassifier. Refer to scikit-learn's documentation for detailed information on these parameters.</p> Source code in <code>framework3/plugins/filters/classification/knn.py</code> <pre><code>def __init__(\n    self,\n    n_neighbors: int = 5,\n    weights: Literal[\"uniform\", \"distance\"] = \"uniform\",\n    algorithm: Literal[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"] = \"auto\",\n    leaf_size: int = 30,\n    p: int = 2,\n    metric: str = \"minkowski\",\n    metric_params: Optional[Dict[str, Any]] = None,\n    n_jobs: Optional[int] = None,\n):\n    \"\"\"\n    Initialize a new KnnFilter instance.\n\n    This constructor sets up the KnnFilter with the specified parameters and\n    initializes the underlying scikit-learn KNeighborsClassifier.\n\n    Args:\n        n_neighbors (int): Number of neighbors to use for knn. Defaults to 5.\n        weights (Literal[\"uniform\", \"distance\"]): Weight function used in prediction. Defaults to \"uniform\".\n        algorithm (Literal[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]): Algorithm used to compute nearest neighbors. Defaults to \"auto\".\n        leaf_size (int): Leaf size passed to BallTree or KDTree. Defaults to 30.\n        p (int): Power parameter for the Minkowski metric. Defaults to 2 (Euclidean distance).\n        metric (str): The distance metric to use for the tree. Defaults to \"minkowski\".\n        metric_params (Optional[Dict[str, Any]]): Additional keyword arguments for the metric function. Defaults to None.\n        n_jobs (Optional[int]): The number of parallel jobs to run for neighbors search. Defaults to None.\n\n    Note:\n        The parameters are passed directly to scikit-learn's KNeighborsClassifier.\n        Refer to scikit-learn's documentation for detailed information on these parameters.\n    \"\"\"\n    super().__init__(\n        n_neighbors=n_neighbors,\n        weights=weights,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        p=p,\n        metric=metric,\n        metric_params=metric_params,\n        n_jobs=n_jobs,\n    )\n    self._clf = KNeighborsClassifier(\n        n_neighbors=n_neighbors,\n        weights=weights,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        p=p,\n        metric=metric,\n        metric_params=metric_params,\n        n_jobs=n_jobs,\n    )\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.KnnFilter.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the KNN model to the given data.</p> <p>This method trains the KNN classifier on the provided input features and target values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features for training.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target values for training.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The score of the fitted model on the training data.</p> Note <p>This method uses scikit-learn's fit method internally. The score is calculated using scikit-learn's score method, which computes the mean accuracy.</p> Source code in <code>framework3/plugins/filters/classification/knn.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the KNN model to the given data.\n\n    This method trains the KNN classifier on the provided input features and target values.\n\n    Args:\n        x (XYData): The input features for training.\n        y (Optional[XYData]): The target values for training.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: The score of the fitted model on the training data.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The score is calculated using scikit-learn's score method, which computes the mean accuracy.\n    \"\"\"\n    self._clf.fit(x.value, y.value)  # type: ignore\n    return self._clf.score(x.value, y.value)  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.KnnFilter.item_grid","title":"<code>item_grid(**kwargs)</code>  <code>staticmethod</code>","text":"<p>Generate a parameter grid for hyperparameter tuning.</p> <p>This static method provides a way to generate a grid of parameters for use in hyperparameter optimization techniques like grid search.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Dict[str, List[Any]]</code> <p>Keyword arguments to override default parameter ranges.</p> <code>{}</code> <p>Returns:</p> Type Description <code>type[BaseFilter]</code> <p>tuple[type[BaseFilter], Dict[str, List[Any]]]: A tuple containing the KnnFilter class</p> <code>Dict[str, List[Any]]</code> <p>and a dictionary of parameter names and their possible values.</p> Note <p>The returned dictionary can be used directly with hyperparameter tuning tools that accept parameter grids, such as scikit-learn's GridSearchCV.</p> Source code in <code>framework3/plugins/filters/classification/knn.py</code> <pre><code>@staticmethod\ndef item_grid(\n    **kwargs: Dict[str, List[Any]],\n) -&gt; tuple[type[BaseFilter], Dict[str, List[Any]]]:\n    \"\"\"\n    Generate a parameter grid for hyperparameter tuning.\n\n    This static method provides a way to generate a grid of parameters for use in\n    hyperparameter optimization techniques like grid search.\n\n    Args:\n        **kwargs (Dict[str, List[Any]]): Keyword arguments to override default parameter ranges.\n\n    Returns:\n        tuple[type[BaseFilter], Dict[str, List[Any]]]: A tuple containing the KnnFilter class\n        and a dictionary of parameter names and their possible values.\n\n    Note:\n        The returned dictionary can be used directly with hyperparameter tuning tools\n        that accept parameter grids, such as scikit-learn's GridSearchCV.\n    \"\"\"\n\n    return KnnFilter, kwargs  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/classification/#framework3.plugins.filters.classification.KnnFilter.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted KNN model.</p> <p>This method uses the trained KNN classifier to make predictions on new input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to predict.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predicted values wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's predict method internally. The predictions are wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/classification/knn.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted KNN model.\n\n    This method uses the trained KNN classifier to make predictions on new input data.\n\n    Args:\n        x (XYData): The input features to predict.\n\n    Returns:\n        XYData: The predicted values wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's predict method internally.\n        The predictions are wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    predictions = self._clf.predict(x.value)\n    return XYData.mock(predictions)\n</code></pre>"},{"location":"api/plugins/filters/classification/#overview","title":"Overview","text":"<p>The Classification Filters module in framework3 provides a collection of powerful classification algorithms that can be easily integrated into your machine learning pipelines. These filters are designed to work seamlessly with the framework3 ecosystem, providing a consistent interface and enhanced functionality.</p>"},{"location":"api/plugins/filters/classification/#available-classifiers","title":"Available Classifiers","text":""},{"location":"api/plugins/filters/classification/#svm-classifier","title":"SVM Classifier","text":"<p>The Support Vector Machine (SVM) classifier is implemented in the <code>ClassifierSVMPlugin</code>. This versatile classifier is effective for both linear and non-linear classification tasks.</p>"},{"location":"api/plugins/filters/classification/#usage","title":"Usage","text":"<pre><code>from framework3.plugins.filters.classification.svm import ClassifierSVMPlugin\n\nsvm_classifier = ClassifierSVMPlugin(C=1.0, kernel='rbf', gamma='scale')\n</code></pre>"},{"location":"api/plugins/filters/classification/#parameters","title":"Parameters","text":"<ul> <li><code>C</code> (float): Regularization parameter. The strength of the regularization is inversely proportional to C.</li> <li><code>kernel</code> (str): The kernel type to be used in the algorithm. Options include 'linear', 'poly', 'rbf', and 'sigmoid'.</li> <li><code>gamma</code> (str or float): Kernel coefficient for 'rbf', 'poly' and 'sigmoid' kernels.</li> </ul>"},{"location":"api/plugins/filters/classification/#k-nearest-neighbors-classifier","title":"K-Nearest Neighbors Classifier","text":"<p>The K-Nearest Neighbors (KNN) classifier is implemented in the <code>KnnFilter</code>. This simple yet effective classifier is based on the principle of finding the K nearest neighbors to make predictions.</p>"},{"location":"api/plugins/filters/classification/#usage_1","title":"Usage","text":"<pre><code>from framework3.plugins.filters.classification.knn import KnnFilter\n\nknn_classifier = KnnFilter(n_neighbors=5, weights='uniform')\n</code></pre>"},{"location":"api/plugins/filters/classification/#parameters_1","title":"Parameters","text":"<ul> <li><code>n_neighbors</code> (int): Number of neighbors to use for kneighbors queries.</li> <li><code>weights</code> (str): Weight function used in prediction. Options are 'uniform' (all points in each neighborhood are weighted equally) or 'distance' (weight points by the inverse of their distance).</li> </ul>"},{"location":"api/plugins/filters/classification/#comprehensive-example-iris-dataset-classification","title":"Comprehensive Example: Iris Dataset Classification","text":"<p>In this example, we'll demonstrate how to use the Classification Filters with the Iris dataset, showcasing both SVM and KNN classifiers, as well as integration with GridSearchCV.</p> <pre><code>from framework3.plugins.pipelines.gs_cv_pipeline import GridSearchCVPipeline\nfrom framework3.plugins.filters.classification.svm import ClassifierSVMPlugin\nfrom framework3.plugins.filters.classification.knn import KnnFilter\nfrom framework3.base.base_types import XYData\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create XYData objects\nX_train_data = XYData(_hash='X_train', _path='/tmp', _value=X_train)\ny_train_data = XYData(_hash='y_train', _path='/tmp', _value=y_train)\nX_test_data = XYData(_hash='X_test', _path='/tmp', _value=X_test)\ny_test_data = XYData(_hash='y_test', _path='/tmp', _value=y_test)\n\n# Create a pipeline with SVM classifier\nsvm_pipeline = GridSearchCVPipeline(\n    filterx=[ClassifierSVMPlugin],\n    param_grid=ClassifierSVMPlugin.item_grid(C=[0.1, 1, 10], kernel=['linear', 'rbf']),\n    scoring='accuracy',\n    cv=5\n)\n\n# Fit the SVM pipeline\nsvm_pipeline.fit(X_train_data, y_train_data)\n\n# Make predictions with SVM\nsvm_predictions = svm_pipeline.predict(X_test_data)\nprint(\"SVM Predictions:\", svm_predictions.value)\n\n# Create a pipeline with KNN classifier\nknn_pipeline = GridSearchCVPipeline(\n    filterx=[KnnFilter],\n    param_grid=KnnFilter.item_grid(n_neighbors=[3, 5, 7], weights=['uniform', 'distance']),\n    scoring='accuracy',\n    cv=5\n)\n\n# Fit the KNN pipeline\nknn_pipeline.fit(X_train_data, y_train_data)\n\n# Make predictions with KNN\nknn_predictions = knn_pipeline.predict(X_test_data)\nprint(\"KNN Predictions:\", knn_predictions.value)\n\n# Evaluate the models\nfrom sklearn.metrics import accuracy_score\n\nsvm_accuracy = accuracy_score(y_test, svm_predictions.value)\nknn_accuracy = accuracy_score(y_test, knn_predictions.value)\n\nprint(\"SVM Accuracy:\", svm_accuracy)\nprint(\"KNN Accuracy:\", knn_accuracy)\n</code></pre> <p>This example demonstrates how to:</p> <ol> <li>Load and prepare the Iris dataset</li> <li>Create XYData objects for use with framework3</li> <li>Set up GridSearchCV pipelines for both SVM and KNN classifiers</li> <li>Fit the models and make predictions</li> <li>Evaluate the models using accuracy scores</li> </ol>"},{"location":"api/plugins/filters/classification/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Data Preprocessing: Ensure your data is properly preprocessed before applying classification filters. This may include scaling, normalization, or handling missing values.</p> </li> <li> <p>Hyperparameter Tuning: Use <code>GridSearchCVPipeline</code> to find the optimal hyperparameters for your chosen classifier, as demonstrated in the example.</p> </li> <li> <p>Model Evaluation: Always evaluate your model's performance using appropriate metrics and cross-validation techniques. In the example, we used accuracy, but consider other metrics like precision, recall, or F1-score depending on your specific problem.</p> </li> <li> <p>Feature Selection: Consider applying feature selection techniques to improve model performance and reduce overfitting, especially when dealing with high-dimensional datasets.</p> </li> <li> <p>Ensemble Methods: Experiment with combining multiple classifiers to create ensemble models, which can often lead to improved performance.</p> </li> </ol>"},{"location":"api/plugins/filters/classification/#conclusion","title":"Conclusion","text":"<p>The Classification Filters module in framework3 provides a robust set of tools for tackling various classification tasks. By leveraging these filters in combination with other framework3 components, you can build powerful and efficient machine learning pipelines. The example with the Iris dataset demonstrates how easy it is to use these classifiers and integrate them with GridSearchCV for hyperparameter tuning.</p>"},{"location":"api/plugins/filters/clustering/","title":"Clustering Filters","text":""},{"location":"api/plugins/filters/clustering/#framework3.plugins.filters.clustering.KMeansFilter","title":"<code>KMeansFilter</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A wrapper for scikit-learn's KMeans clustering algorithm using the framework3 BaseFilter interface.</p> <p>This filter implements the K-Means clustering algorithm within the framework3 ecosystem.</p> Key Features <ul> <li>Integrates scikit-learn's KMeans with framework3</li> <li>Supports various KMeans parameters like number of clusters, initialization method, and algorithm</li> <li>Provides methods for fitting the model, making predictions, and transforming data</li> <li>Includes a static method for generating parameter grids for hyperparameter tuning</li> </ul> Usage <p>The KMeansFilter can be used to perform K-Means clustering on your data:</p> <pre><code>from framework3.plugins.filters.clustering.kmeans import KMeansFilter\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nX = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n\n# Create and fit the KMeans filter\nkmeans = KMeansFilter(n_clusters=2, random_state=42)\nkmeans.fit(X_data)\n\n# Make predictions\nX_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[0, 0], [4, 4]]))\npredictions = kmeans.predict(X_test)\nprint(predictions.value)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_clf</code> <code>KMeans</code> <p>The underlying scikit-learn KMeans clustering model.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the KMeans model to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Predict the closest cluster for each sample in X.</p> <code>transform</code> <p>XYData) -&gt; XYData: Transform X to a cluster-distance space.</p> <code>item_grid</code> <p>Generate a parameter grid for hyperparameter tuning.</p> Note <p>This filter uses scikit-learn's implementation of KMeans, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/clustering/kmeans.py</code> <pre><code>@Container.bind()\nclass KMeansFilter(BaseFilter):\n    \"\"\"\n    A wrapper for scikit-learn's KMeans clustering algorithm using the framework3 BaseFilter interface.\n\n    This filter implements the K-Means clustering algorithm within the framework3 ecosystem.\n\n    Key Features:\n        - Integrates scikit-learn's KMeans with framework3\n        - Supports various KMeans parameters like number of clusters, initialization method, and algorithm\n        - Provides methods for fitting the model, making predictions, and transforming data\n        - Includes a static method for generating parameter grids for hyperparameter tuning\n\n    Usage:\n        The KMeansFilter can be used to perform K-Means clustering on your data:\n\n        ```python\n        from framework3.plugins.filters.clustering.kmeans import KMeansFilter\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n        X_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n\n        # Create and fit the KMeans filter\n        kmeans = KMeansFilter(n_clusters=2, random_state=42)\n        kmeans.fit(X_data)\n\n        # Make predictions\n        X_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[0, 0], [4, 4]]))\n        predictions = kmeans.predict(X_test)\n        print(predictions.value)\n        ```\n\n    Attributes:\n        _clf (KMeans): The underlying scikit-learn KMeans clustering model.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the KMeans model to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Predict the closest cluster for each sample in X.\n        transform(x: XYData) -&gt; XYData:\n            Transform X to a cluster-distance space.\n        item_grid(**kwargs) -&gt; Dict[str, Any]:\n            Generate a parameter grid for hyperparameter tuning.\n\n    Note:\n        This filter uses scikit-learn's implementation of KMeans, which may have its own dependencies and requirements.\n        Ensure that scikit-learn is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_clusters: int = 8,\n        init: Literal[\"k-means++\", \"random\"] = \"k-means++\",\n        n_init: int = 10,\n        max_iter: int = 300,\n        tol: float = 1e-4,\n        random_state: Optional[int] = None,\n        algorithm: Literal[\"lloyd\", \"elkan\"] = \"lloyd\",\n    ):\n        \"\"\"\n        Initialize a new KMeansFilter instance.\n\n        This constructor sets up the KMeansFilter with the specified parameters and\n        initializes the underlying scikit-learn KMeans model.\n\n        Args:\n            n_clusters (int): The number of clusters to form. Defaults to 8.\n            init (Literal[\"k-means++\", \"random\"]): Method for initialization. Defaults to 'k-means++'.\n            n_init (int): Number of times the k-means algorithm will be run with different centroid seeds. Defaults to 10.\n            max_iter (int): Maximum number of iterations of the k-means algorithm for a single run. Defaults to 300.\n            tol (float): Relative tolerance with regards to Frobenius norm of the difference\n                         in the cluster centers of two consecutive iterations to declare convergence. Defaults to 1e-4.\n            random_state (Optional[int]): Determines random number generation for centroid initialization. Defaults to None.\n            algorithm (Literal[\"lloyd\", \"elkan\"]): K-means algorithm to use. Defaults to 'lloyd'.\n\n        Note:\n            The parameters are passed directly to scikit-learn's KMeans.\n            Refer to scikit-learn's documentation for detailed information on these parameters.\n        \"\"\"\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            n_init=n_init,\n            max_iter=max_iter,\n            tol=tol,\n            random_state=random_state,\n            algorithm=algorithm,\n        )\n        self._clf = KMeans(\n            n_clusters=n_clusters,\n            init=init,\n            n_init=n_init,\n            max_iter=max_iter,\n            tol=tol,\n            random_state=random_state,\n            algorithm=algorithm,\n        )\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the KMeans model to the given data.\n\n        This method trains the KMeans model on the provided input features.\n\n        Args:\n            x (XYData): The input features for training.\n            y (Optional[XYData]): Not used, present for API consistency.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: The inertia (within-cluster sum-of-squares) of the fitted model.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The inertia is returned as a measure of how well the model fits the data.\n        \"\"\"\n        self._clf.fit(x.value)\n        return self._clf.inertia_  # type: ignore\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Predict the closest cluster for each sample in X.\n\n        This method uses the trained KMeans model to predict cluster labels for new input data.\n\n        Args:\n            x (XYData): The input features to predict.\n\n        Returns:\n            XYData: The predicted cluster labels wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's predict method internally.\n            The predictions are wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        predictions = self._clf.predict(x.value)\n        return XYData.mock(predictions)\n\n    def transform(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Transform X to a cluster-distance space.\n\n        This method computes the distance between each sample in X and the cluster centers.\n\n        Args:\n            x (XYData): The input features to transform.\n\n        Returns:\n            XYData: The transformed data wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's transform method internally.\n            The transformed data is wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        transformed = self._clf.transform(x.value)\n        return XYData.mock(transformed)\n\n    @staticmethod\n    def item_grid(**kwargs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate a parameter grid for hyperparameter tuning.\n\n        This static method provides a way to generate a grid of parameters for use in\n        hyperparameter optimization techniques like grid search.\n\n        Args:\n            **kwargs (Dict[str, Any]): Keyword arguments representing the parameter names and their possible values.\n\n        Returns:\n            Dict[str, Any]: A dictionary of parameter names and their possible values.\n\n        Note:\n            The returned dictionary can be used directly with hyperparameter tuning tools\n            that accept parameter grids, such as scikit-learn's GridSearchCV.\n            The parameter names are prefixed with \"KMeansFilter__\" for compatibility with nested estimators.\n        \"\"\"\n\n        return dict(map(lambda x: (f\"KMeansFilter__{x[0]}\", x[1]), kwargs.items()))\n</code></pre>"},{"location":"api/plugins/filters/clustering/#framework3.plugins.filters.clustering.KMeansFilter.__init__","title":"<code>__init__(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, random_state=None, algorithm='lloyd')</code>","text":"<p>Initialize a new KMeansFilter instance.</p> <p>This constructor sets up the KMeansFilter with the specified parameters and initializes the underlying scikit-learn KMeans model.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>The number of clusters to form. Defaults to 8.</p> <code>8</code> <code>init</code> <code>Literal['k-means++', 'random']</code> <p>Method for initialization. Defaults to 'k-means++'.</p> <code>'k-means++'</code> <code>n_init</code> <code>int</code> <p>Number of times the k-means algorithm will be run with different centroid seeds. Defaults to 10.</p> <code>10</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations of the k-means algorithm for a single run. Defaults to 300.</p> <code>300</code> <code>tol</code> <code>float</code> <p>Relative tolerance with regards to Frobenius norm of the difference          in the cluster centers of two consecutive iterations to declare convergence. Defaults to 1e-4.</p> <code>0.0001</code> <code>random_state</code> <code>Optional[int]</code> <p>Determines random number generation for centroid initialization. Defaults to None.</p> <code>None</code> <code>algorithm</code> <code>Literal['lloyd', 'elkan']</code> <p>K-means algorithm to use. Defaults to 'lloyd'.</p> <code>'lloyd'</code> Note <p>The parameters are passed directly to scikit-learn's KMeans. Refer to scikit-learn's documentation for detailed information on these parameters.</p> Source code in <code>framework3/plugins/filters/clustering/kmeans.py</code> <pre><code>def __init__(\n    self,\n    n_clusters: int = 8,\n    init: Literal[\"k-means++\", \"random\"] = \"k-means++\",\n    n_init: int = 10,\n    max_iter: int = 300,\n    tol: float = 1e-4,\n    random_state: Optional[int] = None,\n    algorithm: Literal[\"lloyd\", \"elkan\"] = \"lloyd\",\n):\n    \"\"\"\n    Initialize a new KMeansFilter instance.\n\n    This constructor sets up the KMeansFilter with the specified parameters and\n    initializes the underlying scikit-learn KMeans model.\n\n    Args:\n        n_clusters (int): The number of clusters to form. Defaults to 8.\n        init (Literal[\"k-means++\", \"random\"]): Method for initialization. Defaults to 'k-means++'.\n        n_init (int): Number of times the k-means algorithm will be run with different centroid seeds. Defaults to 10.\n        max_iter (int): Maximum number of iterations of the k-means algorithm for a single run. Defaults to 300.\n        tol (float): Relative tolerance with regards to Frobenius norm of the difference\n                     in the cluster centers of two consecutive iterations to declare convergence. Defaults to 1e-4.\n        random_state (Optional[int]): Determines random number generation for centroid initialization. Defaults to None.\n        algorithm (Literal[\"lloyd\", \"elkan\"]): K-means algorithm to use. Defaults to 'lloyd'.\n\n    Note:\n        The parameters are passed directly to scikit-learn's KMeans.\n        Refer to scikit-learn's documentation for detailed information on these parameters.\n    \"\"\"\n    super().__init__(\n        n_clusters=n_clusters,\n        init=init,\n        n_init=n_init,\n        max_iter=max_iter,\n        tol=tol,\n        random_state=random_state,\n        algorithm=algorithm,\n    )\n    self._clf = KMeans(\n        n_clusters=n_clusters,\n        init=init,\n        n_init=n_init,\n        max_iter=max_iter,\n        tol=tol,\n        random_state=random_state,\n        algorithm=algorithm,\n    )\n</code></pre>"},{"location":"api/plugins/filters/clustering/#framework3.plugins.filters.clustering.KMeansFilter.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the KMeans model to the given data.</p> <p>This method trains the KMeans model on the provided input features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features for training.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Not used, present for API consistency.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The inertia (within-cluster sum-of-squares) of the fitted model.</p> Note <p>This method uses scikit-learn's fit method internally. The inertia is returned as a measure of how well the model fits the data.</p> Source code in <code>framework3/plugins/filters/clustering/kmeans.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the KMeans model to the given data.\n\n    This method trains the KMeans model on the provided input features.\n\n    Args:\n        x (XYData): The input features for training.\n        y (Optional[XYData]): Not used, present for API consistency.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: The inertia (within-cluster sum-of-squares) of the fitted model.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The inertia is returned as a measure of how well the model fits the data.\n    \"\"\"\n    self._clf.fit(x.value)\n    return self._clf.inertia_  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/clustering/#framework3.plugins.filters.clustering.KMeansFilter.item_grid","title":"<code>item_grid(**kwargs)</code>  <code>staticmethod</code>","text":"<p>Generate a parameter grid for hyperparameter tuning.</p> <p>This static method provides a way to generate a grid of parameters for use in hyperparameter optimization techniques like grid search.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments representing the parameter names and their possible values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of parameter names and their possible values.</p> Note <p>The returned dictionary can be used directly with hyperparameter tuning tools that accept parameter grids, such as scikit-learn's GridSearchCV. The parameter names are prefixed with \"KMeansFilter__\" for compatibility with nested estimators.</p> Source code in <code>framework3/plugins/filters/clustering/kmeans.py</code> <pre><code>@staticmethod\ndef item_grid(**kwargs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a parameter grid for hyperparameter tuning.\n\n    This static method provides a way to generate a grid of parameters for use in\n    hyperparameter optimization techniques like grid search.\n\n    Args:\n        **kwargs (Dict[str, Any]): Keyword arguments representing the parameter names and their possible values.\n\n    Returns:\n        Dict[str, Any]: A dictionary of parameter names and their possible values.\n\n    Note:\n        The returned dictionary can be used directly with hyperparameter tuning tools\n        that accept parameter grids, such as scikit-learn's GridSearchCV.\n        The parameter names are prefixed with \"KMeansFilter__\" for compatibility with nested estimators.\n    \"\"\"\n\n    return dict(map(lambda x: (f\"KMeansFilter__{x[0]}\", x[1]), kwargs.items()))\n</code></pre>"},{"location":"api/plugins/filters/clustering/#framework3.plugins.filters.clustering.KMeansFilter.predict","title":"<code>predict(x)</code>","text":"<p>Predict the closest cluster for each sample in X.</p> <p>This method uses the trained KMeans model to predict cluster labels for new input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to predict.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predicted cluster labels wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's predict method internally. The predictions are wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/clustering/kmeans.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Predict the closest cluster for each sample in X.\n\n    This method uses the trained KMeans model to predict cluster labels for new input data.\n\n    Args:\n        x (XYData): The input features to predict.\n\n    Returns:\n        XYData: The predicted cluster labels wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's predict method internally.\n        The predictions are wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    predictions = self._clf.predict(x.value)\n    return XYData.mock(predictions)\n</code></pre>"},{"location":"api/plugins/filters/clustering/#framework3.plugins.filters.clustering.KMeansFilter.transform","title":"<code>transform(x)</code>","text":"<p>Transform X to a cluster-distance space.</p> <p>This method computes the distance between each sample in X and the cluster centers.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to transform.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The transformed data wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's transform method internally. The transformed data is wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/clustering/kmeans.py</code> <pre><code>def transform(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Transform X to a cluster-distance space.\n\n    This method computes the distance between each sample in X and the cluster centers.\n\n    Args:\n        x (XYData): The input features to transform.\n\n    Returns:\n        XYData: The transformed data wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's transform method internally.\n        The transformed data is wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    transformed = self._clf.transform(x.value)\n    return XYData.mock(transformed)\n</code></pre>"},{"location":"api/plugins/filters/clustering/#overview","title":"Overview","text":"<p>The Clustering Filters module in framework3 provides a collection of unsupervised learning algorithms for clustering data. These filters are designed to work seamlessly within the framework3 ecosystem, offering a consistent interface and enhanced functionality for various clustering tasks.</p>"},{"location":"api/plugins/filters/clustering/#available-clustering-algorithms","title":"Available Clustering Algorithms","text":""},{"location":"api/plugins/filters/clustering/#k-means-clustering","title":"K-Means Clustering","text":"<p>The K-Means clustering algorithm is implemented in the <code>KMeansFilter</code>. This popular clustering method aims to partition n observations into k clusters, where each observation belongs to the cluster with the nearest mean (cluster centroid).</p>"},{"location":"api/plugins/filters/clustering/#usage","title":"Usage","text":"<pre><code>from framework3.plugins.filters.clustering.kmeans import KMeansFilter\n\nkmeans_clusterer = KMeansFilter(n_clusters=3, init='k-means++', n_init=10, max_iter=300)\n</code></pre>"},{"location":"api/plugins/filters/clustering/#parameters","title":"Parameters","text":"<ul> <li><code>n_clusters</code> (int): The number of clusters to form and the number of centroids to generate.</li> <li><code>init</code> (str): Method for initialization of centroids. Options include 'k-means++' and 'random'.</li> <li><code>n_init</code> (int): Number of times the k-means algorithm will be run with different centroid seeds.</li> <li><code>max_iter</code> (int): Maximum number of iterations for a single run.</li> </ul>"},{"location":"api/plugins/filters/clustering/#dbscan-clustering","title":"DBSCAN Clustering","text":"<p>The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is implemented in the <code>DBSCANFilter</code>. This algorithm is particularly effective for datasets with clusters of arbitrary shape.</p>"},{"location":"api/plugins/filters/clustering/#usage_1","title":"Usage","text":"<pre><code>from framework3.plugins.filters.clustering.dbscan import DBSCANFilter\n\ndbscan_clusterer = DBSCANFilter(eps=0.5, min_samples=5)\n</code></pre>"},{"location":"api/plugins/filters/clustering/#parameters_1","title":"Parameters","text":"<ul> <li><code>eps</code> (float): The maximum distance between two samples for one to be considered as in the neighborhood of the other.</li> <li><code>min_samples</code> (int): The number of samples in a neighborhood for a point to be considered as a core point.</li> </ul>"},{"location":"api/plugins/filters/clustering/#comprehensive-example-clustering-with-synthetic-data","title":"Comprehensive Example: Clustering with Synthetic Data","text":"<p>In this example, we'll demonstrate how to use the Clustering Filters with synthetic data, showcasing both K-Means and DBSCAN algorithms, as well as integration with GridSearchCV for parameter tuning.</p> <pre><code>from framework3.plugins.pipelines.gs_cv_pipeline import GridSearchCVPipeline\nfrom framework3.plugins.filters.clustering.kmeans import KMeansFilter\nfrom framework3.plugins.filters.clustering.dbscan import DBSCANFilter\nfrom framework3.base.base_types import XYData\nfrom sklearn.datasets import make_blobs, make_moons\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\n\n# Generate synthetic datasets\nX_blobs, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\nX_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=0)\n\n# Create XYData objects\nX_blobs_data = XYData(_hash='X_blobs', _path='/tmp', _value=X_blobs)\nX_moons_data = XYData(_hash='X_moons', _path='/tmp', _value=X_moons)\n\n# K-Means Clustering\nkmeans_pipeline = GridSearchCVPipeline(\n    filterx=[KMeansFilter],\n    param_grid=KMeansFilter.item_grid(n_clusters=[2, 3, 4, 5], init=['k-means++', 'random']),\n    scoring='silhouette',\n    cv=5\n)\n\n# Fit K-Means on blobs dataset\nkmeans_pipeline.fit(X_blobs_data)\n\n# Make predictions\nkmeans_labels = kmeans_pipeline.predict(X_blobs_data)\nprint(\"K-Means Cluster Labels:\", kmeans_labels.value)\n\n# DBSCAN Clustering\ndbscan_pipeline = GridSearchCVPipeline(\n    filterx=[DBSCANFilter],\n    param_grid=DBSCANFilter.item_grid(eps=[0.1, 0.2, 0.3], min_samples=[3, 5, 7]),\n    scoring='silhouette',\n    cv=5\n)\n\n# Fit DBSCAN on moons dataset\ndbscan_pipeline.fit(X_moons_data)\n\n# Make predictions\ndbscan_labels = dbscan_pipeline.predict(X_moons_data)\nprint(\"DBSCAN Cluster Labels:\", dbscan_labels.value)\n\n# Evaluate the models\nkmeans_silhouette = silhouette_score(X_blobs, kmeans_labels.value)\ndbscan_silhouette = silhouette_score(X_moons, dbscan_labels.value)\n\nprint(\"K-Means Silhouette Score:\", kmeans_silhouette)\nprint(\"DBSCAN Silhouette Score:\", dbscan_silhouette)\n</code></pre> <p>This example demonstrates how to:</p> <ol> <li>Generate synthetic datasets suitable for different clustering algorithms</li> <li>Create XYData objects for use with framework3</li> <li>Set up GridSearchCV pipelines for both K-Means and DBSCAN clustering</li> <li>Fit the models and make predictions</li> <li>Evaluate the models using silhouette scores</li> </ol>"},{"location":"api/plugins/filters/clustering/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Data Preprocessing: Ensure your data is properly preprocessed before applying clustering filters. This may include scaling, normalization, or handling missing values.</p> </li> <li> <p>Algorithm Selection: Choose the appropriate clustering algorithm based on the characteristics of your data and the specific requirements of your problem.</p> </li> <li> <p>Parameter Tuning: Use <code>GridSearchCVPipeline</code> to find the optimal parameters for your chosen clustering algorithm, as demonstrated in the example.</p> </li> <li> <p>Cluster Evaluation: Always evaluate your clustering results using appropriate metrics such as silhouette score, Calinski-Harabasz index, or Davies-Bouldin index.</p> </li> <li> <p>Visualization: Visualize your clustering results to gain insights into the structure of your data and the performance of the clustering algorithm.</p> </li> <li> <p>Ensemble Methods: Consider using ensemble clustering techniques to improve the robustness and stability of your clustering results.</p> </li> </ol>"},{"location":"api/plugins/filters/clustering/#conclusion","title":"Conclusion","text":"<p>The Clustering Filters module in framework3 provides a powerful set of tools for unsupervised learning tasks. By leveraging these filters in combination with other framework3 components, you can build efficient and effective clustering pipelines. The example with synthetic data demonstrates how easy it is to use these clustering algorithms and integrate them with GridSearchCV for parameter tuning.</p>"},{"location":"api/plugins/filters/grid_search/","title":"GridSearchCVFilter","text":""},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search","title":"<code>framework3.plugins.filters.grid_search</code>","text":""},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.GridSearchCVPlugin","title":"<code>GridSearchCVPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A plugin for performing hyperparameter tuning on BaseFilter objects using scikit-learn's GridSearchCV.</p> <p>This plugin automates the process of finding optimal hyperparameters for a given BaseFilter by evaluating different combinations of parameters through cross-validation.</p> Key Features <ul> <li>Integrates scikit-learn's GridSearchCV with framework3's BaseFilter</li> <li>Supports hyperparameter tuning for any BaseFilter</li> <li>Allows specification of parameter grid, scoring metric, and cross-validation strategy</li> <li>Provides methods for fitting the model and making predictions with the best found parameters</li> </ul> Usage <p>The GridSearchCVPlugin can be used to perform hyperparameter tuning on a BaseFilter:</p> <pre><code>from framework3.plugins.filters.clasification.svm import ClassifierSVMPlugin\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\ny_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n# Define the parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf'],\n    'gamma': ['scale', 'auto']\n}\n\n# Create the GridSearchCVPlugin\ngrid_search = GridSearchCVPlugin(\n    filterx=ClassifierSVMPlugin,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3\n)\n\n# Fit the grid search\ngrid_search.fit(X_data, y_data)\n\n# Make predictions\nX_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\npredictions = grid_search.predict(X_test)\nprint(predictions.value)\n\n# Access the best parameters\nprint(grid_search._clf.best_params_)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_clf</code> <code>GridSearchCV</code> <p>The GridSearchCV object used for hyperparameter tuning.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: XYData): Fit the GridSearchCV object to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the best estimator found by GridSearchCV.</p> Note <p>This plugin uses scikit-learn's GridSearchCV, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>@Container.bind()\nclass GridSearchCVPlugin(BaseFilter):\n    \"\"\"\n    A plugin for performing hyperparameter tuning on BaseFilter objects using scikit-learn's GridSearchCV.\n\n    This plugin automates the process of finding optimal hyperparameters for a given BaseFilter\n    by evaluating different combinations of parameters through cross-validation.\n\n    Key Features:\n        - Integrates scikit-learn's GridSearchCV with framework3's BaseFilter\n        - Supports hyperparameter tuning for any BaseFilter\n        - Allows specification of parameter grid, scoring metric, and cross-validation strategy\n        - Provides methods for fitting the model and making predictions with the best found parameters\n\n    Usage:\n        The GridSearchCVPlugin can be used to perform hyperparameter tuning on a BaseFilter:\n\n        ```python\n        from framework3.plugins.filters.clasification.svm import ClassifierSVMPlugin\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        X_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n        y_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n        # Define the parameter grid\n        param_grid = {\n            'C': [0.1, 1, 10],\n            'kernel': ['linear', 'rbf'],\n            'gamma': ['scale', 'auto']\n        }\n\n        # Create the GridSearchCVPlugin\n        grid_search = GridSearchCVPlugin(\n            filterx=ClassifierSVMPlugin,\n            param_grid=param_grid,\n            scoring='accuracy',\n            cv=3\n        )\n\n        # Fit the grid search\n        grid_search.fit(X_data, y_data)\n\n        # Make predictions\n        X_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\n        predictions = grid_search.predict(X_test)\n        print(predictions.value)\n\n        # Access the best parameters\n        print(grid_search._clf.best_params_)\n        ```\n\n    Attributes:\n        _clf (GridSearchCV): The GridSearchCV object used for hyperparameter tuning.\n\n    Methods:\n        fit(x: XYData, y: XYData): Fit the GridSearchCV object to the given data.\n        predict(x: XYData) -&gt; XYData: Make predictions using the best estimator found by GridSearchCV.\n\n    Note:\n        This plugin uses scikit-learn's GridSearchCV, which may have its own dependencies and requirements.\n        Ensure that scikit-learn is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        filterx: Type[BaseFilter],\n        param_grid: Dict[str, Any],\n        scoring: str,\n        cv: int = 2,\n    ):\n        \"\"\"\n        Initialize a new GridSearchCVPlugin instance.\n\n        This constructor sets up the GridSearchCVPlugin with the specified BaseFilter,\n        parameter grid, scoring metric, and cross-validation strategy.\n\n        Args:\n            filterx (Type[BaseFilter]): The BaseFilter class to be tuned.\n            param_grid (Dict[str, Any]): Dictionary with parameters names as keys and lists of parameter settings to try as values.\n            scoring (str): Strategy to evaluate the performance of the cross-validated model on the test set.\n            cv (int): Determines the cross-validation splitting strategy. Defaults to 2.\n\n        Note:\n            The GridSearchCV object is initialized with a Pipeline containing the specified BaseFilter\n            wrapped in an SkWrapper to ensure compatibility with scikit-learn's API.\n        \"\"\"\n        super().__init__(filterx=filterx, param_grid=param_grid, scoring=scoring, cv=cv)\n\n        self._clf: GridSearchCV = GridSearchCV(\n            estimator=Pipeline(steps=[(filterx.__name__, SkWrapper(filterx))]),\n            param_grid=param_grid,\n            scoring=scoring,\n            cv=cv,\n            refit=True,\n            verbose=0,\n        )\n\n    def fit(self, x, y):\n        \"\"\"\n        Fit the GridSearchCV object to the given data.\n\n        This method performs the grid search over the specified parameter grid,\n        fitting the model with different parameter combinations and selecting the best one.\n\n        Args:\n            x (XYData): The input features.\n            y (XYData): The target values.\n\n        Note:\n            This method modifies the internal state of the GridSearchCV object,\n            storing the best parameters and the corresponding fitted model.\n        \"\"\"\n        self._clf.fit(x.value, y.value)  # type: ignore\n\n    def predict(self, x) -&gt; XYData:\n        \"\"\"\n        Make predictions using the best estimator found by GridSearchCV.\n\n        This method uses the best model found during the grid search to make predictions\n        on new data.\n\n        Args:\n            x (XYData): The input features.\n\n        Returns:\n            (XYData): The predicted values wrapped in an XYData object.\n\n        Note:\n            The predictions are wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._clf.predict(x.value))  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.GridSearchCVPlugin.__init__","title":"<code>__init__(filterx, param_grid, scoring, cv=2)</code>","text":"<p>Initialize a new GridSearchCVPlugin instance.</p> <p>This constructor sets up the GridSearchCVPlugin with the specified BaseFilter, parameter grid, scoring metric, and cross-validation strategy.</p> <p>Parameters:</p> Name Type Description Default <code>filterx</code> <code>Type[BaseFilter]</code> <p>The BaseFilter class to be tuned.</p> required <code>param_grid</code> <code>Dict[str, Any]</code> <p>Dictionary with parameters names as keys and lists of parameter settings to try as values.</p> required <code>scoring</code> <code>str</code> <p>Strategy to evaluate the performance of the cross-validated model on the test set.</p> required <code>cv</code> <code>int</code> <p>Determines the cross-validation splitting strategy. Defaults to 2.</p> <code>2</code> Note <p>The GridSearchCV object is initialized with a Pipeline containing the specified BaseFilter wrapped in an SkWrapper to ensure compatibility with scikit-learn's API.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def __init__(\n    self,\n    filterx: Type[BaseFilter],\n    param_grid: Dict[str, Any],\n    scoring: str,\n    cv: int = 2,\n):\n    \"\"\"\n    Initialize a new GridSearchCVPlugin instance.\n\n    This constructor sets up the GridSearchCVPlugin with the specified BaseFilter,\n    parameter grid, scoring metric, and cross-validation strategy.\n\n    Args:\n        filterx (Type[BaseFilter]): The BaseFilter class to be tuned.\n        param_grid (Dict[str, Any]): Dictionary with parameters names as keys and lists of parameter settings to try as values.\n        scoring (str): Strategy to evaluate the performance of the cross-validated model on the test set.\n        cv (int): Determines the cross-validation splitting strategy. Defaults to 2.\n\n    Note:\n        The GridSearchCV object is initialized with a Pipeline containing the specified BaseFilter\n        wrapped in an SkWrapper to ensure compatibility with scikit-learn's API.\n    \"\"\"\n    super().__init__(filterx=filterx, param_grid=param_grid, scoring=scoring, cv=cv)\n\n    self._clf: GridSearchCV = GridSearchCV(\n        estimator=Pipeline(steps=[(filterx.__name__, SkWrapper(filterx))]),\n        param_grid=param_grid,\n        scoring=scoring,\n        cv=cv,\n        refit=True,\n        verbose=0,\n    )\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.GridSearchCVPlugin.fit","title":"<code>fit(x, y)</code>","text":"<p>Fit the GridSearchCV object to the given data.</p> <p>This method performs the grid search over the specified parameter grid, fitting the model with different parameter combinations and selecting the best one.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>XYData</code> <p>The target values.</p> required Note <p>This method modifies the internal state of the GridSearchCV object, storing the best parameters and the corresponding fitted model.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def fit(self, x, y):\n    \"\"\"\n    Fit the GridSearchCV object to the given data.\n\n    This method performs the grid search over the specified parameter grid,\n    fitting the model with different parameter combinations and selecting the best one.\n\n    Args:\n        x (XYData): The input features.\n        y (XYData): The target values.\n\n    Note:\n        This method modifies the internal state of the GridSearchCV object,\n        storing the best parameters and the corresponding fitted model.\n    \"\"\"\n    self._clf.fit(x.value, y.value)  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.GridSearchCVPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the best estimator found by GridSearchCV.</p> <p>This method uses the best model found during the grid search to make predictions on new data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <p>Returns:</p> Type Description <code>XYData</code> <p>The predicted values wrapped in an XYData object.</p> Note <p>The predictions are wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def predict(self, x) -&gt; XYData:\n    \"\"\"\n    Make predictions using the best estimator found by GridSearchCV.\n\n    This method uses the best model found during the grid search to make predictions\n    on new data.\n\n    Args:\n        x (XYData): The input features.\n\n    Returns:\n        (XYData): The predicted values wrapped in an XYData object.\n\n    Note:\n        The predictions are wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._clf.predict(x.value))  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search","title":"<code>cv_grid_search</code>","text":""},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.__all__","title":"<code>__all__ = ['GridSearchCVPlugin']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.GridSearchCVPlugin","title":"<code>GridSearchCVPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A plugin for performing hyperparameter tuning on BaseFilter objects using scikit-learn's GridSearchCV.</p> <p>This plugin automates the process of finding optimal hyperparameters for a given BaseFilter by evaluating different combinations of parameters through cross-validation.</p> Key Features <ul> <li>Integrates scikit-learn's GridSearchCV with framework3's BaseFilter</li> <li>Supports hyperparameter tuning for any BaseFilter</li> <li>Allows specification of parameter grid, scoring metric, and cross-validation strategy</li> <li>Provides methods for fitting the model and making predictions with the best found parameters</li> </ul> Usage <p>The GridSearchCVPlugin can be used to perform hyperparameter tuning on a BaseFilter:</p> <pre><code>from framework3.plugins.filters.clasification.svm import ClassifierSVMPlugin\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\ny_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n# Define the parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf'],\n    'gamma': ['scale', 'auto']\n}\n\n# Create the GridSearchCVPlugin\ngrid_search = GridSearchCVPlugin(\n    filterx=ClassifierSVMPlugin,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3\n)\n\n# Fit the grid search\ngrid_search.fit(X_data, y_data)\n\n# Make predictions\nX_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\npredictions = grid_search.predict(X_test)\nprint(predictions.value)\n\n# Access the best parameters\nprint(grid_search._clf.best_params_)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_clf</code> <code>GridSearchCV</code> <p>The GridSearchCV object used for hyperparameter tuning.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: XYData): Fit the GridSearchCV object to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the best estimator found by GridSearchCV.</p> Note <p>This plugin uses scikit-learn's GridSearchCV, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>@Container.bind()\nclass GridSearchCVPlugin(BaseFilter):\n    \"\"\"\n    A plugin for performing hyperparameter tuning on BaseFilter objects using scikit-learn's GridSearchCV.\n\n    This plugin automates the process of finding optimal hyperparameters for a given BaseFilter\n    by evaluating different combinations of parameters through cross-validation.\n\n    Key Features:\n        - Integrates scikit-learn's GridSearchCV with framework3's BaseFilter\n        - Supports hyperparameter tuning for any BaseFilter\n        - Allows specification of parameter grid, scoring metric, and cross-validation strategy\n        - Provides methods for fitting the model and making predictions with the best found parameters\n\n    Usage:\n        The GridSearchCVPlugin can be used to perform hyperparameter tuning on a BaseFilter:\n\n        ```python\n        from framework3.plugins.filters.clasification.svm import ClassifierSVMPlugin\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        X_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n        y_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n        # Define the parameter grid\n        param_grid = {\n            'C': [0.1, 1, 10],\n            'kernel': ['linear', 'rbf'],\n            'gamma': ['scale', 'auto']\n        }\n\n        # Create the GridSearchCVPlugin\n        grid_search = GridSearchCVPlugin(\n            filterx=ClassifierSVMPlugin,\n            param_grid=param_grid,\n            scoring='accuracy',\n            cv=3\n        )\n\n        # Fit the grid search\n        grid_search.fit(X_data, y_data)\n\n        # Make predictions\n        X_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\n        predictions = grid_search.predict(X_test)\n        print(predictions.value)\n\n        # Access the best parameters\n        print(grid_search._clf.best_params_)\n        ```\n\n    Attributes:\n        _clf (GridSearchCV): The GridSearchCV object used for hyperparameter tuning.\n\n    Methods:\n        fit(x: XYData, y: XYData): Fit the GridSearchCV object to the given data.\n        predict(x: XYData) -&gt; XYData: Make predictions using the best estimator found by GridSearchCV.\n\n    Note:\n        This plugin uses scikit-learn's GridSearchCV, which may have its own dependencies and requirements.\n        Ensure that scikit-learn is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        filterx: Type[BaseFilter],\n        param_grid: Dict[str, Any],\n        scoring: str,\n        cv: int = 2,\n    ):\n        \"\"\"\n        Initialize a new GridSearchCVPlugin instance.\n\n        This constructor sets up the GridSearchCVPlugin with the specified BaseFilter,\n        parameter grid, scoring metric, and cross-validation strategy.\n\n        Args:\n            filterx (Type[BaseFilter]): The BaseFilter class to be tuned.\n            param_grid (Dict[str, Any]): Dictionary with parameters names as keys and lists of parameter settings to try as values.\n            scoring (str): Strategy to evaluate the performance of the cross-validated model on the test set.\n            cv (int): Determines the cross-validation splitting strategy. Defaults to 2.\n\n        Note:\n            The GridSearchCV object is initialized with a Pipeline containing the specified BaseFilter\n            wrapped in an SkWrapper to ensure compatibility with scikit-learn's API.\n        \"\"\"\n        super().__init__(filterx=filterx, param_grid=param_grid, scoring=scoring, cv=cv)\n\n        self._clf: GridSearchCV = GridSearchCV(\n            estimator=Pipeline(steps=[(filterx.__name__, SkWrapper(filterx))]),\n            param_grid=param_grid,\n            scoring=scoring,\n            cv=cv,\n            refit=True,\n            verbose=0,\n        )\n\n    def fit(self, x, y):\n        \"\"\"\n        Fit the GridSearchCV object to the given data.\n\n        This method performs the grid search over the specified parameter grid,\n        fitting the model with different parameter combinations and selecting the best one.\n\n        Args:\n            x (XYData): The input features.\n            y (XYData): The target values.\n\n        Note:\n            This method modifies the internal state of the GridSearchCV object,\n            storing the best parameters and the corresponding fitted model.\n        \"\"\"\n        self._clf.fit(x.value, y.value)  # type: ignore\n\n    def predict(self, x) -&gt; XYData:\n        \"\"\"\n        Make predictions using the best estimator found by GridSearchCV.\n\n        This method uses the best model found during the grid search to make predictions\n        on new data.\n\n        Args:\n            x (XYData): The input features.\n\n        Returns:\n            (XYData): The predicted values wrapped in an XYData object.\n\n        Note:\n            The predictions are wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._clf.predict(x.value))  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.GridSearchCVPlugin.__init__","title":"<code>__init__(filterx, param_grid, scoring, cv=2)</code>","text":"<p>Initialize a new GridSearchCVPlugin instance.</p> <p>This constructor sets up the GridSearchCVPlugin with the specified BaseFilter, parameter grid, scoring metric, and cross-validation strategy.</p> <p>Parameters:</p> Name Type Description Default <code>filterx</code> <code>Type[BaseFilter]</code> <p>The BaseFilter class to be tuned.</p> required <code>param_grid</code> <code>Dict[str, Any]</code> <p>Dictionary with parameters names as keys and lists of parameter settings to try as values.</p> required <code>scoring</code> <code>str</code> <p>Strategy to evaluate the performance of the cross-validated model on the test set.</p> required <code>cv</code> <code>int</code> <p>Determines the cross-validation splitting strategy. Defaults to 2.</p> <code>2</code> Note <p>The GridSearchCV object is initialized with a Pipeline containing the specified BaseFilter wrapped in an SkWrapper to ensure compatibility with scikit-learn's API.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def __init__(\n    self,\n    filterx: Type[BaseFilter],\n    param_grid: Dict[str, Any],\n    scoring: str,\n    cv: int = 2,\n):\n    \"\"\"\n    Initialize a new GridSearchCVPlugin instance.\n\n    This constructor sets up the GridSearchCVPlugin with the specified BaseFilter,\n    parameter grid, scoring metric, and cross-validation strategy.\n\n    Args:\n        filterx (Type[BaseFilter]): The BaseFilter class to be tuned.\n        param_grid (Dict[str, Any]): Dictionary with parameters names as keys and lists of parameter settings to try as values.\n        scoring (str): Strategy to evaluate the performance of the cross-validated model on the test set.\n        cv (int): Determines the cross-validation splitting strategy. Defaults to 2.\n\n    Note:\n        The GridSearchCV object is initialized with a Pipeline containing the specified BaseFilter\n        wrapped in an SkWrapper to ensure compatibility with scikit-learn's API.\n    \"\"\"\n    super().__init__(filterx=filterx, param_grid=param_grid, scoring=scoring, cv=cv)\n\n    self._clf: GridSearchCV = GridSearchCV(\n        estimator=Pipeline(steps=[(filterx.__name__, SkWrapper(filterx))]),\n        param_grid=param_grid,\n        scoring=scoring,\n        cv=cv,\n        refit=True,\n        verbose=0,\n    )\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.GridSearchCVPlugin.fit","title":"<code>fit(x, y)</code>","text":"<p>Fit the GridSearchCV object to the given data.</p> <p>This method performs the grid search over the specified parameter grid, fitting the model with different parameter combinations and selecting the best one.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>XYData</code> <p>The target values.</p> required Note <p>This method modifies the internal state of the GridSearchCV object, storing the best parameters and the corresponding fitted model.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def fit(self, x, y):\n    \"\"\"\n    Fit the GridSearchCV object to the given data.\n\n    This method performs the grid search over the specified parameter grid,\n    fitting the model with different parameter combinations and selecting the best one.\n\n    Args:\n        x (XYData): The input features.\n        y (XYData): The target values.\n\n    Note:\n        This method modifies the internal state of the GridSearchCV object,\n        storing the best parameters and the corresponding fitted model.\n    \"\"\"\n    self._clf.fit(x.value, y.value)  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.GridSearchCVPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the best estimator found by GridSearchCV.</p> <p>This method uses the best model found during the grid search to make predictions on new data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <p>Returns:</p> Type Description <code>XYData</code> <p>The predicted values wrapped in an XYData object.</p> Note <p>The predictions are wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def predict(self, x) -&gt; XYData:\n    \"\"\"\n    Make predictions using the best estimator found by GridSearchCV.\n\n    This method uses the best model found during the grid search to make predictions\n    on new data.\n\n    Args:\n        x (XYData): The input features.\n\n    Returns:\n        (XYData): The predicted values wrapped in an XYData object.\n\n    Note:\n        The predictions are wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._clf.predict(x.value))  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper","title":"<code>SkFilterWrapper</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>A wrapper class for BaseFilter that implements scikit-learn's BaseEstimator interface.</p> <p>This class enables BaseFilter objects to be used with scikit-learn's GridSearchCV, bridging the gap between framework3's filters and scikit-learn's estimator interface.</p> Key Features <ul> <li>Wraps BaseFilter objects to comply with scikit-learn's BaseEstimator interface</li> <li>Allows use of framework3 filters in scikit-learn's GridSearchCV</li> <li>Provides methods for fitting, predicting, and parameter management</li> </ul> Usage <p>The SkFilterWrapper can be used to wrap a BaseFilter for use with GridSearchCV:</p> <pre><code>from framework3.plugins.filters.clasification.svm import ClassifierSVMPlugin\nimport numpy as np\n\n# Set the class to be wrapped\nSkFilterWrapper.z_clazz = ClassifierSVMPlugin\n\n# Create an instance of SkFilterWrapper\nwrapper = SkFilterWrapper(C=1.0, kernel='rbf')\n\n# Use the wrapper with sklearn's GridSearchCV\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nwrapper.fit(X, y)\nprint(wrapper.predict([[2.5, 3.5]]))\n</code></pre> <p>Attributes:</p> Name Type Description <code>z_clazz</code> <code>Type[BaseFilter]</code> <p>The BaseFilter class to be wrapped.</p> <code>_model</code> <code>BaseFilter</code> <p>The instance of the wrapped BaseFilter.</p> <code>kwargs</code> <code>Dict[str, Any]</code> <p>The keyword arguments used to initialize the wrapped BaseFilter.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit the wrapped model to the given data.</p> <code>predict</code> <p>Make predictions using the wrapped model.</p> <code>get_params</code> <p>Get the parameters of the estimator.</p> <code>set_params</code> <p>Set the parameters of the estimator.</p> Note <p>This wrapper is specifically designed to work with framework3's BaseFilter and scikit-learn's GridSearchCV. Ensure that the wrapped BaseFilter is compatible with the data and operations you intend to perform.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>class SkFilterWrapper(BaseEstimator):\n    \"\"\"\n    A wrapper class for BaseFilter that implements scikit-learn's BaseEstimator interface.\n\n    This class enables BaseFilter objects to be used with scikit-learn's GridSearchCV,\n    bridging the gap between framework3's filters and scikit-learn's estimator interface.\n\n    Key Features:\n        - Wraps BaseFilter objects to comply with scikit-learn's BaseEstimator interface\n        - Allows use of framework3 filters in scikit-learn's GridSearchCV\n        - Provides methods for fitting, predicting, and parameter management\n\n    Usage:\n        The SkFilterWrapper can be used to wrap a BaseFilter for use with GridSearchCV:\n\n        ```python\n        from framework3.plugins.filters.clasification.svm import ClassifierSVMPlugin\n        import numpy as np\n\n        # Set the class to be wrapped\n        SkFilterWrapper.z_clazz = ClassifierSVMPlugin\n\n        # Create an instance of SkFilterWrapper\n        wrapper = SkFilterWrapper(C=1.0, kernel='rbf')\n\n        # Use the wrapper with sklearn's GridSearchCV\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        wrapper.fit(X, y)\n        print(wrapper.predict([[2.5, 3.5]]))\n        ```\n\n    Attributes:\n        z_clazz (Type[BaseFilter]): The BaseFilter class to be wrapped.\n        _model (BaseFilter): The instance of the wrapped BaseFilter.\n        kwargs (Dict[str, Any]): The keyword arguments used to initialize the wrapped BaseFilter.\n\n    Methods:\n        fit(x, y, *args, **kwargs): Fit the wrapped model to the given data.\n        predict(x): Make predictions using the wrapped model.\n        get_params(deep=True): Get the parameters of the estimator.\n        set_params(**parameters): Set the parameters of the estimator.\n\n    Note:\n        This wrapper is specifically designed to work with framework3's BaseFilter and\n        scikit-learn's GridSearchCV. Ensure that the wrapped BaseFilter is compatible\n        with the data and operations you intend to perform.\n    \"\"\"\n\n    z_clazz: Type[BaseFilter]\n\n    def __init__(self, clazz, **kwargs: Dict[str, Any]):\n        \"\"\"\n        Initialize a new SkFilterWrapper instance.\n\n        This constructor creates an instance of the specified BaseFilter class\n        with the given parameters.\n\n        Args:\n            clazz (Type[BaseFilter]): The BaseFilter class to be instantiated.\n            **kwargs (Dict[str, Any]): Keyword arguments to be passed to the BaseFilter constructor.\n\n        Note:\n            The initialized BaseFilter instance is stored in self._model, and\n            the kwargs are stored for later use in get_params and set_params.\n        \"\"\"\n        self._model = clazz(**kwargs)\n        self.kwargs = kwargs\n\n    def fit(\n        self, x: TxyData, y: TxyData, *args: List[Any], **kwargs: Dict[str, Any]\n    ) -&gt; \"SkFilterWrapper\":\n        \"\"\"\n        Fit the wrapped model to the given data.\n\n        This method wraps the input data in XYData objects and calls the fit method\n        of the wrapped BaseFilter.\n\n        Args:\n            x (TxyData): The input features.\n            y (TxyData): The target values.\n            *args (List[Any]): Additional positional arguments (not used).\n            **kwargs (Dict[str,Any]): Additional keyword arguments (not used).\n\n        Returns:\n            self (SkFilterWrapper): The fitted estimator.\n\n        Note:\n            This method modifies the internal state of the wrapped model.\n        \"\"\"\n        self._model.fit(XYData.mock(x), XYData.mock(y))\n        return self\n\n    def predict(self, x: TxyData) -&gt; TxyData:\n        \"\"\"\n        Make predictions using the wrapped model.\n\n        This method wraps the input data in an XYData object, calls the predict method\n        of the wrapped BaseFilter, and returns the raw value from the resulting XYData.\n\n        Args:\n            x (TxyData): The input features.\n\n        Returns:\n            (TxyData): The predicted values.\n\n        Note:\n            The return value is the raw prediction, not wrapped in an XYData object.\n        \"\"\"\n        return self._model.predict(XYData.mock(x)).value\n\n    def get_params(self, deep=True) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the parameters of the estimator.\n\n        This method returns the kwargs used to initialize the wrapped BaseFilter.\n\n        Args:\n            deep (bool): If True, will return the parameters for this estimator and\n                         contained subobjects that are estimators. Not used in this implementation.\n\n        Returns:\n            (Dict[str, Any]): Parameter names mapped to their values.\n\n        Note:\n            The 'deep' parameter is included for compatibility with scikit-learn,\n            but it doesn't affect the output in this implementation.\n        \"\"\"\n        return {**self.kwargs}\n\n    def set_params(self, **parameters: Dict[str, Any]) -&gt; \"SkFilterWrapper\":\n        \"\"\"\n        Set the parameters of the estimator.\n\n        This method creates a new instance of the wrapped BaseFilter with the specified parameters.\n\n        Args:\n            **parameters (Dict[str,Any]): Estimator parameters.\n\n        Returns:\n            self (SkFilterWrapper): Estimator instance.\n\n        Note:\n            This method replaces the existing wrapped model with a new instance.\n        \"\"\"\n        self._model = SkFilterWrapper.z_clazz(**parameters)  # type: ignore\n        return self\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper.kwargs","title":"<code>kwargs = kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper.z_clazz","title":"<code>z_clazz</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper.__init__","title":"<code>__init__(clazz, **kwargs)</code>","text":"<p>Initialize a new SkFilterWrapper instance.</p> <p>This constructor creates an instance of the specified BaseFilter class with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>clazz</code> <code>Type[BaseFilter]</code> <p>The BaseFilter class to be instantiated.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments to be passed to the BaseFilter constructor.</p> <code>{}</code> Note <p>The initialized BaseFilter instance is stored in self._model, and the kwargs are stored for later use in get_params and set_params.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def __init__(self, clazz, **kwargs: Dict[str, Any]):\n    \"\"\"\n    Initialize a new SkFilterWrapper instance.\n\n    This constructor creates an instance of the specified BaseFilter class\n    with the given parameters.\n\n    Args:\n        clazz (Type[BaseFilter]): The BaseFilter class to be instantiated.\n        **kwargs (Dict[str, Any]): Keyword arguments to be passed to the BaseFilter constructor.\n\n    Note:\n        The initialized BaseFilter instance is stored in self._model, and\n        the kwargs are stored for later use in get_params and set_params.\n    \"\"\"\n    self._model = clazz(**kwargs)\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper.fit","title":"<code>fit(x, y, *args, **kwargs)</code>","text":"<p>Fit the wrapped model to the given data.</p> <p>This method wraps the input data in XYData objects and calls the fit method of the wrapped BaseFilter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>TxyData</code> <p>The input features.</p> required <code>y</code> <code>TxyData</code> <p>The target values.</p> required <code>*args</code> <code>List[Any]</code> <p>Additional positional arguments (not used).</p> <code>()</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments (not used).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>SkFilterWrapper</code> <p>The fitted estimator.</p> Note <p>This method modifies the internal state of the wrapped model.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def fit(\n    self, x: TxyData, y: TxyData, *args: List[Any], **kwargs: Dict[str, Any]\n) -&gt; \"SkFilterWrapper\":\n    \"\"\"\n    Fit the wrapped model to the given data.\n\n    This method wraps the input data in XYData objects and calls the fit method\n    of the wrapped BaseFilter.\n\n    Args:\n        x (TxyData): The input features.\n        y (TxyData): The target values.\n        *args (List[Any]): Additional positional arguments (not used).\n        **kwargs (Dict[str,Any]): Additional keyword arguments (not used).\n\n    Returns:\n        self (SkFilterWrapper): The fitted estimator.\n\n    Note:\n        This method modifies the internal state of the wrapped model.\n    \"\"\"\n    self._model.fit(XYData.mock(x), XYData.mock(y))\n    return self\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get the parameters of the estimator.</p> <p>This method returns the kwargs used to initialize the wrapped BaseFilter.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, will return the parameters for this estimator and          contained subobjects that are estimators. Not used in this implementation.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parameter names mapped to their values.</p> Note <p>The 'deep' parameter is included for compatibility with scikit-learn, but it doesn't affect the output in this implementation.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def get_params(self, deep=True) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the parameters of the estimator.\n\n    This method returns the kwargs used to initialize the wrapped BaseFilter.\n\n    Args:\n        deep (bool): If True, will return the parameters for this estimator and\n                     contained subobjects that are estimators. Not used in this implementation.\n\n    Returns:\n        (Dict[str, Any]): Parameter names mapped to their values.\n\n    Note:\n        The 'deep' parameter is included for compatibility with scikit-learn,\n        but it doesn't affect the output in this implementation.\n    \"\"\"\n    return {**self.kwargs}\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the wrapped model.</p> <p>This method wraps the input data in an XYData object, calls the predict method of the wrapped BaseFilter, and returns the raw value from the resulting XYData.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>TxyData</code> <p>The input features.</p> required <p>Returns:</p> Type Description <code>TxyData</code> <p>The predicted values.</p> Note <p>The return value is the raw prediction, not wrapped in an XYData object.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def predict(self, x: TxyData) -&gt; TxyData:\n    \"\"\"\n    Make predictions using the wrapped model.\n\n    This method wraps the input data in an XYData object, calls the predict method\n    of the wrapped BaseFilter, and returns the raw value from the resulting XYData.\n\n    Args:\n        x (TxyData): The input features.\n\n    Returns:\n        (TxyData): The predicted values.\n\n    Note:\n        The return value is the raw prediction, not wrapped in an XYData object.\n    \"\"\"\n    return self._model.predict(XYData.mock(x)).value\n</code></pre>"},{"location":"api/plugins/filters/grid_search/#framework3.plugins.filters.grid_search.cv_grid_search.SkFilterWrapper.set_params","title":"<code>set_params(**parameters)</code>","text":"<p>Set the parameters of the estimator.</p> <p>This method creates a new instance of the wrapped BaseFilter with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**parameters</code> <code>Dict[str, Any]</code> <p>Estimator parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>SkFilterWrapper</code> <p>Estimator instance.</p> Note <p>This method replaces the existing wrapped model with a new instance.</p> Source code in <code>framework3/plugins/filters/grid_search/cv_grid_search.py</code> <pre><code>def set_params(self, **parameters: Dict[str, Any]) -&gt; \"SkFilterWrapper\":\n    \"\"\"\n    Set the parameters of the estimator.\n\n    This method creates a new instance of the wrapped BaseFilter with the specified parameters.\n\n    Args:\n        **parameters (Dict[str,Any]): Estimator parameters.\n\n    Returns:\n        self (SkFilterWrapper): Estimator instance.\n\n    Note:\n        This method replaces the existing wrapped model with a new instance.\n    \"\"\"\n    self._model = SkFilterWrapper.z_clazz(**parameters)  # type: ignore\n    return self\n</code></pre>"},{"location":"api/plugins/filters/regression/","title":"Regression","text":""},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression","title":"<code>framework3.plugins.filters.regression</code>","text":""},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.LogistiRegressionlugin","title":"<code>LogistiRegressionlugin</code>","text":"<p>               Bases: <code>BaseFilter</code>, <code>BasePlugin</code></p> <p>A plugin that implements logistic regression using scikit-learn's LogisticRegression.</p> <p>This plugin wraps the LogisticRegression model from scikit-learn and adapts it to work within the framework3 ecosystem, providing a seamless integration for logistic regression tasks.</p> Key Features <ul> <li>Utilizes scikit-learn's LogisticRegression implementation</li> <li>Supports customization of maximum iterations and tolerance</li> <li>Provides methods for fitting the model and making predictions</li> <li>Integrates with framework3's BaseFilter and BasePlugin interfaces</li> </ul> Usage <p>The LogistiRegressionlugin can be used to perform logistic regression on your data:</p> <pre><code>import numpy as np\nfrom framework3.base import XYData\nfrom framework3.plugins.filters.regression.logistic_regression import LogistiRegressionlugin\n\n# Create sample data\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\ny_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n# Create and fit the LogistiRegressionlugin\nlog_reg = LogistiRegressionlugin(max_ite=100, tol=1e-4)\nlog_reg.fit(X_data, y_data)\n\n# Make predictions\nX_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\npredictions = log_reg.predict(X_test)\nprint(predictions.value)\n\n# Access the underlying scikit-learn model\nprint(log_reg._logistic.coef_)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_logistic</code> <code>LogisticRegression</code> <p>The underlying scikit-learn LogisticRegression model.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the logistic regression model to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the fitted logistic regression model.</p> Note <p>This plugin uses scikit-learn's implementation of LogisticRegression, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>class LogistiRegressionlugin(BaseFilter, BasePlugin):\n    \"\"\"\n    A plugin that implements logistic regression using scikit-learn's LogisticRegression.\n\n    This plugin wraps the LogisticRegression model from scikit-learn and adapts it\n    to work within the framework3 ecosystem, providing a seamless integration for\n    logistic regression tasks.\n\n    Key Features:\n        - Utilizes scikit-learn's LogisticRegression implementation\n        - Supports customization of maximum iterations and tolerance\n        - Provides methods for fitting the model and making predictions\n        - Integrates with framework3's BaseFilter and BasePlugin interfaces\n\n    Usage:\n        The LogistiRegressionlugin can be used to perform logistic regression on your data:\n\n        ```python\n        import numpy as np\n        from framework3.base import XYData\n        from framework3.plugins.filters.regression.logistic_regression import LogistiRegressionlugin\n\n        # Create sample data\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        X_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n        y_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n        # Create and fit the LogistiRegressionlugin\n        log_reg = LogistiRegressionlugin(max_ite=100, tol=1e-4)\n        log_reg.fit(X_data, y_data)\n\n        # Make predictions\n        X_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\n        predictions = log_reg.predict(X_test)\n        print(predictions.value)\n\n        # Access the underlying scikit-learn model\n        print(log_reg._logistic.coef_)\n        ```\n\n    Attributes:\n        _logistic (LogisticRegression): The underlying scikit-learn LogisticRegression model.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the logistic regression model to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Make predictions using the fitted logistic regression model.\n\n    Note:\n        This plugin uses scikit-learn's implementation of LogisticRegression, which may have its own\n        dependencies and requirements. Ensure that scikit-learn is properly installed and compatible\n        with your environment.\n    \"\"\"\n\n    def __init__(self, max_ite: int, tol: float):\n        \"\"\"\n        Initialize a new LogistiRegressionlugin instance.\n\n        This constructor sets up the LogistiRegressionlugin with the specified parameters\n        and initializes the underlying scikit-learn LogisticRegression model.\n\n        Args:\n            max_ite (int): Maximum number of iterations for the solver to converge.\n            tol (float): Tolerance for stopping criteria.\n\n        Note:\n            The parameters are passed directly to scikit-learn's LogisticRegression.\n            Refer to scikit-learn's documentation for detailed information on these parameters.\n        \"\"\"\n        self._logistic = LogisticRegression(max_iter=max_ite, tol=tol)\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the logistic regression model to the given data.\n\n        This method trains the logistic regression model on the provided input features and target values.\n\n        Args:\n            x (XYData): The input features.\n            y (Optional[XYData]): The target values.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: The mean accuracy on the given test data and labels.\n\n        Raises:\n            ValueError: If y is None.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The score (mean accuracy) is returned as a measure of how well the model fits the data.\n        \"\"\"\n        if y is None:\n            raise ValueError(\n                \"Target values (y) cannot be None for logistic regression.\"\n            )\n        self._logistic.fit(x._value, y._value)  # type: ignore\n        return self._logistic.score(x._value, y._value)  # type: ignore\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted logistic regression model.\n\n        This method uses the trained logistic regression model to predict class labels for new input data.\n\n        Args:\n            x (XYData): The input features to predict.\n\n        Returns:\n            XYData: The predicted class labels wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's predict method internally.\n            The predictions are wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._logistic.predict(x.value))\n</code></pre>"},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.LogistiRegressionlugin.__init__","title":"<code>__init__(max_ite, tol)</code>","text":"<p>Initialize a new LogistiRegressionlugin instance.</p> <p>This constructor sets up the LogistiRegressionlugin with the specified parameters and initializes the underlying scikit-learn LogisticRegression model.</p> <p>Parameters:</p> Name Type Description Default <code>max_ite</code> <code>int</code> <p>Maximum number of iterations for the solver to converge.</p> required <code>tol</code> <code>float</code> <p>Tolerance for stopping criteria.</p> required Note <p>The parameters are passed directly to scikit-learn's LogisticRegression. Refer to scikit-learn's documentation for detailed information on these parameters.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>def __init__(self, max_ite: int, tol: float):\n    \"\"\"\n    Initialize a new LogistiRegressionlugin instance.\n\n    This constructor sets up the LogistiRegressionlugin with the specified parameters\n    and initializes the underlying scikit-learn LogisticRegression model.\n\n    Args:\n        max_ite (int): Maximum number of iterations for the solver to converge.\n        tol (float): Tolerance for stopping criteria.\n\n    Note:\n        The parameters are passed directly to scikit-learn's LogisticRegression.\n        Refer to scikit-learn's documentation for detailed information on these parameters.\n    \"\"\"\n    self._logistic = LogisticRegression(max_iter=max_ite, tol=tol)\n</code></pre>"},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.LogistiRegressionlugin.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the logistic regression model to the given data.</p> <p>This method trains the logistic regression model on the provided input features and target values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target values.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The mean accuracy on the given test data and labels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y is None.</p> Note <p>This method uses scikit-learn's fit method internally. The score (mean accuracy) is returned as a measure of how well the model fits the data.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the logistic regression model to the given data.\n\n    This method trains the logistic regression model on the provided input features and target values.\n\n    Args:\n        x (XYData): The input features.\n        y (Optional[XYData]): The target values.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: The mean accuracy on the given test data and labels.\n\n    Raises:\n        ValueError: If y is None.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The score (mean accuracy) is returned as a measure of how well the model fits the data.\n    \"\"\"\n    if y is None:\n        raise ValueError(\n            \"Target values (y) cannot be None for logistic regression.\"\n        )\n    self._logistic.fit(x._value, y._value)  # type: ignore\n    return self._logistic.score(x._value, y._value)  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.LogistiRegressionlugin.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted logistic regression model.</p> <p>This method uses the trained logistic regression model to predict class labels for new input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to predict.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predicted class labels wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's predict method internally. The predictions are wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted logistic regression model.\n\n    This method uses the trained logistic regression model to predict class labels for new input data.\n\n    Args:\n        x (XYData): The input features to predict.\n\n    Returns:\n        XYData: The predicted class labels wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's predict method internally.\n        The predictions are wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._logistic.predict(x.value))\n</code></pre>"},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.logistic_regression","title":"<code>logistic_regression</code>","text":""},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.logistic_regression.__all__","title":"<code>__all__ = ['LogistiRegressionlugin']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.logistic_regression.LogistiRegressionlugin","title":"<code>LogistiRegressionlugin</code>","text":"<p>               Bases: <code>BaseFilter</code>, <code>BasePlugin</code></p> <p>A plugin that implements logistic regression using scikit-learn's LogisticRegression.</p> <p>This plugin wraps the LogisticRegression model from scikit-learn and adapts it to work within the framework3 ecosystem, providing a seamless integration for logistic regression tasks.</p> Key Features <ul> <li>Utilizes scikit-learn's LogisticRegression implementation</li> <li>Supports customization of maximum iterations and tolerance</li> <li>Provides methods for fitting the model and making predictions</li> <li>Integrates with framework3's BaseFilter and BasePlugin interfaces</li> </ul> Usage <p>The LogistiRegressionlugin can be used to perform logistic regression on your data:</p> <pre><code>import numpy as np\nfrom framework3.base import XYData\nfrom framework3.plugins.filters.regression.logistic_regression import LogistiRegressionlugin\n\n# Create sample data\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\ny_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n# Create and fit the LogistiRegressionlugin\nlog_reg = LogistiRegressionlugin(max_ite=100, tol=1e-4)\nlog_reg.fit(X_data, y_data)\n\n# Make predictions\nX_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\npredictions = log_reg.predict(X_test)\nprint(predictions.value)\n\n# Access the underlying scikit-learn model\nprint(log_reg._logistic.coef_)\n</code></pre> <p>Attributes:</p> Name Type Description <code>_logistic</code> <code>LogisticRegression</code> <p>The underlying scikit-learn LogisticRegression model.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the logistic regression model to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the fitted logistic regression model.</p> Note <p>This plugin uses scikit-learn's implementation of LogisticRegression, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>class LogistiRegressionlugin(BaseFilter, BasePlugin):\n    \"\"\"\n    A plugin that implements logistic regression using scikit-learn's LogisticRegression.\n\n    This plugin wraps the LogisticRegression model from scikit-learn and adapts it\n    to work within the framework3 ecosystem, providing a seamless integration for\n    logistic regression tasks.\n\n    Key Features:\n        - Utilizes scikit-learn's LogisticRegression implementation\n        - Supports customization of maximum iterations and tolerance\n        - Provides methods for fitting the model and making predictions\n        - Integrates with framework3's BaseFilter and BasePlugin interfaces\n\n    Usage:\n        The LogistiRegressionlugin can be used to perform logistic regression on your data:\n\n        ```python\n        import numpy as np\n        from framework3.base import XYData\n        from framework3.plugins.filters.regression.logistic_regression import LogistiRegressionlugin\n\n        # Create sample data\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        X_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n        y_data = XYData(_hash='y_data', _path='/tmp', _value=y)\n\n        # Create and fit the LogistiRegressionlugin\n        log_reg = LogistiRegressionlugin(max_ite=100, tol=1e-4)\n        log_reg.fit(X_data, y_data)\n\n        # Make predictions\n        X_test = XYData(_hash='X_test', _path='/tmp', _value=np.array([[2.5, 3.5]]))\n        predictions = log_reg.predict(X_test)\n        print(predictions.value)\n\n        # Access the underlying scikit-learn model\n        print(log_reg._logistic.coef_)\n        ```\n\n    Attributes:\n        _logistic (LogisticRegression): The underlying scikit-learn LogisticRegression model.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the logistic regression model to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Make predictions using the fitted logistic regression model.\n\n    Note:\n        This plugin uses scikit-learn's implementation of LogisticRegression, which may have its own\n        dependencies and requirements. Ensure that scikit-learn is properly installed and compatible\n        with your environment.\n    \"\"\"\n\n    def __init__(self, max_ite: int, tol: float):\n        \"\"\"\n        Initialize a new LogistiRegressionlugin instance.\n\n        This constructor sets up the LogistiRegressionlugin with the specified parameters\n        and initializes the underlying scikit-learn LogisticRegression model.\n\n        Args:\n            max_ite (int): Maximum number of iterations for the solver to converge.\n            tol (float): Tolerance for stopping criteria.\n\n        Note:\n            The parameters are passed directly to scikit-learn's LogisticRegression.\n            Refer to scikit-learn's documentation for detailed information on these parameters.\n        \"\"\"\n        self._logistic = LogisticRegression(max_iter=max_ite, tol=tol)\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the logistic regression model to the given data.\n\n        This method trains the logistic regression model on the provided input features and target values.\n\n        Args:\n            x (XYData): The input features.\n            y (Optional[XYData]): The target values.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: The mean accuracy on the given test data and labels.\n\n        Raises:\n            ValueError: If y is None.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The score (mean accuracy) is returned as a measure of how well the model fits the data.\n        \"\"\"\n        if y is None:\n            raise ValueError(\n                \"Target values (y) cannot be None for logistic regression.\"\n            )\n        self._logistic.fit(x._value, y._value)  # type: ignore\n        return self._logistic.score(x._value, y._value)  # type: ignore\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted logistic regression model.\n\n        This method uses the trained logistic regression model to predict class labels for new input data.\n\n        Args:\n            x (XYData): The input features to predict.\n\n        Returns:\n            XYData: The predicted class labels wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's predict method internally.\n            The predictions are wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._logistic.predict(x.value))\n</code></pre>"},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.logistic_regression.LogistiRegressionlugin.__init__","title":"<code>__init__(max_ite, tol)</code>","text":"<p>Initialize a new LogistiRegressionlugin instance.</p> <p>This constructor sets up the LogistiRegressionlugin with the specified parameters and initializes the underlying scikit-learn LogisticRegression model.</p> <p>Parameters:</p> Name Type Description Default <code>max_ite</code> <code>int</code> <p>Maximum number of iterations for the solver to converge.</p> required <code>tol</code> <code>float</code> <p>Tolerance for stopping criteria.</p> required Note <p>The parameters are passed directly to scikit-learn's LogisticRegression. Refer to scikit-learn's documentation for detailed information on these parameters.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>def __init__(self, max_ite: int, tol: float):\n    \"\"\"\n    Initialize a new LogistiRegressionlugin instance.\n\n    This constructor sets up the LogistiRegressionlugin with the specified parameters\n    and initializes the underlying scikit-learn LogisticRegression model.\n\n    Args:\n        max_ite (int): Maximum number of iterations for the solver to converge.\n        tol (float): Tolerance for stopping criteria.\n\n    Note:\n        The parameters are passed directly to scikit-learn's LogisticRegression.\n        Refer to scikit-learn's documentation for detailed information on these parameters.\n    \"\"\"\n    self._logistic = LogisticRegression(max_iter=max_ite, tol=tol)\n</code></pre>"},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.logistic_regression.LogistiRegressionlugin.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the logistic regression model to the given data.</p> <p>This method trains the logistic regression model on the provided input features and target values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target values.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The mean accuracy on the given test data and labels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y is None.</p> Note <p>This method uses scikit-learn's fit method internally. The score (mean accuracy) is returned as a measure of how well the model fits the data.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the logistic regression model to the given data.\n\n    This method trains the logistic regression model on the provided input features and target values.\n\n    Args:\n        x (XYData): The input features.\n        y (Optional[XYData]): The target values.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: The mean accuracy on the given test data and labels.\n\n    Raises:\n        ValueError: If y is None.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The score (mean accuracy) is returned as a measure of how well the model fits the data.\n    \"\"\"\n    if y is None:\n        raise ValueError(\n            \"Target values (y) cannot be None for logistic regression.\"\n        )\n    self._logistic.fit(x._value, y._value)  # type: ignore\n    return self._logistic.score(x._value, y._value)  # type: ignore\n</code></pre>"},{"location":"api/plugins/filters/regression/#framework3.plugins.filters.regression.logistic_regression.LogistiRegressionlugin.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted logistic regression model.</p> <p>This method uses the trained logistic regression model to predict class labels for new input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to predict.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predicted class labels wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's predict method internally. The predictions are wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/regression/logistic_regression.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted logistic regression model.\n\n    This method uses the trained logistic regression model to predict class labels for new input data.\n\n    Args:\n        x (XYData): The input features to predict.\n\n    Returns:\n        XYData: The predicted class labels wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's predict method internally.\n        The predictions are wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._logistic.predict(x.value))\n</code></pre>"},{"location":"api/plugins/filters/text_processing/","title":"Text Processing","text":""},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm","title":"<code>framework3.plugins.filters.llm</code>","text":""},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.HuggingFaceSentenceTransformerPlugin","title":"<code>HuggingFaceSentenceTransformerPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code>, <code>BasePlugin</code></p> <p>A plugin for generating sentence embeddings using Hugging Face's Sentence Transformers.</p> <p>This plugin integrates Sentence Transformers from Hugging Face into the framework3 ecosystem, allowing for easy generation of sentence embeddings within pipelines.</p> Key Features <ul> <li>Utilizes pre-trained Sentence Transformer models from Hugging Face</li> <li>Supports custom model selection</li> <li>Generates embeddings for input text data</li> <li>Integrates seamlessly with framework3's BaseFilter interface</li> </ul> Usage <p>The HuggingFaceSentenceTransformerPlugin can be used to generate embeddings for text data:</p> <pre><code>from framework3.plugins.filters.llm.huggingface_st import HuggingFaceSentenceTransformerPlugin\nfrom framework3.base.base_types import XYData\n\n# Create an instance of the plugin\nst_plugin = HuggingFaceSentenceTransformerPlugin(model_name=\"all-MiniLM-L6-v2\")\n\n# Prepare input data\ninput_texts = [\"This is a sample sentence.\", \"Another example text.\"]\nx_data = XYData(_hash='input_data', _path='/tmp', _value=input_texts)\n\n# Generate embeddings\nembeddings = st_plugin.predict(x_data)\nprint(embeddings.value)\n</code></pre> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the Sentence Transformer model to use.</p> <code>_model</code> <code>SentenceTransformer</code> <p>The underlying Sentence Transformer model.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: XYData | None) -&gt; float | None: Placeholder method for compatibility with BaseFilter interface.</p> <code>predict</code> <p>XYData) -&gt; XYData: Generate embeddings for the input text data.</p> Note <p>This plugin requires the <code>sentence-transformers</code> library to be installed. Ensure that you have the necessary dependencies installed in your environment.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>@Container.bind()\nclass HuggingFaceSentenceTransformerPlugin(BaseFilter, BasePlugin):\n    \"\"\"\n    A plugin for generating sentence embeddings using Hugging Face's Sentence Transformers.\n\n    This plugin integrates Sentence Transformers from Hugging Face into the framework3 ecosystem,\n    allowing for easy generation of sentence embeddings within pipelines.\n\n    Key Features:\n        - Utilizes pre-trained Sentence Transformer models from Hugging Face\n        - Supports custom model selection\n        - Generates embeddings for input text data\n        - Integrates seamlessly with framework3's BaseFilter interface\n\n    Usage:\n        The HuggingFaceSentenceTransformerPlugin can be used to generate embeddings for text data:\n\n        ```python\n        from framework3.plugins.filters.llm.huggingface_st import HuggingFaceSentenceTransformerPlugin\n        from framework3.base.base_types import XYData\n\n        # Create an instance of the plugin\n        st_plugin = HuggingFaceSentenceTransformerPlugin(model_name=\"all-MiniLM-L6-v2\")\n\n        # Prepare input data\n        input_texts = [\"This is a sample sentence.\", \"Another example text.\"]\n        x_data = XYData(_hash='input_data', _path='/tmp', _value=input_texts)\n\n        # Generate embeddings\n        embeddings = st_plugin.predict(x_data)\n        print(embeddings.value)\n        ```\n\n    Attributes:\n        model_name (str): The name of the Sentence Transformer model to use.\n        _model (SentenceTransformer): The underlying Sentence Transformer model.\n\n    Methods:\n        fit(x: XYData, y: XYData | None) -&gt; float | None:\n            Placeholder method for compatibility with BaseFilter interface.\n        predict(x: XYData) -&gt; XYData:\n            Generate embeddings for the input text data.\n\n    Note:\n        This plugin requires the `sentence-transformers` library to be installed.\n        Ensure that you have the necessary dependencies installed in your environment.\n    \"\"\"\n\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Initialize a new HuggingFaceSentenceTransformerPlugin instance.\n\n        This constructor sets up the plugin with the specified Sentence Transformer model.\n\n        Args:\n            model_name (str): The name of the Sentence Transformer model to use.\n                              Defaults to \"all-MiniLM-L6-v2\".\n\n        Note:\n            The specified model will be downloaded and loaded upon initialization.\n            Ensure you have a stable internet connection and sufficient disk space.\n        \"\"\"\n        super().__init__()\n        self.model_name = model_name\n        self._model = SentenceTransformer(self.model_name)\n\n    def fit(self, x: XYData, y: XYData | None) -&gt; float | None:\n        \"\"\"\n        Placeholder method for compatibility with BaseFilter interface.\n\n        This method is not implemented as Sentence Transformers typically don't require fitting.\n\n        Args:\n            x (XYData): The input features (not used).\n            y (XYData | None): The target values (not used).\n\n        Returns:\n            float | None: Always returns None.\n\n        Note:\n            This method is included for API consistency but does not perform any operation.\n        \"\"\"\n        ...\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Generate embeddings for the input text data.\n\n        This method uses the loaded Sentence Transformer model to create embeddings\n        for the input text.\n\n        Args:\n            x (XYData): The input text data to generate embeddings for.\n\n        Returns:\n            XYData: The generated embeddings wrapped in an XYData object.\n\n        Note:\n            The input text should be in a format compatible with the Sentence Transformer model.\n            The output embeddings are converted to a PyTorch tensor before being wrapped in XYData.\n        \"\"\"\n        embeddings = self._model.encode(x.value)\n        return XYData.mock(torch.tensor(embeddings))\n</code></pre>"},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.HuggingFaceSentenceTransformerPlugin.model_name","title":"<code>model_name = model_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.HuggingFaceSentenceTransformerPlugin.__init__","title":"<code>__init__(model_name='all-MiniLM-L6-v2')</code>","text":"<p>Initialize a new HuggingFaceSentenceTransformerPlugin instance.</p> <p>This constructor sets up the plugin with the specified Sentence Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Sentence Transformer model to use.               Defaults to \"all-MiniLM-L6-v2\".</p> <code>'all-MiniLM-L6-v2'</code> Note <p>The specified model will be downloaded and loaded upon initialization. Ensure you have a stable internet connection and sufficient disk space.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n    \"\"\"\n    Initialize a new HuggingFaceSentenceTransformerPlugin instance.\n\n    This constructor sets up the plugin with the specified Sentence Transformer model.\n\n    Args:\n        model_name (str): The name of the Sentence Transformer model to use.\n                          Defaults to \"all-MiniLM-L6-v2\".\n\n    Note:\n        The specified model will be downloaded and loaded upon initialization.\n        Ensure you have a stable internet connection and sufficient disk space.\n    \"\"\"\n    super().__init__()\n    self.model_name = model_name\n    self._model = SentenceTransformer(self.model_name)\n</code></pre>"},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.HuggingFaceSentenceTransformerPlugin.fit","title":"<code>fit(x, y)</code>","text":"<p>Placeholder method for compatibility with BaseFilter interface.</p> <p>This method is not implemented as Sentence Transformers typically don't require fitting.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features (not used).</p> required <code>y</code> <code>XYData | None</code> <p>The target values (not used).</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>float | None: Always returns None.</p> Note <p>This method is included for API consistency but does not perform any operation.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>def fit(self, x: XYData, y: XYData | None) -&gt; float | None:\n    \"\"\"\n    Placeholder method for compatibility with BaseFilter interface.\n\n    This method is not implemented as Sentence Transformers typically don't require fitting.\n\n    Args:\n        x (XYData): The input features (not used).\n        y (XYData | None): The target values (not used).\n\n    Returns:\n        float | None: Always returns None.\n\n    Note:\n        This method is included for API consistency but does not perform any operation.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.HuggingFaceSentenceTransformerPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Generate embeddings for the input text data.</p> <p>This method uses the loaded Sentence Transformer model to create embeddings for the input text.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input text data to generate embeddings for.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The generated embeddings wrapped in an XYData object.</p> Note <p>The input text should be in a format compatible with the Sentence Transformer model. The output embeddings are converted to a PyTorch tensor before being wrapped in XYData.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Generate embeddings for the input text data.\n\n    This method uses the loaded Sentence Transformer model to create embeddings\n    for the input text.\n\n    Args:\n        x (XYData): The input text data to generate embeddings for.\n\n    Returns:\n        XYData: The generated embeddings wrapped in an XYData object.\n\n    Note:\n        The input text should be in a format compatible with the Sentence Transformer model.\n        The output embeddings are converted to a PyTorch tensor before being wrapped in XYData.\n    \"\"\"\n    embeddings = self._model.encode(x.value)\n    return XYData.mock(torch.tensor(embeddings))\n</code></pre>"},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.huggingface_st","title":"<code>huggingface_st</code>","text":""},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.huggingface_st.__all__","title":"<code>__all__ = ['HuggingFaceSentenceTransformerPlugin']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.huggingface_st.HuggingFaceSentenceTransformerPlugin","title":"<code>HuggingFaceSentenceTransformerPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code>, <code>BasePlugin</code></p> <p>A plugin for generating sentence embeddings using Hugging Face's Sentence Transformers.</p> <p>This plugin integrates Sentence Transformers from Hugging Face into the framework3 ecosystem, allowing for easy generation of sentence embeddings within pipelines.</p> Key Features <ul> <li>Utilizes pre-trained Sentence Transformer models from Hugging Face</li> <li>Supports custom model selection</li> <li>Generates embeddings for input text data</li> <li>Integrates seamlessly with framework3's BaseFilter interface</li> </ul> Usage <p>The HuggingFaceSentenceTransformerPlugin can be used to generate embeddings for text data:</p> <pre><code>from framework3.plugins.filters.llm.huggingface_st import HuggingFaceSentenceTransformerPlugin\nfrom framework3.base.base_types import XYData\n\n# Create an instance of the plugin\nst_plugin = HuggingFaceSentenceTransformerPlugin(model_name=\"all-MiniLM-L6-v2\")\n\n# Prepare input data\ninput_texts = [\"This is a sample sentence.\", \"Another example text.\"]\nx_data = XYData(_hash='input_data', _path='/tmp', _value=input_texts)\n\n# Generate embeddings\nembeddings = st_plugin.predict(x_data)\nprint(embeddings.value)\n</code></pre> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the Sentence Transformer model to use.</p> <code>_model</code> <code>SentenceTransformer</code> <p>The underlying Sentence Transformer model.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: XYData | None) -&gt; float | None: Placeholder method for compatibility with BaseFilter interface.</p> <code>predict</code> <p>XYData) -&gt; XYData: Generate embeddings for the input text data.</p> Note <p>This plugin requires the <code>sentence-transformers</code> library to be installed. Ensure that you have the necessary dependencies installed in your environment.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>@Container.bind()\nclass HuggingFaceSentenceTransformerPlugin(BaseFilter, BasePlugin):\n    \"\"\"\n    A plugin for generating sentence embeddings using Hugging Face's Sentence Transformers.\n\n    This plugin integrates Sentence Transformers from Hugging Face into the framework3 ecosystem,\n    allowing for easy generation of sentence embeddings within pipelines.\n\n    Key Features:\n        - Utilizes pre-trained Sentence Transformer models from Hugging Face\n        - Supports custom model selection\n        - Generates embeddings for input text data\n        - Integrates seamlessly with framework3's BaseFilter interface\n\n    Usage:\n        The HuggingFaceSentenceTransformerPlugin can be used to generate embeddings for text data:\n\n        ```python\n        from framework3.plugins.filters.llm.huggingface_st import HuggingFaceSentenceTransformerPlugin\n        from framework3.base.base_types import XYData\n\n        # Create an instance of the plugin\n        st_plugin = HuggingFaceSentenceTransformerPlugin(model_name=\"all-MiniLM-L6-v2\")\n\n        # Prepare input data\n        input_texts = [\"This is a sample sentence.\", \"Another example text.\"]\n        x_data = XYData(_hash='input_data', _path='/tmp', _value=input_texts)\n\n        # Generate embeddings\n        embeddings = st_plugin.predict(x_data)\n        print(embeddings.value)\n        ```\n\n    Attributes:\n        model_name (str): The name of the Sentence Transformer model to use.\n        _model (SentenceTransformer): The underlying Sentence Transformer model.\n\n    Methods:\n        fit(x: XYData, y: XYData | None) -&gt; float | None:\n            Placeholder method for compatibility with BaseFilter interface.\n        predict(x: XYData) -&gt; XYData:\n            Generate embeddings for the input text data.\n\n    Note:\n        This plugin requires the `sentence-transformers` library to be installed.\n        Ensure that you have the necessary dependencies installed in your environment.\n    \"\"\"\n\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Initialize a new HuggingFaceSentenceTransformerPlugin instance.\n\n        This constructor sets up the plugin with the specified Sentence Transformer model.\n\n        Args:\n            model_name (str): The name of the Sentence Transformer model to use.\n                              Defaults to \"all-MiniLM-L6-v2\".\n\n        Note:\n            The specified model will be downloaded and loaded upon initialization.\n            Ensure you have a stable internet connection and sufficient disk space.\n        \"\"\"\n        super().__init__()\n        self.model_name = model_name\n        self._model = SentenceTransformer(self.model_name)\n\n    def fit(self, x: XYData, y: XYData | None) -&gt; float | None:\n        \"\"\"\n        Placeholder method for compatibility with BaseFilter interface.\n\n        This method is not implemented as Sentence Transformers typically don't require fitting.\n\n        Args:\n            x (XYData): The input features (not used).\n            y (XYData | None): The target values (not used).\n\n        Returns:\n            float | None: Always returns None.\n\n        Note:\n            This method is included for API consistency but does not perform any operation.\n        \"\"\"\n        ...\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Generate embeddings for the input text data.\n\n        This method uses the loaded Sentence Transformer model to create embeddings\n        for the input text.\n\n        Args:\n            x (XYData): The input text data to generate embeddings for.\n\n        Returns:\n            XYData: The generated embeddings wrapped in an XYData object.\n\n        Note:\n            The input text should be in a format compatible with the Sentence Transformer model.\n            The output embeddings are converted to a PyTorch tensor before being wrapped in XYData.\n        \"\"\"\n        embeddings = self._model.encode(x.value)\n        return XYData.mock(torch.tensor(embeddings))\n</code></pre>"},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.huggingface_st.HuggingFaceSentenceTransformerPlugin.model_name","title":"<code>model_name = model_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.huggingface_st.HuggingFaceSentenceTransformerPlugin.__init__","title":"<code>__init__(model_name='all-MiniLM-L6-v2')</code>","text":"<p>Initialize a new HuggingFaceSentenceTransformerPlugin instance.</p> <p>This constructor sets up the plugin with the specified Sentence Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Sentence Transformer model to use.               Defaults to \"all-MiniLM-L6-v2\".</p> <code>'all-MiniLM-L6-v2'</code> Note <p>The specified model will be downloaded and loaded upon initialization. Ensure you have a stable internet connection and sufficient disk space.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n    \"\"\"\n    Initialize a new HuggingFaceSentenceTransformerPlugin instance.\n\n    This constructor sets up the plugin with the specified Sentence Transformer model.\n\n    Args:\n        model_name (str): The name of the Sentence Transformer model to use.\n                          Defaults to \"all-MiniLM-L6-v2\".\n\n    Note:\n        The specified model will be downloaded and loaded upon initialization.\n        Ensure you have a stable internet connection and sufficient disk space.\n    \"\"\"\n    super().__init__()\n    self.model_name = model_name\n    self._model = SentenceTransformer(self.model_name)\n</code></pre>"},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.huggingface_st.HuggingFaceSentenceTransformerPlugin.fit","title":"<code>fit(x, y)</code>","text":"<p>Placeholder method for compatibility with BaseFilter interface.</p> <p>This method is not implemented as Sentence Transformers typically don't require fitting.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features (not used).</p> required <code>y</code> <code>XYData | None</code> <p>The target values (not used).</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>float | None: Always returns None.</p> Note <p>This method is included for API consistency but does not perform any operation.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>def fit(self, x: XYData, y: XYData | None) -&gt; float | None:\n    \"\"\"\n    Placeholder method for compatibility with BaseFilter interface.\n\n    This method is not implemented as Sentence Transformers typically don't require fitting.\n\n    Args:\n        x (XYData): The input features (not used).\n        y (XYData | None): The target values (not used).\n\n    Returns:\n        float | None: Always returns None.\n\n    Note:\n        This method is included for API consistency but does not perform any operation.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/plugins/filters/text_processing/#framework3.plugins.filters.llm.huggingface_st.HuggingFaceSentenceTransformerPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Generate embeddings for the input text data.</p> <p>This method uses the loaded Sentence Transformer model to create embeddings for the input text.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input text data to generate embeddings for.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The generated embeddings wrapped in an XYData object.</p> Note <p>The input text should be in a format compatible with the Sentence Transformer model. The output embeddings are converted to a PyTorch tensor before being wrapped in XYData.</p> Source code in <code>framework3/plugins/filters/llm/huggingface_st.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Generate embeddings for the input text data.\n\n    This method uses the loaded Sentence Transformer model to create embeddings\n    for the input text.\n\n    Args:\n        x (XYData): The input text data to generate embeddings for.\n\n    Returns:\n        XYData: The generated embeddings wrapped in an XYData object.\n\n    Note:\n        The input text should be in a format compatible with the Sentence Transformer model.\n        The output embeddings are converted to a PyTorch tensor before being wrapped in XYData.\n    \"\"\"\n    embeddings = self._model.encode(x.value)\n    return XYData.mock(torch.tensor(embeddings))\n</code></pre>"},{"location":"api/plugins/filters/transformation/","title":"Transformation","text":""},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation","title":"<code>framework3.plugins.filters.transformation</code>","text":""},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.PCAPlugin","title":"<code>PCAPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A plugin for performing Principal Component Analysis (PCA) on input data.</p> <p>This plugin integrates scikit-learn's PCA implementation into the framework3 ecosystem, allowing for easy dimensionality reduction within pipelines.</p> Key Features <ul> <li>Utilizes scikit-learn's PCA for dimensionality reduction</li> <li>Supports customization of the number of components to keep</li> <li>Provides methods for fitting the PCA model and transforming data</li> <li>Integrates seamlessly with framework3's BaseFilter interface</li> <li>Includes a static method for generating parameter grids for hyperparameter tuning</li> </ul> Usage <p>The PCAPlugin can be used to perform dimensionality reduction on your data:</p> <pre><code>from framework3.plugins.filters.transformation.pca import PCAPlugin\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create a PCAPlugin instance\npca_plugin = PCAPlugin(n_components=2)\n\n# Create some sample data\nX = XYData.mock(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\ny = None  # PCA doesn't use y for fitting\n\n# Fit the PCA model\npca_plugin.fit(X, y)\n\n# Transform new data\nnew_data = XYData.mock(np.array([[2, 3, 4], [5, 6, 7]]))\ntransformed_data = pca_plugin.predict(new_data)\nprint(transformed_data.value)  # This will be a 2x2 array\n</code></pre> <p>Attributes:</p> Name Type Description <code>_pca</code> <code>PCA</code> <p>The underlying scikit-learn PCA object used for dimensionality reduction.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the PCA model to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Apply dimensionality reduction to the input data.</p> <code>item_grid</code> <p>List[int]) -&gt; Dict[str, Any]: Generate a parameter grid for hyperparameter tuning.</p> Note <p>This plugin uses scikit-learn's implementation of PCA, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>@Container.bind()\nclass PCAPlugin(BaseFilter):\n    \"\"\"\n    A plugin for performing Principal Component Analysis (PCA) on input data.\n\n    This plugin integrates scikit-learn's PCA implementation into the framework3 ecosystem,\n    allowing for easy dimensionality reduction within pipelines.\n\n    Key Features:\n        - Utilizes scikit-learn's PCA for dimensionality reduction\n        - Supports customization of the number of components to keep\n        - Provides methods for fitting the PCA model and transforming data\n        - Integrates seamlessly with framework3's BaseFilter interface\n        - Includes a static method for generating parameter grids for hyperparameter tuning\n\n    Usage:\n        The PCAPlugin can be used to perform dimensionality reduction on your data:\n\n        ```python\n        from framework3.plugins.filters.transformation.pca import PCAPlugin\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create a PCAPlugin instance\n        pca_plugin = PCAPlugin(n_components=2)\n\n        # Create some sample data\n        X = XYData.mock(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n        y = None  # PCA doesn't use y for fitting\n\n        # Fit the PCA model\n        pca_plugin.fit(X, y)\n\n        # Transform new data\n        new_data = XYData.mock(np.array([[2, 3, 4], [5, 6, 7]]))\n        transformed_data = pca_plugin.predict(new_data)\n        print(transformed_data.value)  # This will be a 2x2 array\n        ```\n\n    Attributes:\n        _pca (PCA): The underlying scikit-learn PCA object used for dimensionality reduction.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the PCA model to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Apply dimensionality reduction to the input data.\n        item_grid(n_components: List[int]) -&gt; Dict[str, Any]:\n            Generate a parameter grid for hyperparameter tuning.\n\n    Note:\n        This plugin uses scikit-learn's implementation of PCA, which may have its own\n        dependencies and requirements. Ensure that scikit-learn is properly installed\n        and compatible with your environment.\n    \"\"\"\n\n    def __init__(self, n_components: int = 2):\n        \"\"\"\n        Initialize a new PCAPlugin instance.\n\n        This constructor sets up the PCAPlugin with the specified number of components\n        and initializes the underlying scikit-learn PCA object.\n\n        Args:\n            n_components (int): The number of components to keep after dimensionality reduction.\n                                Defaults to 2.\n\n        Note:\n            The n_components parameter is passed directly to scikit-learn's PCA.\n            Refer to scikit-learn's documentation for detailed information on this parameter.\n        \"\"\"\n        super().__init__(\n            n_components=n_components\n        )  # Initialize the BaseFilter and BasePlugin parent classes.\n        self._pca = PCA(n_components=n_components)\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the PCA model to the given data.\n\n        This method trains the PCA model on the provided input features.\n\n        Args:\n            x (XYData): The input features to fit the PCA model.\n            y (Optional[XYData]): Not used in PCA, but required by the BaseFilter interface.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: Always returns None as PCA doesn't have a standard evaluation metric.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The y parameter is ignored as PCA is an unsupervised method.\n        \"\"\"\n        self._pca.fit(x.value)\n        return None\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Apply dimensionality reduction to the input data.\n\n        This method uses the trained PCA model to transform new input data,\n        reducing its dimensionality.\n\n        Args:\n            x (XYData): The input features to transform.\n\n        Returns:\n            XYData: The transformed data with reduced dimensionality, wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's transform method internally.\n            The transformed data is wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._pca.transform(x.value))\n\n    @staticmethod\n    def item_grid(n_components: List[int]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate a parameter grid for hyperparameter tuning.\n\n        This static method creates a dictionary that can be used for grid search\n        over different numbers of components in PCA.\n\n        Args:\n            n_components (List[int]): A list of integers representing different numbers\n                                      of components to try in the grid search.\n\n        Returns:\n            Dict[str, Any]: A dictionary with the parameter name as key and the list of\n                            values to try as value.\n\n        Note:\n            This method is typically used in conjunction with hyperparameter tuning\n            techniques like GridSearchCV.\n        \"\"\"\n        return {\"PCAPlugin__n_components\": n_components}\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.PCAPlugin.__init__","title":"<code>__init__(n_components=2)</code>","text":"<p>Initialize a new PCAPlugin instance.</p> <p>This constructor sets up the PCAPlugin with the specified number of components and initializes the underlying scikit-learn PCA object.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>The number of components to keep after dimensionality reduction.                 Defaults to 2.</p> <code>2</code> Note <p>The n_components parameter is passed directly to scikit-learn's PCA. Refer to scikit-learn's documentation for detailed information on this parameter.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>def __init__(self, n_components: int = 2):\n    \"\"\"\n    Initialize a new PCAPlugin instance.\n\n    This constructor sets up the PCAPlugin with the specified number of components\n    and initializes the underlying scikit-learn PCA object.\n\n    Args:\n        n_components (int): The number of components to keep after dimensionality reduction.\n                            Defaults to 2.\n\n    Note:\n        The n_components parameter is passed directly to scikit-learn's PCA.\n        Refer to scikit-learn's documentation for detailed information on this parameter.\n    \"\"\"\n    super().__init__(\n        n_components=n_components\n    )  # Initialize the BaseFilter and BasePlugin parent classes.\n    self._pca = PCA(n_components=n_components)\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.PCAPlugin.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the PCA model to the given data.</p> <p>This method trains the PCA model on the provided input features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to fit the PCA model.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Not used in PCA, but required by the BaseFilter interface.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: Always returns None as PCA doesn't have a standard evaluation metric.</p> Note <p>This method uses scikit-learn's fit method internally. The y parameter is ignored as PCA is an unsupervised method.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the PCA model to the given data.\n\n    This method trains the PCA model on the provided input features.\n\n    Args:\n        x (XYData): The input features to fit the PCA model.\n        y (Optional[XYData]): Not used in PCA, but required by the BaseFilter interface.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: Always returns None as PCA doesn't have a standard evaluation metric.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The y parameter is ignored as PCA is an unsupervised method.\n    \"\"\"\n    self._pca.fit(x.value)\n    return None\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.PCAPlugin.item_grid","title":"<code>item_grid(n_components)</code>  <code>staticmethod</code>","text":"<p>Generate a parameter grid for hyperparameter tuning.</p> <p>This static method creates a dictionary that can be used for grid search over different numbers of components in PCA.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>List[int]</code> <p>A list of integers representing different numbers                       of components to try in the grid search.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary with the parameter name as key and the list of             values to try as value.</p> Note <p>This method is typically used in conjunction with hyperparameter tuning techniques like GridSearchCV.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>@staticmethod\ndef item_grid(n_components: List[int]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a parameter grid for hyperparameter tuning.\n\n    This static method creates a dictionary that can be used for grid search\n    over different numbers of components in PCA.\n\n    Args:\n        n_components (List[int]): A list of integers representing different numbers\n                                  of components to try in the grid search.\n\n    Returns:\n        Dict[str, Any]: A dictionary with the parameter name as key and the list of\n                        values to try as value.\n\n    Note:\n        This method is typically used in conjunction with hyperparameter tuning\n        techniques like GridSearchCV.\n    \"\"\"\n    return {\"PCAPlugin__n_components\": n_components}\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.PCAPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Apply dimensionality reduction to the input data.</p> <p>This method uses the trained PCA model to transform new input data, reducing its dimensionality.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to transform.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The transformed data with reduced dimensionality, wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's transform method internally. The transformed data is wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Apply dimensionality reduction to the input data.\n\n    This method uses the trained PCA model to transform new input data,\n    reducing its dimensionality.\n\n    Args:\n        x (XYData): The input features to transform.\n\n    Returns:\n        XYData: The transformed data with reduced dimensionality, wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's transform method internally.\n        The transformed data is wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._pca.transform(x.value))\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.StandardScalerPlugin","title":"<code>StandardScalerPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A plugin for standardizing features by removing the mean and scaling to unit variance.</p> <p>This plugin integrates scikit-learn's StandardScaler into the framework3 ecosystem, allowing for easy feature standardization within pipelines.</p> Key Features <ul> <li>Utilizes scikit-learn's StandardScaler for feature standardization</li> <li>Removes the mean and scales features to unit variance</li> <li>Provides methods for fitting the scaler and transforming data</li> <li>Integrates seamlessly with framework3's BaseFilter interface</li> </ul> Usage <p>The StandardScalerPlugin can be used to standardize features in your data:</p> <pre><code>from framework3.plugins.filters.transformation.scaler import StandardScalerPlugin\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create a StandardScalerPlugin instance\nscaler_plugin = StandardScalerPlugin()\n\n# Create some sample data\nX = XYData.mock(np.array([[0, 0], [0, 0], [1, 1], [1, 1]]))\ny = None  # StandardScaler doesn't use y for fitting\n\n# Fit the StandardScaler\nscaler_plugin.fit(X, y)\n\n# Transform new data\nnew_data = XYData.mock(np.array([[2, 2], [-1, -1]]))\nscaled_data = scaler_plugin.predict(new_data)\nprint(scaled_data.value)\n# Output will be standardized, with mean 0 and unit variance\n# For example: [[ 1.41421356  1.41421356]\n#               [-1.41421356 -1.41421356]]\n</code></pre> <p>Attributes:</p> Name Type Description <code>_scaler</code> <code>StandardScaler</code> <p>The underlying scikit-learn StandardScaler object used for standardization.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the StandardScaler to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Perform standardization on the input data.</p> Note <p>This plugin uses scikit-learn's implementation of StandardScaler, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>@Container.bind()\nclass StandardScalerPlugin(BaseFilter):\n    \"\"\"\n    A plugin for standardizing features by removing the mean and scaling to unit variance.\n\n    This plugin integrates scikit-learn's StandardScaler into the framework3 ecosystem,\n    allowing for easy feature standardization within pipelines.\n\n    Key Features:\n        - Utilizes scikit-learn's StandardScaler for feature standardization\n        - Removes the mean and scales features to unit variance\n        - Provides methods for fitting the scaler and transforming data\n        - Integrates seamlessly with framework3's BaseFilter interface\n\n    Usage:\n        The StandardScalerPlugin can be used to standardize features in your data:\n\n        ```python\n        from framework3.plugins.filters.transformation.scaler import StandardScalerPlugin\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create a StandardScalerPlugin instance\n        scaler_plugin = StandardScalerPlugin()\n\n        # Create some sample data\n        X = XYData.mock(np.array([[0, 0], [0, 0], [1, 1], [1, 1]]))\n        y = None  # StandardScaler doesn't use y for fitting\n\n        # Fit the StandardScaler\n        scaler_plugin.fit(X, y)\n\n        # Transform new data\n        new_data = XYData.mock(np.array([[2, 2], [-1, -1]]))\n        scaled_data = scaler_plugin.predict(new_data)\n        print(scaled_data.value)\n        # Output will be standardized, with mean 0 and unit variance\n        # For example: [[ 1.41421356  1.41421356]\n        #               [-1.41421356 -1.41421356]]\n        ```\n\n    Attributes:\n        _scaler (StandardScaler): The underlying scikit-learn StandardScaler object used for standardization.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the StandardScaler to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Perform standardization on the input data.\n\n    Note:\n        This plugin uses scikit-learn's implementation of StandardScaler, which may have its own\n        dependencies and requirements. Ensure that scikit-learn is properly installed and compatible\n        with your environment.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize a new StandardScalerPlugin instance.\n\n        This constructor sets up the StandardScalerPlugin and initializes the underlying\n        scikit-learn StandardScaler object.\n\n        Note:\n            No parameters are required for initialization as StandardScaler uses default settings.\n            For customized scaling, consider extending this class and modifying the StandardScaler initialization.\n        \"\"\"\n        super().__init__()  # Call the BaseFilter constructor to initialize the plugin's parameters\n        self._scaler = StandardScaler()\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the StandardScaler to the given data.\n\n        This method computes the mean and standard deviation of the input features,\n        which will be used for subsequent scaling operations.\n\n        Args:\n            x (XYData): The input features to fit the StandardScaler.\n            y (Optional[XYData]): Not used in StandardScaler, but required by the BaseFilter interface.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: Always returns None as StandardScaler doesn't have a standard evaluation metric.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The y parameter is ignored as StandardScaler is an unsupervised method.\n        \"\"\"\n        self._scaler.fit(x.value)\n        return None  # StandardScaler doesn't use y for fitting\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Perform standardization on the input data.\n\n        This method applies the standardization transformation to new input data,\n        centering and scaling the features based on the computed mean and standard deviation.\n\n        Args:\n            x (XYData): The input features to standardize.\n\n        Returns:\n            XYData: The standardized version of the input data, wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's transform method internally.\n            The transformed data is wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._scaler.transform(x.value))\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.StandardScalerPlugin.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a new StandardScalerPlugin instance.</p> <p>This constructor sets up the StandardScalerPlugin and initializes the underlying scikit-learn StandardScaler object.</p> Note <p>No parameters are required for initialization as StandardScaler uses default settings. For customized scaling, consider extending this class and modifying the StandardScaler initialization.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize a new StandardScalerPlugin instance.\n\n    This constructor sets up the StandardScalerPlugin and initializes the underlying\n    scikit-learn StandardScaler object.\n\n    Note:\n        No parameters are required for initialization as StandardScaler uses default settings.\n        For customized scaling, consider extending this class and modifying the StandardScaler initialization.\n    \"\"\"\n    super().__init__()  # Call the BaseFilter constructor to initialize the plugin's parameters\n    self._scaler = StandardScaler()\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.StandardScalerPlugin.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the StandardScaler to the given data.</p> <p>This method computes the mean and standard deviation of the input features, which will be used for subsequent scaling operations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to fit the StandardScaler.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Not used in StandardScaler, but required by the BaseFilter interface.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: Always returns None as StandardScaler doesn't have a standard evaluation metric.</p> Note <p>This method uses scikit-learn's fit method internally. The y parameter is ignored as StandardScaler is an unsupervised method.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the StandardScaler to the given data.\n\n    This method computes the mean and standard deviation of the input features,\n    which will be used for subsequent scaling operations.\n\n    Args:\n        x (XYData): The input features to fit the StandardScaler.\n        y (Optional[XYData]): Not used in StandardScaler, but required by the BaseFilter interface.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: Always returns None as StandardScaler doesn't have a standard evaluation metric.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The y parameter is ignored as StandardScaler is an unsupervised method.\n    \"\"\"\n    self._scaler.fit(x.value)\n    return None  # StandardScaler doesn't use y for fitting\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.StandardScalerPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Perform standardization on the input data.</p> <p>This method applies the standardization transformation to new input data, centering and scaling the features based on the computed mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to standardize.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The standardized version of the input data, wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's transform method internally. The transformed data is wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Perform standardization on the input data.\n\n    This method applies the standardization transformation to new input data,\n    centering and scaling the features based on the computed mean and standard deviation.\n\n    Args:\n        x (XYData): The input features to standardize.\n\n    Returns:\n        XYData: The standardized version of the input data, wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's transform method internally.\n        The transformed data is wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._scaler.transform(x.value))\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.pca","title":"<code>pca</code>","text":""},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.pca.__all__","title":"<code>__all__ = ['PCAPlugin']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.pca.PCAPlugin","title":"<code>PCAPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A plugin for performing Principal Component Analysis (PCA) on input data.</p> <p>This plugin integrates scikit-learn's PCA implementation into the framework3 ecosystem, allowing for easy dimensionality reduction within pipelines.</p> Key Features <ul> <li>Utilizes scikit-learn's PCA for dimensionality reduction</li> <li>Supports customization of the number of components to keep</li> <li>Provides methods for fitting the PCA model and transforming data</li> <li>Integrates seamlessly with framework3's BaseFilter interface</li> <li>Includes a static method for generating parameter grids for hyperparameter tuning</li> </ul> Usage <p>The PCAPlugin can be used to perform dimensionality reduction on your data:</p> <pre><code>from framework3.plugins.filters.transformation.pca import PCAPlugin\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create a PCAPlugin instance\npca_plugin = PCAPlugin(n_components=2)\n\n# Create some sample data\nX = XYData.mock(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\ny = None  # PCA doesn't use y for fitting\n\n# Fit the PCA model\npca_plugin.fit(X, y)\n\n# Transform new data\nnew_data = XYData.mock(np.array([[2, 3, 4], [5, 6, 7]]))\ntransformed_data = pca_plugin.predict(new_data)\nprint(transformed_data.value)  # This will be a 2x2 array\n</code></pre> <p>Attributes:</p> Name Type Description <code>_pca</code> <code>PCA</code> <p>The underlying scikit-learn PCA object used for dimensionality reduction.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the PCA model to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Apply dimensionality reduction to the input data.</p> <code>item_grid</code> <p>List[int]) -&gt; Dict[str, Any]: Generate a parameter grid for hyperparameter tuning.</p> Note <p>This plugin uses scikit-learn's implementation of PCA, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>@Container.bind()\nclass PCAPlugin(BaseFilter):\n    \"\"\"\n    A plugin for performing Principal Component Analysis (PCA) on input data.\n\n    This plugin integrates scikit-learn's PCA implementation into the framework3 ecosystem,\n    allowing for easy dimensionality reduction within pipelines.\n\n    Key Features:\n        - Utilizes scikit-learn's PCA for dimensionality reduction\n        - Supports customization of the number of components to keep\n        - Provides methods for fitting the PCA model and transforming data\n        - Integrates seamlessly with framework3's BaseFilter interface\n        - Includes a static method for generating parameter grids for hyperparameter tuning\n\n    Usage:\n        The PCAPlugin can be used to perform dimensionality reduction on your data:\n\n        ```python\n        from framework3.plugins.filters.transformation.pca import PCAPlugin\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create a PCAPlugin instance\n        pca_plugin = PCAPlugin(n_components=2)\n\n        # Create some sample data\n        X = XYData.mock(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n        y = None  # PCA doesn't use y for fitting\n\n        # Fit the PCA model\n        pca_plugin.fit(X, y)\n\n        # Transform new data\n        new_data = XYData.mock(np.array([[2, 3, 4], [5, 6, 7]]))\n        transformed_data = pca_plugin.predict(new_data)\n        print(transformed_data.value)  # This will be a 2x2 array\n        ```\n\n    Attributes:\n        _pca (PCA): The underlying scikit-learn PCA object used for dimensionality reduction.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the PCA model to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Apply dimensionality reduction to the input data.\n        item_grid(n_components: List[int]) -&gt; Dict[str, Any]:\n            Generate a parameter grid for hyperparameter tuning.\n\n    Note:\n        This plugin uses scikit-learn's implementation of PCA, which may have its own\n        dependencies and requirements. Ensure that scikit-learn is properly installed\n        and compatible with your environment.\n    \"\"\"\n\n    def __init__(self, n_components: int = 2):\n        \"\"\"\n        Initialize a new PCAPlugin instance.\n\n        This constructor sets up the PCAPlugin with the specified number of components\n        and initializes the underlying scikit-learn PCA object.\n\n        Args:\n            n_components (int): The number of components to keep after dimensionality reduction.\n                                Defaults to 2.\n\n        Note:\n            The n_components parameter is passed directly to scikit-learn's PCA.\n            Refer to scikit-learn's documentation for detailed information on this parameter.\n        \"\"\"\n        super().__init__(\n            n_components=n_components\n        )  # Initialize the BaseFilter and BasePlugin parent classes.\n        self._pca = PCA(n_components=n_components)\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the PCA model to the given data.\n\n        This method trains the PCA model on the provided input features.\n\n        Args:\n            x (XYData): The input features to fit the PCA model.\n            y (Optional[XYData]): Not used in PCA, but required by the BaseFilter interface.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: Always returns None as PCA doesn't have a standard evaluation metric.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The y parameter is ignored as PCA is an unsupervised method.\n        \"\"\"\n        self._pca.fit(x.value)\n        return None\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Apply dimensionality reduction to the input data.\n\n        This method uses the trained PCA model to transform new input data,\n        reducing its dimensionality.\n\n        Args:\n            x (XYData): The input features to transform.\n\n        Returns:\n            XYData: The transformed data with reduced dimensionality, wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's transform method internally.\n            The transformed data is wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._pca.transform(x.value))\n\n    @staticmethod\n    def item_grid(n_components: List[int]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate a parameter grid for hyperparameter tuning.\n\n        This static method creates a dictionary that can be used for grid search\n        over different numbers of components in PCA.\n\n        Args:\n            n_components (List[int]): A list of integers representing different numbers\n                                      of components to try in the grid search.\n\n        Returns:\n            Dict[str, Any]: A dictionary with the parameter name as key and the list of\n                            values to try as value.\n\n        Note:\n            This method is typically used in conjunction with hyperparameter tuning\n            techniques like GridSearchCV.\n        \"\"\"\n        return {\"PCAPlugin__n_components\": n_components}\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.pca.PCAPlugin.__init__","title":"<code>__init__(n_components=2)</code>","text":"<p>Initialize a new PCAPlugin instance.</p> <p>This constructor sets up the PCAPlugin with the specified number of components and initializes the underlying scikit-learn PCA object.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>The number of components to keep after dimensionality reduction.                 Defaults to 2.</p> <code>2</code> Note <p>The n_components parameter is passed directly to scikit-learn's PCA. Refer to scikit-learn's documentation for detailed information on this parameter.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>def __init__(self, n_components: int = 2):\n    \"\"\"\n    Initialize a new PCAPlugin instance.\n\n    This constructor sets up the PCAPlugin with the specified number of components\n    and initializes the underlying scikit-learn PCA object.\n\n    Args:\n        n_components (int): The number of components to keep after dimensionality reduction.\n                            Defaults to 2.\n\n    Note:\n        The n_components parameter is passed directly to scikit-learn's PCA.\n        Refer to scikit-learn's documentation for detailed information on this parameter.\n    \"\"\"\n    super().__init__(\n        n_components=n_components\n    )  # Initialize the BaseFilter and BasePlugin parent classes.\n    self._pca = PCA(n_components=n_components)\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.pca.PCAPlugin.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the PCA model to the given data.</p> <p>This method trains the PCA model on the provided input features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to fit the PCA model.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Not used in PCA, but required by the BaseFilter interface.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: Always returns None as PCA doesn't have a standard evaluation metric.</p> Note <p>This method uses scikit-learn's fit method internally. The y parameter is ignored as PCA is an unsupervised method.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the PCA model to the given data.\n\n    This method trains the PCA model on the provided input features.\n\n    Args:\n        x (XYData): The input features to fit the PCA model.\n        y (Optional[XYData]): Not used in PCA, but required by the BaseFilter interface.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: Always returns None as PCA doesn't have a standard evaluation metric.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The y parameter is ignored as PCA is an unsupervised method.\n    \"\"\"\n    self._pca.fit(x.value)\n    return None\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.pca.PCAPlugin.item_grid","title":"<code>item_grid(n_components)</code>  <code>staticmethod</code>","text":"<p>Generate a parameter grid for hyperparameter tuning.</p> <p>This static method creates a dictionary that can be used for grid search over different numbers of components in PCA.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>List[int]</code> <p>A list of integers representing different numbers                       of components to try in the grid search.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary with the parameter name as key and the list of             values to try as value.</p> Note <p>This method is typically used in conjunction with hyperparameter tuning techniques like GridSearchCV.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>@staticmethod\ndef item_grid(n_components: List[int]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a parameter grid for hyperparameter tuning.\n\n    This static method creates a dictionary that can be used for grid search\n    over different numbers of components in PCA.\n\n    Args:\n        n_components (List[int]): A list of integers representing different numbers\n                                  of components to try in the grid search.\n\n    Returns:\n        Dict[str, Any]: A dictionary with the parameter name as key and the list of\n                        values to try as value.\n\n    Note:\n        This method is typically used in conjunction with hyperparameter tuning\n        techniques like GridSearchCV.\n    \"\"\"\n    return {\"PCAPlugin__n_components\": n_components}\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.pca.PCAPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Apply dimensionality reduction to the input data.</p> <p>This method uses the trained PCA model to transform new input data, reducing its dimensionality.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to transform.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The transformed data with reduced dimensionality, wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's transform method internally. The transformed data is wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/transformation/pca.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Apply dimensionality reduction to the input data.\n\n    This method uses the trained PCA model to transform new input data,\n    reducing its dimensionality.\n\n    Args:\n        x (XYData): The input features to transform.\n\n    Returns:\n        XYData: The transformed data with reduced dimensionality, wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's transform method internally.\n        The transformed data is wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._pca.transform(x.value))\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.scaler","title":"<code>scaler</code>","text":""},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.scaler.__all__","title":"<code>__all__ = ['StandardScalerPlugin']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.scaler.StandardScalerPlugin","title":"<code>StandardScalerPlugin</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A plugin for standardizing features by removing the mean and scaling to unit variance.</p> <p>This plugin integrates scikit-learn's StandardScaler into the framework3 ecosystem, allowing for easy feature standardization within pipelines.</p> Key Features <ul> <li>Utilizes scikit-learn's StandardScaler for feature standardization</li> <li>Removes the mean and scales features to unit variance</li> <li>Provides methods for fitting the scaler and transforming data</li> <li>Integrates seamlessly with framework3's BaseFilter interface</li> </ul> Usage <p>The StandardScalerPlugin can be used to standardize features in your data:</p> <pre><code>from framework3.plugins.filters.transformation.scaler import StandardScalerPlugin\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create a StandardScalerPlugin instance\nscaler_plugin = StandardScalerPlugin()\n\n# Create some sample data\nX = XYData.mock(np.array([[0, 0], [0, 0], [1, 1], [1, 1]]))\ny = None  # StandardScaler doesn't use y for fitting\n\n# Fit the StandardScaler\nscaler_plugin.fit(X, y)\n\n# Transform new data\nnew_data = XYData.mock(np.array([[2, 2], [-1, -1]]))\nscaled_data = scaler_plugin.predict(new_data)\nprint(scaled_data.value)\n# Output will be standardized, with mean 0 and unit variance\n# For example: [[ 1.41421356  1.41421356]\n#               [-1.41421356 -1.41421356]]\n</code></pre> <p>Attributes:</p> Name Type Description <code>_scaler</code> <code>StandardScaler</code> <p>The underlying scikit-learn StandardScaler object used for standardization.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit the StandardScaler to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Perform standardization on the input data.</p> Note <p>This plugin uses scikit-learn's implementation of StandardScaler, which may have its own dependencies and requirements. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>@Container.bind()\nclass StandardScalerPlugin(BaseFilter):\n    \"\"\"\n    A plugin for standardizing features by removing the mean and scaling to unit variance.\n\n    This plugin integrates scikit-learn's StandardScaler into the framework3 ecosystem,\n    allowing for easy feature standardization within pipelines.\n\n    Key Features:\n        - Utilizes scikit-learn's StandardScaler for feature standardization\n        - Removes the mean and scales features to unit variance\n        - Provides methods for fitting the scaler and transforming data\n        - Integrates seamlessly with framework3's BaseFilter interface\n\n    Usage:\n        The StandardScalerPlugin can be used to standardize features in your data:\n\n        ```python\n        from framework3.plugins.filters.transformation.scaler import StandardScalerPlugin\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create a StandardScalerPlugin instance\n        scaler_plugin = StandardScalerPlugin()\n\n        # Create some sample data\n        X = XYData.mock(np.array([[0, 0], [0, 0], [1, 1], [1, 1]]))\n        y = None  # StandardScaler doesn't use y for fitting\n\n        # Fit the StandardScaler\n        scaler_plugin.fit(X, y)\n\n        # Transform new data\n        new_data = XYData.mock(np.array([[2, 2], [-1, -1]]))\n        scaled_data = scaler_plugin.predict(new_data)\n        print(scaled_data.value)\n        # Output will be standardized, with mean 0 and unit variance\n        # For example: [[ 1.41421356  1.41421356]\n        #               [-1.41421356 -1.41421356]]\n        ```\n\n    Attributes:\n        _scaler (StandardScaler): The underlying scikit-learn StandardScaler object used for standardization.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit the StandardScaler to the given data.\n        predict(x: XYData) -&gt; XYData:\n            Perform standardization on the input data.\n\n    Note:\n        This plugin uses scikit-learn's implementation of StandardScaler, which may have its own\n        dependencies and requirements. Ensure that scikit-learn is properly installed and compatible\n        with your environment.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize a new StandardScalerPlugin instance.\n\n        This constructor sets up the StandardScalerPlugin and initializes the underlying\n        scikit-learn StandardScaler object.\n\n        Note:\n            No parameters are required for initialization as StandardScaler uses default settings.\n            For customized scaling, consider extending this class and modifying the StandardScaler initialization.\n        \"\"\"\n        super().__init__()  # Call the BaseFilter constructor to initialize the plugin's parameters\n        self._scaler = StandardScaler()\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit the StandardScaler to the given data.\n\n        This method computes the mean and standard deviation of the input features,\n        which will be used for subsequent scaling operations.\n\n        Args:\n            x (XYData): The input features to fit the StandardScaler.\n            y (Optional[XYData]): Not used in StandardScaler, but required by the BaseFilter interface.\n            evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n        Returns:\n            Optional[float]: Always returns None as StandardScaler doesn't have a standard evaluation metric.\n\n        Note:\n            This method uses scikit-learn's fit method internally.\n            The y parameter is ignored as StandardScaler is an unsupervised method.\n        \"\"\"\n        self._scaler.fit(x.value)\n        return None  # StandardScaler doesn't use y for fitting\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Perform standardization on the input data.\n\n        This method applies the standardization transformation to new input data,\n        centering and scaling the features based on the computed mean and standard deviation.\n\n        Args:\n            x (XYData): The input features to standardize.\n\n        Returns:\n            XYData: The standardized version of the input data, wrapped in an XYData object.\n\n        Note:\n            This method uses scikit-learn's transform method internally.\n            The transformed data is wrapped in an XYData object for consistency with the framework.\n        \"\"\"\n        return XYData.mock(self._scaler.transform(x.value))\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.scaler.StandardScalerPlugin.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a new StandardScalerPlugin instance.</p> <p>This constructor sets up the StandardScalerPlugin and initializes the underlying scikit-learn StandardScaler object.</p> Note <p>No parameters are required for initialization as StandardScaler uses default settings. For customized scaling, consider extending this class and modifying the StandardScaler initialization.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize a new StandardScalerPlugin instance.\n\n    This constructor sets up the StandardScalerPlugin and initializes the underlying\n    scikit-learn StandardScaler object.\n\n    Note:\n        No parameters are required for initialization as StandardScaler uses default settings.\n        For customized scaling, consider extending this class and modifying the StandardScaler initialization.\n    \"\"\"\n    super().__init__()  # Call the BaseFilter constructor to initialize the plugin's parameters\n    self._scaler = StandardScaler()\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.scaler.StandardScalerPlugin.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit the StandardScaler to the given data.</p> <p>This method computes the mean and standard deviation of the input features, which will be used for subsequent scaling operations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to fit the StandardScaler.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Not used in StandardScaler, but required by the BaseFilter interface.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An optional evaluator for the model. Not used in this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: Always returns None as StandardScaler doesn't have a standard evaluation metric.</p> Note <p>This method uses scikit-learn's fit method internally. The y parameter is ignored as StandardScaler is an unsupervised method.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit the StandardScaler to the given data.\n\n    This method computes the mean and standard deviation of the input features,\n    which will be used for subsequent scaling operations.\n\n    Args:\n        x (XYData): The input features to fit the StandardScaler.\n        y (Optional[XYData]): Not used in StandardScaler, but required by the BaseFilter interface.\n        evaluator (BaseMetric | None): An optional evaluator for the model. Not used in this method.\n\n    Returns:\n        Optional[float]: Always returns None as StandardScaler doesn't have a standard evaluation metric.\n\n    Note:\n        This method uses scikit-learn's fit method internally.\n        The y parameter is ignored as StandardScaler is an unsupervised method.\n    \"\"\"\n    self._scaler.fit(x.value)\n    return None  # StandardScaler doesn't use y for fitting\n</code></pre>"},{"location":"api/plugins/filters/transformation/#framework3.plugins.filters.transformation.scaler.StandardScalerPlugin.predict","title":"<code>predict(x)</code>","text":"<p>Perform standardization on the input data.</p> <p>This method applies the standardization transformation to new input data, centering and scaling the features based on the computed mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features to standardize.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The standardized version of the input data, wrapped in an XYData object.</p> Note <p>This method uses scikit-learn's transform method internally. The transformed data is wrapped in an XYData object for consistency with the framework.</p> Source code in <code>framework3/plugins/filters/transformation/scaler.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Perform standardization on the input data.\n\n    This method applies the standardization transformation to new input data,\n    centering and scaling the features based on the computed mean and standard deviation.\n\n    Args:\n        x (XYData): The input features to standardize.\n\n    Returns:\n        XYData: The standardized version of the input data, wrapped in an XYData object.\n\n    Note:\n        This method uses scikit-learn's transform method internally.\n        The transformed data is wrapped in an XYData object for consistency with the framework.\n    \"\"\"\n    return XYData.mock(self._scaler.transform(x.value))\n</code></pre>"},{"location":"api/plugins/metrics/classification/","title":"Classification Metrics","text":""},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.F1","title":"<code>F1</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>F1 score metric for classification tasks.</p> <p>This class calculates the F1 score, which is the harmonic mean of precision and recall. It's particularly useful when you need a balance between precision and recall.</p> Key Features <ul> <li>Calculates F1 score for binary and multiclass classification</li> <li>Supports different averaging methods (micro, macro, weighted, etc.)</li> <li>Integrates with framework3's BaseMetric interface</li> </ul> Usage <p>The F1 metric can be used to evaluate classification models:</p> <pre><code>from framework3.plugins.metrics.classification import F1\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\ny_true = XYData(value=np.array([0, 1, 2, 0, 1, 2]))\ny_pred = XYData(value=np.array([0, 2, 1, 0, 0, 1]))\nx_data = XYData(value=np.array([1, 2, 3, 4, 5, 6]))\n\n# Create and use the F1 metric\nf1_metric = F1(average='macro')\nscore = f1_metric.evaluate(x_data, y_true, y_pred)\nprint(f\"F1 Score: {score}\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>average</code> <code>str</code> <p>The type of averaging performed on the data. Default is 'weighted'.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray: Calculate the F1 score for the given predictions and true values.</p> Note <p>This metric uses scikit-learn's f1_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>@Container.bind()\nclass F1(BaseMetric):\n    \"\"\"\n    F1 score metric for classification tasks.\n\n    This class calculates the F1 score, which is the harmonic mean of precision and recall.\n    It's particularly useful when you need a balance between precision and recall.\n\n    Key Features:\n        - Calculates F1 score for binary and multiclass classification\n        - Supports different averaging methods (micro, macro, weighted, etc.)\n        - Integrates with framework3's BaseMetric interface\n\n    Usage:\n        The F1 metric can be used to evaluate classification models:\n\n        ```python\n        from framework3.plugins.metrics.classification import F1\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        y_true = XYData(value=np.array([0, 1, 2, 0, 1, 2]))\n        y_pred = XYData(value=np.array([0, 2, 1, 0, 0, 1]))\n        x_data = XYData(value=np.array([1, 2, 3, 4, 5, 6]))\n\n        # Create and use the F1 metric\n        f1_metric = F1(average='macro')\n        score = f1_metric.evaluate(x_data, y_true, y_pred)\n        print(f\"F1 Score: {score}\")\n        ```\n\n    Attributes:\n        average (str): The type of averaging performed on the data. Default is 'weighted'.\n\n    Methods:\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the F1 score for the given predictions and true values.\n\n    Note:\n        This metric uses scikit-learn's f1_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        average: Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"] = \"binary\",\n    ):\n        \"\"\"\n        Initialize a new F1 metric instance.\n\n        This constructor sets up the F1 metric with the specified averaging method.\n\n        Args:\n            average (Literal['micro', 'macro', 'samples', 'weighted', 'binary']): The type of averaging performed on the data. Default is 'weighted'.\n                           Other options include 'micro', 'macro', 'samples', 'binary', or None.\n\n        Note:\n            The 'average' parameter is passed directly to scikit-learn's f1_score function.\n            Refer to scikit-learn's documentation for detailed information on averaging methods.\n        \"\"\"\n        super().__init__(average=average)\n        self.average = average\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: XYData | None,\n        y_pred: XYData,\n        **kwargs: Unpack[PrecissionKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the F1 score for the given predictions and true values.\n\n        This method computes the F1 score, which is the harmonic mean of precision and recall.\n\n        Args:\n            x_data (XYData): The input data (not used in this metric, but required by the interface).\n            y_true (XYData | None): The ground truth (correct) target values.\n            y_pred (XYData): The estimated targets as returned by a classifier.\n            **kwargs (Unpack[PrecissionKwargs]): Additional keyword arguments passed to sklearn's f1_score function.\n\n        Returns:\n            Float | np.ndarray: The F1 score or array of F1 scores if average is None.\n\n        Raises:\n            ValueError: If y_true is None.\n\n        Note:\n            This method uses scikit-learn's f1_score function internally with zero_division=0.\n        \"\"\"\n        if y_true is None:\n            raise ValueError(\"Ground truth (y_true) must be provided.\")\n\n        return f1_score(\n            y_true.value,\n            y_pred.value,\n            average=self.average,\n            **kwargs,  # type: ignore\n        )  # type: ignore\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.F1.__init__","title":"<code>__init__(average='binary')</code>","text":"<p>Initialize a new F1 metric instance.</p> <p>This constructor sets up the F1 metric with the specified averaging method.</p> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>Literal['micro', 'macro', 'samples', 'weighted', 'binary']</code> <p>The type of averaging performed on the data. Default is 'weighted'.            Other options include 'micro', 'macro', 'samples', 'binary', or None.</p> <code>'binary'</code> Note <p>The 'average' parameter is passed directly to scikit-learn's f1_score function. Refer to scikit-learn's documentation for detailed information on averaging methods.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>def __init__(\n    self,\n    average: Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"] = \"binary\",\n):\n    \"\"\"\n    Initialize a new F1 metric instance.\n\n    This constructor sets up the F1 metric with the specified averaging method.\n\n    Args:\n        average (Literal['micro', 'macro', 'samples', 'weighted', 'binary']): The type of averaging performed on the data. Default is 'weighted'.\n                       Other options include 'micro', 'macro', 'samples', 'binary', or None.\n\n    Note:\n        The 'average' parameter is passed directly to scikit-learn's f1_score function.\n        Refer to scikit-learn's documentation for detailed information on averaging methods.\n    \"\"\"\n    super().__init__(average=average)\n    self.average = average\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.F1.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the F1 score for the given predictions and true values.</p> <p>This method computes the F1 score, which is the harmonic mean of precision and recall.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data (not used in this metric, but required by the interface).</p> required <code>y_true</code> <code>XYData | None</code> <p>The ground truth (correct) target values.</p> required <code>y_pred</code> <code>XYData</code> <p>The estimated targets as returned by a classifier.</p> required <code>**kwargs</code> <code>Unpack[PrecissionKwargs]</code> <p>Additional keyword arguments passed to sklearn's f1_score function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The F1 score or array of F1 scores if average is None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y_true is None.</p> Note <p>This method uses scikit-learn's f1_score function internally with zero_division=0.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: XYData | None,\n    y_pred: XYData,\n    **kwargs: Unpack[PrecissionKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the F1 score for the given predictions and true values.\n\n    This method computes the F1 score, which is the harmonic mean of precision and recall.\n\n    Args:\n        x_data (XYData): The input data (not used in this metric, but required by the interface).\n        y_true (XYData | None): The ground truth (correct) target values.\n        y_pred (XYData): The estimated targets as returned by a classifier.\n        **kwargs (Unpack[PrecissionKwargs]): Additional keyword arguments passed to sklearn's f1_score function.\n\n    Returns:\n        Float | np.ndarray: The F1 score or array of F1 scores if average is None.\n\n    Raises:\n        ValueError: If y_true is None.\n\n    Note:\n        This method uses scikit-learn's f1_score function internally with zero_division=0.\n    \"\"\"\n    if y_true is None:\n        raise ValueError(\"Ground truth (y_true) must be provided.\")\n\n    return f1_score(\n        y_true.value,\n        y_pred.value,\n        average=self.average,\n        **kwargs,  # type: ignore\n    )  # type: ignore\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.Precission","title":"<code>Precission</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Precision metric for classification tasks.</p> <p>This class calculates the precision score, which is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives.</p> Key Features <ul> <li>Calculates precision score for binary and multiclass classification</li> <li>Supports different averaging methods (micro, macro, weighted, etc.)</li> <li>Integrates with framework3's BaseMetric interface</li> </ul> Usage <p>The Precission metric can be used to evaluate classification models:</p> <pre><code>from framework3.plugins.metrics.classification import Precission\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\ny_true = XYData(value=np.array([0, 1, 2, 0, 1, 2]))\ny_pred = XYData(value=np.array([0, 2, 1, 0, 0, 1]))\nx_data = XYData(value=np.array([1, 2, 3, 4, 5, 6]))\n\n# Create and use the Precission metric\nprecision_metric = Precission(average='macro')\nscore = precision_metric.evaluate(x_data, y_true, y_pred)\nprint(f\"Precision Score: {score}\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>average</code> <code>Literal['micro', 'macro', 'samples', 'weighted', 'binary'] | None</code> <p>The type of averaging performed on the data. Default is 'weighted'.</p> <p>Methods:</p> Name Description <code>evaluate </code> <p>XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray: Calculate the precision score for the given predictions and true values.</p> Note <p>This metric uses scikit-learn's precision_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>@Container.bind()\nclass Precission(BaseMetric):\n    \"\"\"\n    Precision metric for classification tasks.\n\n    This class calculates the precision score, which is the ratio tp / (tp + fp) where tp is\n    the number of true positives and fp the number of false positives.\n\n    Key Features:\n        - Calculates precision score for binary and multiclass classification\n        - Supports different averaging methods (micro, macro, weighted, etc.)\n        - Integrates with framework3's BaseMetric interface\n\n    Usage:\n        The Precission metric can be used to evaluate classification models:\n\n        ```python\n        from framework3.plugins.metrics.classification import Precission\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        y_true = XYData(value=np.array([0, 1, 2, 0, 1, 2]))\n        y_pred = XYData(value=np.array([0, 2, 1, 0, 0, 1]))\n        x_data = XYData(value=np.array([1, 2, 3, 4, 5, 6]))\n\n        # Create and use the Precission metric\n        precision_metric = Precission(average='macro')\n        score = precision_metric.evaluate(x_data, y_true, y_pred)\n        print(f\"Precision Score: {score}\")\n        ```\n\n    Attributes:\n        average (Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"]|None): The type of averaging performed on the data. Default is 'weighted'.\n\n    Methods:\n        evaluate (x_data: XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the precision score for the given predictions and true values.\n\n    Note:\n        This metric uses scikit-learn's precision_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        average: Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"]\n        | None = \"binary\",\n    ):\n        \"\"\"\n        Initialize a new Precission metric instance.\n\n        This constructor sets up the Precission metric with the specified averaging method.\n\n        Args:\n            average (Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"]|None): The type of averaging performed on the data. Default is 'weighted'.\n                                  Options are 'micro', 'macro', 'samples', 'weighted', 'binary', or None.\n\n        Note:\n            The 'average' parameter is passed directly to scikit-learn's precision_score function.\n            Refer to scikit-learn's documentation for detailed information on averaging methods.\n        \"\"\"\n        super().__init__(average=average)\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: XYData | None,\n        y_pred: XYData,\n        **kwargs: Unpack[PrecissionKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the precision score for the given predictions and true values.\n\n        This method computes the precision score, which is the ratio of true positives to the\n        sum of true and false positives.\n\n        Args:\n            x_data (XYData): The input data (not used in this metric, but required by the interface).\n            y_true (XYData | None): The ground truth (correct) target values.\n            y_pred (XYData): The estimated targets as returned by a classifier.\n            **kwargs (Unpack[PrecissionKwargs]): Additional keyword arguments passed to sklearn's precision_score function.\n\n        Returns:\n            Float | np.ndarray: The precision score or array of precision scores if average is None.\n\n        Raises:\n            ValueError: If y_true is None.\n\n        Note:\n            This method uses scikit-learn's precision_score function internally with zero_division=0.\n        \"\"\"\n        if y_true is None:\n            raise ValueError(\"Ground truth (y_true) must be provided.\")\n        return precision_score(\n            y_true.value,\n            y_pred.value,\n            average=self.average,\n            **kwargs,  # type: ignore\n        )  # type: ignore\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.Precission.__init__","title":"<code>__init__(average='binary')</code>","text":"<p>Initialize a new Precission metric instance.</p> <p>This constructor sets up the Precission metric with the specified averaging method.</p> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>Literal['micro', 'macro', 'samples', 'weighted', 'binary'] | None</code> <p>The type of averaging performed on the data. Default is 'weighted'.                   Options are 'micro', 'macro', 'samples', 'weighted', 'binary', or None.</p> <code>'binary'</code> Note <p>The 'average' parameter is passed directly to scikit-learn's precision_score function. Refer to scikit-learn's documentation for detailed information on averaging methods.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>def __init__(\n    self,\n    average: Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"]\n    | None = \"binary\",\n):\n    \"\"\"\n    Initialize a new Precission metric instance.\n\n    This constructor sets up the Precission metric with the specified averaging method.\n\n    Args:\n        average (Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"]|None): The type of averaging performed on the data. Default is 'weighted'.\n                              Options are 'micro', 'macro', 'samples', 'weighted', 'binary', or None.\n\n    Note:\n        The 'average' parameter is passed directly to scikit-learn's precision_score function.\n        Refer to scikit-learn's documentation for detailed information on averaging methods.\n    \"\"\"\n    super().__init__(average=average)\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.Precission.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the precision score for the given predictions and true values.</p> <p>This method computes the precision score, which is the ratio of true positives to the sum of true and false positives.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data (not used in this metric, but required by the interface).</p> required <code>y_true</code> <code>XYData | None</code> <p>The ground truth (correct) target values.</p> required <code>y_pred</code> <code>XYData</code> <p>The estimated targets as returned by a classifier.</p> required <code>**kwargs</code> <code>Unpack[PrecissionKwargs]</code> <p>Additional keyword arguments passed to sklearn's precision_score function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The precision score or array of precision scores if average is None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y_true is None.</p> Note <p>This method uses scikit-learn's precision_score function internally with zero_division=0.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: XYData | None,\n    y_pred: XYData,\n    **kwargs: Unpack[PrecissionKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the precision score for the given predictions and true values.\n\n    This method computes the precision score, which is the ratio of true positives to the\n    sum of true and false positives.\n\n    Args:\n        x_data (XYData): The input data (not used in this metric, but required by the interface).\n        y_true (XYData | None): The ground truth (correct) target values.\n        y_pred (XYData): The estimated targets as returned by a classifier.\n        **kwargs (Unpack[PrecissionKwargs]): Additional keyword arguments passed to sklearn's precision_score function.\n\n    Returns:\n        Float | np.ndarray: The precision score or array of precision scores if average is None.\n\n    Raises:\n        ValueError: If y_true is None.\n\n    Note:\n        This method uses scikit-learn's precision_score function internally with zero_division=0.\n    \"\"\"\n    if y_true is None:\n        raise ValueError(\"Ground truth (y_true) must be provided.\")\n    return precision_score(\n        y_true.value,\n        y_pred.value,\n        average=self.average,\n        **kwargs,  # type: ignore\n    )  # type: ignore\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.Recall","title":"<code>Recall</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Recall metric for classification tasks.</p> <p>This class calculates the recall score, which is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives.</p> Key Features <ul> <li>Calculates recall score for binary and multiclass classification</li> <li>Supports different averaging methods (micro, macro, weighted, etc.)</li> <li>Integrates with framework3's BaseMetric interface</li> </ul> Usage <p>The Recall metric can be used to evaluate classification models:</p> <pre><code>from framework3.plugins.metrics.classification import Recall\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\ny_true = XYData(value=np.array([0, 1, 2, 0, 1, 2]))\ny_pred = XYData(value=np.array([0, 2, 1, 0, 0, 1]))\nx_data = XYData(value=np.array([1, 2, 3, 4, 5, 6]))\n\n# Create and use the Recall metric\nrecall_metric = Recall(average='macro')\nscore = recall_metric.evaluate(x_data, y_true, y_pred)\nprint(f\"Recall Score: {score}\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>average</code> <code>str | None</code> <p>The type of averaging performed on the data. Default is 'weighted'.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray: Calculate the recall score for the given predictions and true values.</p> Note <p>This metric uses scikit-learn's recall_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>@Container.bind()\nclass Recall(BaseMetric):\n    \"\"\"\n    Recall metric for classification tasks.\n\n    This class calculates the recall score, which is the ratio tp / (tp + fn) where tp is\n    the number of true positives and fn the number of false negatives.\n\n    Key Features:\n        - Calculates recall score for binary and multiclass classification\n        - Supports different averaging methods (micro, macro, weighted, etc.)\n        - Integrates with framework3's BaseMetric interface\n\n    Usage:\n        The Recall metric can be used to evaluate classification models:\n\n        ```python\n        from framework3.plugins.metrics.classification import Recall\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        y_true = XYData(value=np.array([0, 1, 2, 0, 1, 2]))\n        y_pred = XYData(value=np.array([0, 2, 1, 0, 0, 1]))\n        x_data = XYData(value=np.array([1, 2, 3, 4, 5, 6]))\n\n        # Create and use the Recall metric\n        recall_metric = Recall(average='macro')\n        score = recall_metric.evaluate(x_data, y_true, y_pred)\n        print(f\"Recall Score: {score}\")\n        ```\n\n    Attributes:\n        average (str | None): The type of averaging performed on the data. Default is 'weighted'.\n\n    Methods:\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the recall score for the given predictions and true values.\n\n    Note:\n        This metric uses scikit-learn's recall_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        average: Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"]\n        | None = \"binary\",\n    ):\n        \"\"\"\n        Initialize a new Recall metric instance.\n\n        This constructor sets up the Recall metric with the specified averaging method.\n\n        Args:\n            average (str | None): The type of averaging performed on the data. Default is 'weighted'.\n                                  Options are 'micro', 'macro', 'samples', 'weighted', 'binary', or None.\n\n        Note:\n            The 'average' parameter is passed directly to scikit-learn's recall_score function.\n            Refer to scikit-learn's documentation for detailed information on averaging methods.\n        \"\"\"\n        super().__init__(average=average)\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: XYData | None,\n        y_pred: XYData,\n        **kwargs: Unpack[PrecissionKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the recall score for the given predictions and true values.\n\n        This method computes the recall score, which is the ratio of true positives to the\n        sum of true positives and false negatives.\n\n        Args:\n            x_data (XYData): The input data (not used in this metric, but required by the interface).\n            y_true (XYData | None): The ground truth (correct) target values.\n            y_pred (XYData): The estimated targets as returned by a classifier.\n            **kwargs (Unpack[PrecissionKwargs]): Additional keyword arguments passed to sklearn's recall_score function.\n\n        Returns:\n            Float | np.ndarray: The recall score or array of recall scores if average is None.\n\n        Raises:\n            ValueError: If y_true is None.\n\n        Note:\n            This method uses scikit-learn's recall_score function internally with zero_division=0.\n        \"\"\"\n        if y_true is None:\n            raise ValueError(\"Ground truth (y_true) must be provided.\")\n        return recall_score(\n            y_true.value,\n            y_pred.value,\n            average=self.average,\n            **kwargs,  # type: ignore\n        )  # type: ignore\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.Recall.__init__","title":"<code>__init__(average='binary')</code>","text":"<p>Initialize a new Recall metric instance.</p> <p>This constructor sets up the Recall metric with the specified averaging method.</p> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>str | None</code> <p>The type of averaging performed on the data. Default is 'weighted'.                   Options are 'micro', 'macro', 'samples', 'weighted', 'binary', or None.</p> <code>'binary'</code> Note <p>The 'average' parameter is passed directly to scikit-learn's recall_score function. Refer to scikit-learn's documentation for detailed information on averaging methods.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>def __init__(\n    self,\n    average: Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"]\n    | None = \"binary\",\n):\n    \"\"\"\n    Initialize a new Recall metric instance.\n\n    This constructor sets up the Recall metric with the specified averaging method.\n\n    Args:\n        average (str | None): The type of averaging performed on the data. Default is 'weighted'.\n                              Options are 'micro', 'macro', 'samples', 'weighted', 'binary', or None.\n\n    Note:\n        The 'average' parameter is passed directly to scikit-learn's recall_score function.\n        Refer to scikit-learn's documentation for detailed information on averaging methods.\n    \"\"\"\n    super().__init__(average=average)\n</code></pre>"},{"location":"api/plugins/metrics/classification/#framework3.plugins.metrics.classification.Recall.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the recall score for the given predictions and true values.</p> <p>This method computes the recall score, which is the ratio of true positives to the sum of true positives and false negatives.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data (not used in this metric, but required by the interface).</p> required <code>y_true</code> <code>XYData | None</code> <p>The ground truth (correct) target values.</p> required <code>y_pred</code> <code>XYData</code> <p>The estimated targets as returned by a classifier.</p> required <code>**kwargs</code> <code>Unpack[PrecissionKwargs]</code> <p>Additional keyword arguments passed to sklearn's recall_score function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The recall score or array of recall scores if average is None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y_true is None.</p> Note <p>This method uses scikit-learn's recall_score function internally with zero_division=0.</p> Source code in <code>framework3/plugins/metrics/classification.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: XYData | None,\n    y_pred: XYData,\n    **kwargs: Unpack[PrecissionKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the recall score for the given predictions and true values.\n\n    This method computes the recall score, which is the ratio of true positives to the\n    sum of true positives and false negatives.\n\n    Args:\n        x_data (XYData): The input data (not used in this metric, but required by the interface).\n        y_true (XYData | None): The ground truth (correct) target values.\n        y_pred (XYData): The estimated targets as returned by a classifier.\n        **kwargs (Unpack[PrecissionKwargs]): Additional keyword arguments passed to sklearn's recall_score function.\n\n    Returns:\n        Float | np.ndarray: The recall score or array of recall scores if average is None.\n\n    Raises:\n        ValueError: If y_true is None.\n\n    Note:\n        This method uses scikit-learn's recall_score function internally with zero_division=0.\n    \"\"\"\n    if y_true is None:\n        raise ValueError(\"Ground truth (y_true) must be provided.\")\n    return recall_score(\n        y_true.value,\n        y_pred.value,\n        average=self.average,\n        **kwargs,  # type: ignore\n    )  # type: ignore\n</code></pre>"},{"location":"api/plugins/metrics/classification/#overview","title":"Overview","text":"<p>The Classification Metrics module in LabChain provides a set of evaluation metrics specifically designed for assessing the performance of classification models. These metrics help in understanding various aspects of a classifier's performance, such as accuracy, precision, recall, and F1-score.</p>"},{"location":"api/plugins/metrics/classification/#available-classification-metrics","title":"Available Classification Metrics","text":""},{"location":"api/plugins/metrics/classification/#accuracy-score","title":"Accuracy Score","text":"<p>The Accuracy Score is implemented in the <code>AccuracyScoreMetric</code>. It computes the accuracy of a classification model by comparing the predicted labels with the true labels.</p>"},{"location":"api/plugins/metrics/classification/#usage","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.classification.accuracy_score import AccuracyScoreMetric\n\naccuracy_metric = AccuracyScoreMetric()\nscore = accuracy_metric.compute(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/classification/#precision-score","title":"Precision Score","text":"<p>The Precision Score is implemented in the <code>PrecisionScoreMetric</code>. It computes the precision of a classification model, which is the ratio of true positive predictions to the total number of positive predictions.</p>"},{"location":"api/plugins/metrics/classification/#usage_1","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.classification.precision_score import PrecisionScoreMetric\n\nprecision_metric = PrecisionScoreMetric(average='weighted')\nscore = precision_metric.compute(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/classification/#parameters","title":"Parameters","text":"<ul> <li><code>average</code> (str): The averaging method. Options include 'micro', 'macro', 'weighted', 'samples', and None.</li> </ul>"},{"location":"api/plugins/metrics/classification/#recall-score","title":"Recall Score","text":"<p>The Recall Score is implemented in the <code>RecallScoreMetric</code>. It computes the recall of a classification model, which is the ratio of true positive predictions to the total number of actual positive instances.</p>"},{"location":"api/plugins/metrics/classification/#usage_2","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.classification.recall_score import RecallScoreMetric\n\nrecall_metric = RecallScoreMetric(average='weighted')\nscore = recall_metric.compute(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/classification/#parameters_1","title":"Parameters","text":"<ul> <li><code>average</code> (str): The averaging method. Options include 'micro', 'macro', 'weighted', 'samples', and None.</li> </ul>"},{"location":"api/plugins/metrics/classification/#f1-score","title":"F1 Score","text":"<p>The F1 Score is implemented in the <code>F1ScoreMetric</code>. It computes the F1 score, which is the harmonic mean of precision and recall.</p>"},{"location":"api/plugins/metrics/classification/#usage_3","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.classification.f1_score import F1ScoreMetric\n\nf1_metric = F1ScoreMetric(average='weighted')\nscore = f1_metric.compute(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/classification/#parameters_2","title":"Parameters","text":"<ul> <li><code>average</code> (str): The averaging method. Options include 'micro', 'macro', 'weighted', 'samples', and None.</li> </ul>"},{"location":"api/plugins/metrics/classification/#comprehensive-example-evaluating-a-classification-model","title":"Comprehensive Example: Evaluating a Classification Model","text":"<p>In this example, we'll demonstrate how to use the Classification Metrics to evaluate the performance of a classification model.</p> <pre><code>from framework3.plugins.filters.classification.svm import ClassifierSVMPlugin\nfrom framework3.plugins.metrics.classification.accuracy_score import AccuracyScoreMetric\nfrom framework3.plugins.metrics.classification.precision_score import PrecisionScoreMetric\nfrom framework3.plugins.metrics.classification.recall_score import RecallScoreMetric\nfrom framework3.plugins.metrics.classification.f1_score import F1ScoreMetric\nfrom framework3.base.base_types import XYData\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create XYData objects\nX_train_data = XYData(_hash='X_train', _path='/tmp', _value=X_train)\ny_train_data = XYData(_hash='y_train', _path='/tmp', _value=y_train)\nX_test_data = XYData(_hash='X_test', _path='/tmp', _value=X_test)\ny_test_data = XYData(_hash='y_test', _path='/tmp', _value=y_test)\n\n# Create and train the classifier\nclassifier = ClassifierSVMPlugin(kernel='rbf')\nclassifier.fit(X_train_data, y_train_data)\n\n# Make predictions\npredictions = classifier.predict(X_test_data)\n\n# Initialize metrics\naccuracy_metric = AccuracyScoreMetric()\nprecision_metric = PrecisionScoreMetric(average='weighted')\nrecall_metric = RecallScoreMetric(average='weighted')\nf1_metric = F1ScoreMetric(average='weighted')\n\n# Compute metrics\naccuracy = accuracy_metric.compute(y_test_data, predictions)\nprecision = precision_metric.compute(y_test_data, predictions)\nrecall = recall_metric.compute(y_test_data, predictions)\nf1 = f1_metric.compute(y_test_data, predictions)\n\n# Print results\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 Score: {f1}\")\n</code></pre> <p>This example demonstrates how to:</p> <ol> <li>Load and prepare the Iris dataset</li> <li>Create XYData objects for use with LabChain</li> <li>Train an SVM classifier</li> <li>Make predictions on the test set</li> <li>Initialize and compute various classification metrics</li> <li>Print the evaluation results</li> </ol>"},{"location":"api/plugins/metrics/classification/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Multiple Metrics: Use multiple metrics to get a comprehensive view of your model's performance. Different metrics capture different aspects of classification performance.</p> </li> <li> <p>Class Imbalance: Be aware of class imbalance in your dataset. In such cases, accuracy alone might not be a good metric. Consider using precision, recall, and F1-score.</p> </li> <li> <p>Averaging Methods: When dealing with multi-class classification, pay attention to the averaging method used in metrics like precision, recall, and F1-score. 'Weighted' average is often a good choice for imbalanced datasets.</p> </li> <li> <p>Cross-Validation: Use cross-validation to get a more robust estimate of your model's performance, especially with smaller datasets.</p> </li> <li> <p>Confusion Matrix: Consider using a confusion matrix in addition to these metrics for a more detailed view of your model's performance across different classes.</p> </li> <li> <p>ROC AUC: For binary classification problems, consider using the ROC AUC score as an additional metric.</p> </li> <li> <p>Threshold Adjustment: Remember that metrics like precision and recall can be affected by adjusting the classification threshold. Consider exploring different thresholds if needed.</p> </li> <li> <p>Domain-Specific Metrics: Depending on your specific problem, you might need to implement custom metrics that are more relevant to your domain.</p> </li> </ol>"},{"location":"api/plugins/metrics/classification/#conclusion","title":"Conclusion","text":"<p>The Classification Metrics module in LabChain provides essential tools for evaluating the performance of classification models. By using these metrics in combination with other LabChain components, you can gain valuable insights into your model's strengths and weaknesses. The example demonstrates how easy it is to compute and interpret these metrics within the LabChain ecosystem, enabling you to make informed decisions about your classification models.</p>"},{"location":"api/plugins/metrics/clustering/","title":"Clustering Metrics","text":""},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.ARI","title":"<code>ARI</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Adjusted Rand Index (ARI) metric for clustering evaluation.</p> <p>This class calculates the ARI score, which is the corrected-for-chance version of the Rand Index. It measures similarity between two clusterings, adjusted for chance.</p> Key Features <ul> <li>Measures the similarity of the cluster assignments, ignoring permutations and with chance normalization</li> <li>Suitable for comparing clusterings of different sizes</li> <li>Symmetric: switching argument order will not change the score</li> </ul> Usage <p>The ARI metric can be used to evaluate clustering algorithms:</p> <pre><code>from framework3.plugins.metrics.clustering import ARI\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nx_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\ny_true = np.array([0, 0, 0, 1, 1, 1])\ny_pred = np.array([0, 0, 1, 1, 1, 1])\n\n# Create and use the ARI metric\nari_metric = ARI()\nscore = ari_metric.evaluate(x_data, y_true, y_pred)\nprint(f\"ARI Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray: Calculate the Adjusted Rand Index score.</p> Note <p>This metric uses scikit-learn's adjusted_rand_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>@Container.bind()\nclass ARI(BaseMetric):\n    \"\"\"\n    Adjusted Rand Index (ARI) metric for clustering evaluation.\n\n    This class calculates the ARI score, which is the corrected-for-chance version of the Rand Index.\n    It measures similarity between two clusterings, adjusted for chance.\n\n    Key Features:\n        - Measures the similarity of the cluster assignments, ignoring permutations and with chance normalization\n        - Suitable for comparing clusterings of different sizes\n        - Symmetric: switching argument order will not change the score\n\n    Usage:\n        The ARI metric can be used to evaluate clustering algorithms:\n\n        ```python\n        from framework3.plugins.metrics.clustering import ARI\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        x_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\n        y_true = np.array([0, 0, 0, 1, 1, 1])\n        y_pred = np.array([0, 0, 1, 1, 1, 1])\n\n        # Create and use the ARI metric\n        ari_metric = ARI()\n        score = ari_metric.evaluate(x_data, y_true, y_pred)\n        print(f\"ARI Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the Adjusted Rand Index score.\n\n    Note:\n        This metric uses scikit-learn's adjusted_rand_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def evaluate(\n        self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the Adjusted Rand Index score.\n\n        This method computes the ARI score between two clusterings.\n\n        Args:\n            x_data (XYData): The input data (not used in this metric, but required by the interface).\n            y_true (Any): The ground truth labels.\n            y_pred (Any): The predicted cluster labels.\n            **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's adjusted_rand_score.\n\n        Returns:\n            Float | np.ndarray: The ARI score.\n\n        Note:\n            This method uses scikit-learn's adjusted_rand_score function internally.\n        \"\"\"\n        return adjusted_rand_score(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.ARI.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the Adjusted Rand Index score.</p> <p>This method computes the ARI score between two clusterings.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data (not used in this metric, but required by the interface).</p> required <code>y_true</code> <code>Any</code> <p>The ground truth labels.</p> required <code>y_pred</code> <code>Any</code> <p>The predicted cluster labels.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments passed to sklearn's adjusted_rand_score.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The ARI score.</p> Note <p>This method uses scikit-learn's adjusted_rand_score function internally.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the Adjusted Rand Index score.\n\n    This method computes the ARI score between two clusterings.\n\n    Args:\n        x_data (XYData): The input data (not used in this metric, but required by the interface).\n        y_true (Any): The ground truth labels.\n        y_pred (Any): The predicted cluster labels.\n        **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's adjusted_rand_score.\n\n    Returns:\n        Float | np.ndarray: The ARI score.\n\n    Note:\n        This method uses scikit-learn's adjusted_rand_score function internally.\n    \"\"\"\n    return adjusted_rand_score(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.CalinskiHarabasz","title":"<code>CalinskiHarabasz</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Calinski-Harabasz Index metric for clustering evaluation.</p> <p>This class calculates the Calinski-Harabasz Index, which is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters.</p> Key Features <ul> <li>Measures the ratio of between-cluster variance to within-cluster variance</li> <li>Higher values indicate better-defined clusters</li> <li>Can be used to determine the optimal number of clusters</li> </ul> Usage <p>The Calinski-Harabasz metric can be used to evaluate clustering algorithms:</p> <pre><code>from framework3.plugins.metrics.clustering import CalinskiHarabasz\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nx_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\ny_pred = np.array([0, 0, 0, 1, 1, 1])\n\n# Create and use the Calinski-Harabasz metric\nch_metric = CalinskiHarabasz()\nscore = ch_metric.evaluate(x_data, None, y_pred)\nprint(f\"Calinski-Harabasz Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray: Calculate the Calinski-Harabasz Index.</p> Note <p>This metric uses scikit-learn's calinski_harabasz_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>@Container.bind()\nclass CalinskiHarabasz(BaseMetric):\n    \"\"\"\n    Calinski-Harabasz Index metric for clustering evaluation.\n\n    This class calculates the Calinski-Harabasz Index, which is the ratio of the sum of\n    between-clusters dispersion and of inter-cluster dispersion for all clusters.\n\n    Key Features:\n        - Measures the ratio of between-cluster variance to within-cluster variance\n        - Higher values indicate better-defined clusters\n        - Can be used to determine the optimal number of clusters\n\n    Usage:\n        The Calinski-Harabasz metric can be used to evaluate clustering algorithms:\n\n        ```python\n        from framework3.plugins.metrics.clustering import CalinskiHarabasz\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        x_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\n        y_pred = np.array([0, 0, 0, 1, 1, 1])\n\n        # Create and use the Calinski-Harabasz metric\n        ch_metric = CalinskiHarabasz()\n        score = ch_metric.evaluate(x_data, None, y_pred)\n        print(f\"Calinski-Harabasz Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the Calinski-Harabasz Index.\n\n    Note:\n        This metric uses scikit-learn's calinski_harabasz_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def evaluate(\n        self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the Calinski-Harabasz Index.\n\n        This method computes the Calinski-Harabasz Index for the clustering.\n\n        Args:\n            x_data (XYData): The input data.\n            y_true (Any): Not used for this metric, but required by the interface.\n            y_pred (Any): The predicted cluster labels.\n            **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's calinski_harabasz_score.\n\n        Returns:\n            Float | np.ndarray: The Calinski-Harabasz Index.\n\n        Note:\n            This method uses scikit-learn's calinski_harabasz_score function internally.\n        \"\"\"\n        return calinski_harabasz_score(x_data.value, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.CalinskiHarabasz.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the Calinski-Harabasz Index.</p> <p>This method computes the Calinski-Harabasz Index for the clustering.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data.</p> required <code>y_true</code> <code>Any</code> <p>Not used for this metric, but required by the interface.</p> required <code>y_pred</code> <code>Any</code> <p>The predicted cluster labels.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments passed to sklearn's calinski_harabasz_score.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The Calinski-Harabasz Index.</p> Note <p>This method uses scikit-learn's calinski_harabasz_score function internally.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the Calinski-Harabasz Index.\n\n    This method computes the Calinski-Harabasz Index for the clustering.\n\n    Args:\n        x_data (XYData): The input data.\n        y_true (Any): Not used for this metric, but required by the interface.\n        y_pred (Any): The predicted cluster labels.\n        **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's calinski_harabasz_score.\n\n    Returns:\n        Float | np.ndarray: The Calinski-Harabasz Index.\n\n    Note:\n        This method uses scikit-learn's calinski_harabasz_score function internally.\n    \"\"\"\n    return calinski_harabasz_score(x_data.value, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.Completeness","title":"<code>Completeness</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Completeness metric for clustering evaluation.</p> <p>This class calculates the Completeness score, which measures whether all members of a given class are assigned to the same cluster.</p> Key Features <ul> <li>Measures the extent to which all members of a given class are assigned to the same cluster</li> <li>Ranges from 0 to 1, where 1 indicates perfectly complete clustering</li> <li>Invariant to label switching</li> </ul> Usage <p>The Completeness metric can be used to evaluate clustering algorithms:</p> <pre><code>from framework3.plugins.metrics.clustering import Completeness\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nx_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\ny_true = np.array([0, 0, 0, 1, 1, 1])\ny_pred = np.array([0, 0, 1, 1, 1, 1])\n\n# Create and use the Completeness metric\ncompleteness_metric = Completeness()\nscore = completeness_metric.evaluate(x_data, y_true, y_pred)\nprint(f\"Completeness Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate </code> <p>XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray: Calculate the Completeness score.</p> Note <p>This metric uses scikit-learn's completeness_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>@Container.bind()\nclass Completeness(BaseMetric):\n    \"\"\"\n    Completeness metric for clustering evaluation.\n\n    This class calculates the Completeness score, which measures whether all members of\n    a given class are assigned to the same cluster.\n\n    Key Features:\n        - Measures the extent to which all members of a given class are assigned to the same cluster\n        - Ranges from 0 to 1, where 1 indicates perfectly complete clustering\n        - Invariant to label switching\n\n    Usage:\n        The Completeness metric can be used to evaluate clustering algorithms:\n\n        ```python\n        from framework3.plugins.metrics.clustering import Completeness\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        x_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\n        y_true = np.array([0, 0, 0, 1, 1, 1])\n        y_pred = np.array([0, 0, 1, 1, 1, 1])\n\n        # Create and use the Completeness metric\n        completeness_metric = Completeness()\n        score = completeness_metric.evaluate(x_data, y_true, y_pred)\n        print(f\"Completeness Score: {score}\")\n        ```\n\n    Methods:\n        evaluate (x_data: XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the Completeness score.\n\n    Note:\n        This metric uses scikit-learn's completeness_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def evaluate(\n        self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the Completeness score.\n\n        This method computes the Completeness score for the clustering.\n\n        Args:\n            x_data (XYData): The input data (not used in this metric, but required by the interface).\n            y_true (Any): The ground truth labels.\n            y_pred (Any): The predicted cluster labels.\n            **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's completeness_score.\n\n        Returns:\n            Float | np.ndarray: The Completeness score.\n\n        Note:\n            This method uses scikit-learn's completeness_score function internally.\n        \"\"\"\n        return completeness_score(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.Completeness.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the Completeness score.</p> <p>This method computes the Completeness score for the clustering.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data (not used in this metric, but required by the interface).</p> required <code>y_true</code> <code>Any</code> <p>The ground truth labels.</p> required <code>y_pred</code> <code>Any</code> <p>The predicted cluster labels.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments passed to sklearn's completeness_score.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The Completeness score.</p> Note <p>This method uses scikit-learn's completeness_score function internally.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the Completeness score.\n\n    This method computes the Completeness score for the clustering.\n\n    Args:\n        x_data (XYData): The input data (not used in this metric, but required by the interface).\n        y_true (Any): The ground truth labels.\n        y_pred (Any): The predicted cluster labels.\n        **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's completeness_score.\n\n    Returns:\n        Float | np.ndarray: The Completeness score.\n\n    Note:\n        This method uses scikit-learn's completeness_score function internally.\n    \"\"\"\n    return completeness_score(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.Homogeneity","title":"<code>Homogeneity</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Homogeneity metric for clustering evaluation.</p> <p>This class calculates the Homogeneity score, which measures whether all of its clusters contain only data points which are members of a single class.</p> Key Features <ul> <li>Measures the extent to which each cluster contains only members of a single class</li> <li>Ranges from 0 to 1, where 1 indicates perfectly homogeneous clustering</li> <li>Invariant to label switching</li> </ul> Usage <p>The Homogeneity metric can be used to evaluate clustering algorithms:</p> <pre><code>from framework3.plugins.metrics.clustering import Homogeneity\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nx_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\ny_true = np.array([0, 0, 0, 1, 1, 1])\ny_pred = np.array([0, 0, 1, 1, 1, 1])\n\n# Create and use the Homogeneity metric\nhomogeneity_metric = Homogeneity()\nscore = homogeneity_metric.evaluate(x_data, y_true, y_pred)\nprint(f\"Homogeneity Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray: Calculate the Homogeneity score.</p> Note <p>This metric uses scikit-learn's homogeneity_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>@Container.bind()\nclass Homogeneity(BaseMetric):\n    \"\"\"\n    Homogeneity metric for clustering evaluation.\n\n    This class calculates the Homogeneity score, which measures whether all of its clusters\n    contain only data points which are members of a single class.\n\n    Key Features:\n        - Measures the extent to which each cluster contains only members of a single class\n        - Ranges from 0 to 1, where 1 indicates perfectly homogeneous clustering\n        - Invariant to label switching\n\n    Usage:\n        The Homogeneity metric can be used to evaluate clustering algorithms:\n\n        ```python\n        from framework3.plugins.metrics.clustering import Homogeneity\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        x_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\n        y_true = np.array([0, 0, 0, 1, 1, 1])\n        y_pred = np.array([0, 0, 1, 1, 1, 1])\n\n        # Create and use the Homogeneity metric\n        homogeneity_metric = Homogeneity()\n        score = homogeneity_metric.evaluate(x_data, y_true, y_pred)\n        print(f\"Homogeneity Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the Homogeneity score.\n\n    Note:\n        This metric uses scikit-learn's homogeneity_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def evaluate(\n        self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the Homogeneity score.\n\n        This method computes the Homogeneity score for the clustering.\n\n        Args:\n            x_data (XYData): The input data (not used in this metric, but required by the interface).\n            y_true (Any): The ground truth labels.\n            y_pred (Any): The predicted cluster labels.\n            **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's homogeneity_score.\n\n        Returns:\n            Float | np.ndarray: The Homogeneity score.\n\n        Note:\n            This method uses scikit-learn's homogeneity_score function internally.\n        \"\"\"\n        return homogeneity_score(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.Homogeneity.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the Homogeneity score.</p> <p>This method computes the Homogeneity score for the clustering.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data (not used in this metric, but required by the interface).</p> required <code>y_true</code> <code>Any</code> <p>The ground truth labels.</p> required <code>y_pred</code> <code>Any</code> <p>The predicted cluster labels.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments passed to sklearn's homogeneity_score.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The Homogeneity score.</p> Note <p>This method uses scikit-learn's homogeneity_score function internally.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Dict[str, Any]\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the Homogeneity score.\n\n    This method computes the Homogeneity score for the clustering.\n\n    Args:\n        x_data (XYData): The input data (not used in this metric, but required by the interface).\n        y_true (Any): The ground truth labels.\n        y_pred (Any): The predicted cluster labels.\n        **kwargs (Dict[str,Any]): Additional keyword arguments passed to sklearn's homogeneity_score.\n\n    Returns:\n        Float | np.ndarray: The Homogeneity score.\n\n    Note:\n        This method uses scikit-learn's homogeneity_score function internally.\n    \"\"\"\n    return homogeneity_score(y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.NMI","title":"<code>NMI</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Normalized Mutual Information (NMI) metric for clustering evaluation.</p> <p>This class calculates the NMI score, which is a normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation).</p> Key Features <ul> <li>Measures the agreement of the true labels and predicted clusters, ignoring permutations</li> <li>Normalized to output values between 0 and 1</li> <li>Suitable for comparing clusterings of different sizes</li> </ul> Usage <p>The NMI metric can be used to evaluate clustering algorithms:</p> <pre><code>from framework3.plugins.metrics.clustering import NMI\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nx_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\ny_true = np.array([0, 0, 0, 1, 1, 1])\ny_pred = np.array([0, 0, 1, 1, 1, 1])\n\n# Create and use the NMI metric\nnmi_metric = NMI()\nscore = nmi_metric.evaluate(x_data, y_true, y_pred)\nprint(f\"NMI Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray: Calculate the Normalized Mutual Information score.</p> Note <p>This metric uses scikit-learn's normalized_mutual_info_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>@Container.bind()\nclass NMI(BaseMetric):\n    \"\"\"\n    Normalized Mutual Information (NMI) metric for clustering evaluation.\n\n    This class calculates the NMI score, which is a normalization of the Mutual Information (MI) score\n    to scale the results between 0 (no mutual information) and 1 (perfect correlation).\n\n    Key Features:\n        - Measures the agreement of the true labels and predicted clusters, ignoring permutations\n        - Normalized to output values between 0 and 1\n        - Suitable for comparing clusterings of different sizes\n\n    Usage:\n        The NMI metric can be used to evaluate clustering algorithms:\n\n        ```python\n        from framework3.plugins.metrics.clustering import NMI\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        x_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\n        y_true = np.array([0, 0, 0, 1, 1, 1])\n        y_pred = np.array([0, 0, 1, 1, 1, 1])\n\n        # Create and use the NMI metric\n        nmi_metric = NMI()\n        score = nmi_metric.evaluate(x_data, y_true, y_pred)\n        print(f\"NMI Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the Normalized Mutual Information score.\n\n    Note:\n        This metric uses scikit-learn's normalized_mutual_info_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def evaluate(\n        self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Unpack[NMIClustKwargs]\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the Normalized Mutual Information score.\n\n        This method computes the NMI score between two clusterings.\n\n        Args:\n            x_data (XYData): The input data (not used in this metric, but required by the interface).\n            y_true (Any): The ground truth labels.\n            y_pred (Any): The predicted cluster labels.\n            **kwargs (Unpack[NMIClustKwargs]): Additional keyword arguments passed to sklearn's normalized_mutual_info_score.\n\n        Returns:\n            Float | np.ndarray: The NMI score.\n\n        Note:\n            This method uses scikit-learn's normalized_mutual_info_score function internally.\n        \"\"\"\n        return normalized_mutual_info_score(y_true, y_pred, **kwargs)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.NMI.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the Normalized Mutual Information score.</p> <p>This method computes the NMI score between two clusterings.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data (not used in this metric, but required by the interface).</p> required <code>y_true</code> <code>Any</code> <p>The ground truth labels.</p> required <code>y_pred</code> <code>Any</code> <p>The predicted cluster labels.</p> required <code>**kwargs</code> <code>Unpack[NMIClustKwargs]</code> <p>Additional keyword arguments passed to sklearn's normalized_mutual_info_score.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The NMI score.</p> Note <p>This method uses scikit-learn's normalized_mutual_info_score function internally.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: Any, y_pred: Any, **kwargs: Unpack[NMIClustKwargs]\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the Normalized Mutual Information score.\n\n    This method computes the NMI score between two clusterings.\n\n    Args:\n        x_data (XYData): The input data (not used in this metric, but required by the interface).\n        y_true (Any): The ground truth labels.\n        y_pred (Any): The predicted cluster labels.\n        **kwargs (Unpack[NMIClustKwargs]): Additional keyword arguments passed to sklearn's normalized_mutual_info_score.\n\n    Returns:\n        Float | np.ndarray: The NMI score.\n\n    Note:\n        This method uses scikit-learn's normalized_mutual_info_score function internally.\n    \"\"\"\n    return normalized_mutual_info_score(y_true, y_pred, **kwargs)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.Silhouette","title":"<code>Silhouette</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Silhouette Coefficient metric for clustering evaluation.</p> <p>This class calculates the Silhouette Coefficient, which is calculated using the mean intra-cluster distance and the mean nearest-cluster distance for each sample.</p> Key Features <ul> <li>Measures how similar an object is to its own cluster compared to other clusters</li> <li>Ranges from -1 to 1, where higher values indicate better-defined clusters</li> <li>Can be used to determine the optimal number of clusters</li> </ul> Usage <p>The Silhouette metric can be used to evaluate clustering algorithms:</p> <pre><code>from framework3.plugins.metrics.clustering import Silhouette\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create sample data\nx_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\ny_pred = np.array([0, 0, 0, 1, 1, 1])\n\n# Create and use the Silhouette metric\nsilhouette_metric = Silhouette()\nscore = silhouette_metric.evaluate(x_data, None, y_pred)\nprint(f\"Silhouette Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray: Calculate the Silhouette Coefficient.</p> Note <p>This metric uses scikit-learn's silhouette_score function internally. Ensure that scikit-learn is properly installed and compatible with your environment.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>@Container.bind()\nclass Silhouette(BaseMetric):\n    \"\"\"\n    Silhouette Coefficient metric for clustering evaluation.\n\n    This class calculates the Silhouette Coefficient, which is calculated using the mean\n    intra-cluster distance and the mean nearest-cluster distance for each sample.\n\n    Key Features:\n        - Measures how similar an object is to its own cluster compared to other clusters\n        - Ranges from -1 to 1, where higher values indicate better-defined clusters\n        - Can be used to determine the optimal number of clusters\n\n    Usage:\n        The Silhouette metric can be used to evaluate clustering algorithms:\n\n        ```python\n        from framework3.plugins.metrics.clustering import Silhouette\n        from framework3.base.base_types import XYData\n        import numpy as np\n\n        # Create sample data\n        x_data = XYData(value=np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]))\n        y_pred = np.array([0, 0, 0, 1, 1, 1])\n\n        # Create and use the Silhouette metric\n        silhouette_metric = Silhouette()\n        score = silhouette_metric.evaluate(x_data, None, y_pred)\n        print(f\"Silhouette Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: Any, y_pred: Any, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the Silhouette Coefficient.\n\n    Note:\n        This metric uses scikit-learn's silhouette_score function internally. Ensure that scikit-learn\n        is properly installed and compatible with your environment.\n    \"\"\"\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: Any,\n        y_pred: Any,\n        **kwargs: Unpack[SilhouetteKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the Silhouette Coefficient.\n\n        This method computes the Silhouette Coefficient for each sample.\n\n        Args:\n            x_data (XYData): The input data.\n            y_true (Any): Not used for this metric, but required by the interface.\n            y_pred (Any): The predicted cluster labels.\n            **kwargs (Unpack[SilhouetteKwargs]): Additional keyword arguments passed to sklearn's silhouette_score.\n\n        Returns:\n            Float | np.ndarray: The Silhouette Coefficient.\n\n        Note:\n            This method uses scikit-learn's silhouette_score function internally.\n        \"\"\"\n        return silhouette_score(x_data.value, y_pred, **kwargs)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#framework3.plugins.metrics.clustering.Silhouette.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the Silhouette Coefficient.</p> <p>This method computes the Silhouette Coefficient for each sample.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data.</p> required <code>y_true</code> <code>Any</code> <p>Not used for this metric, but required by the interface.</p> required <code>y_pred</code> <code>Any</code> <p>The predicted cluster labels.</p> required <code>**kwargs</code> <code>Unpack[SilhouetteKwargs]</code> <p>Additional keyword arguments passed to sklearn's silhouette_score.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The Silhouette Coefficient.</p> Note <p>This method uses scikit-learn's silhouette_score function internally.</p> Source code in <code>framework3/plugins/metrics/clustering.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: Any,\n    y_pred: Any,\n    **kwargs: Unpack[SilhouetteKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the Silhouette Coefficient.\n\n    This method computes the Silhouette Coefficient for each sample.\n\n    Args:\n        x_data (XYData): The input data.\n        y_true (Any): Not used for this metric, but required by the interface.\n        y_pred (Any): The predicted cluster labels.\n        **kwargs (Unpack[SilhouetteKwargs]): Additional keyword arguments passed to sklearn's silhouette_score.\n\n    Returns:\n        Float | np.ndarray: The Silhouette Coefficient.\n\n    Note:\n        This method uses scikit-learn's silhouette_score function internally.\n    \"\"\"\n    return silhouette_score(x_data.value, y_pred, **kwargs)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#overview","title":"Overview","text":"<p>The Clustering Metrics module in LabChain provides a set of evaluation metrics specifically designed for assessing the performance of clustering algorithms. These metrics help in understanding various aspects of a clustering model's performance, such as cluster homogeneity, completeness, and overall quality.</p>"},{"location":"api/plugins/metrics/clustering/#available-clustering-metrics","title":"Available Clustering Metrics","text":""},{"location":"api/plugins/metrics/clustering/#normalized-mutual-information-nmi","title":"Normalized Mutual Information (NMI)","text":"<p>The Normalized Mutual Information score is implemented in the <code>NMI</code> class. It measures the mutual information between the true labels and the predicted clusters, normalized by the arithmetic mean of the labels' and clusters' entropy.</p>"},{"location":"api/plugins/metrics/clustering/#usage","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.clustering import NMI\nfrom framework3.base.base_types import XYData\n\nnmi_metric = NMI()\nscore = nmi_metric.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#adjusted-rand-index-ari","title":"Adjusted Rand Index (ARI)","text":"<p>The Adjusted Rand Index is implemented in the <code>ARI</code> class. It measures the similarity between two clusterings, adjusted for chance. It has a value close to 0 for random labeling and 1 for perfect clustering.</p>"},{"location":"api/plugins/metrics/clustering/#usage_1","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.clustering import ARI\nfrom framework3.base.base_types import XYData\n\nari_metric = ARI()\nscore = ari_metric.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#silhouette-score","title":"Silhouette Score","text":"<p>The Silhouette Score is implemented in the <code>Silhouette</code> class. It measures how similar an object is to its own cluster compared to other clusters. The silhouette value ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.</p>"},{"location":"api/plugins/metrics/clustering/#usage_2","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.clustering import Silhouette\nfrom framework3.base.base_types import XYData\n\nsilhouette_metric = Silhouette()\nscore = silhouette_metric.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#calinski-harabasz-index","title":"Calinski-Harabasz Index","text":"<p>The Calinski-Harabasz Index is implemented in the <code>CalinskiHarabasz</code> class. It's also known as the Variance Ratio Criterion. The score is defined as the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters. A higher Calinski-Harabasz score relates to a model with better defined clusters.</p>"},{"location":"api/plugins/metrics/clustering/#usage_3","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.clustering import CalinskiHarabasz\nfrom framework3.base.base_types import XYData\n\nch_metric = CalinskiHarabasz()\nscore = ch_metric.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#homogeneity-score","title":"Homogeneity Score","text":"<p>The Homogeneity Score is implemented in the <code>Homogeneity</code> class. It measures whether all of its clusters contain only data points which are members of a single class. The score is bounded below by 0 and above by 1. A higher value indicates better homogeneity.</p>"},{"location":"api/plugins/metrics/clustering/#usage_4","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.clustering import Homogeneity\nfrom framework3.base.base_types import XYData\n\nhomogeneity_metric = Homogeneity()\nscore = homogeneity_metric.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#completeness-score","title":"Completeness Score","text":"<p>The Completeness Score is implemented in the <code>Completeness</code> class. It measures whether all members of a given class are assigned to the same cluster. The score is bounded below by 0 and above by 1. A higher value indicates better completeness.</p>"},{"location":"api/plugins/metrics/clustering/#usage_5","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.clustering import Completeness\nfrom framework3.base.base_types import XYData\n\ncompleteness_metric = Completeness()\nscore = completeness_metric.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/metrics/clustering/#comprehensive-example-evaluating-a-clustering-model","title":"Comprehensive Example: Evaluating a Clustering Model","text":"<p>In this example, we'll demonstrate how to use the Clustering Metrics to evaluate the performance of a clustering model.</p> <pre><code>from framework3.plugins.filters.clustering.kmeans import KMeansPlugin\nfrom framework3.plugins.metrics.clustering import NMI, ARI, Silhouette, CalinskiHarabasz, Homogeneity, Completeness\nfrom framework3.base.base_types import XYData\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n# Generate sample data\nX, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Create XYData object\nX_data = XYData(_hash='X_data', _path='/tmp', _value=X)\n\n# Create and fit the clustering model\nkmeans = KMeansPlugin(n_clusters=4, random_state=0)\nkmeans.fit(X_data)\n\n# Get cluster predictions\ny_pred = kmeans.predict(X_data)\n\n# Initialize metrics\nnmi_metric = NMI()\nari_metric = ARI()\nsilhouette_metric = Silhouette()\nch_metric = CalinskiHarabasz()\nhomogeneity_metric = Homogeneity()\ncompleteness_metric = Completeness()\n\n# Compute metrics\nnmi_score = nmi_metric.evaluate(X_data, y_true, y_pred.value)\nari_score = ari_metric.evaluate(X_data, y_true, y_pred.value)\nsilhouette_score = silhouette_metric.evaluate(X_data, y_true, y_pred.value)\nch_score = ch_metric.evaluate(X_data, y_true, y_pred.value)\nhomogeneity_score = homogeneity_metric.evaluate(X_data, y_true, y_pred.value)\ncompleteness_score = completeness_metric.evaluate(X_data, y_true, y_pred.value)\n\n# Print results\nprint(f\"Normalized Mutual Information: {nmi_score}\")\nprint(f\"Adjusted Rand Index: {ari_score}\")\nprint(f\"Silhouette Score: {silhouette_score}\")\nprint(f\"Calinski-Harabasz Index: {ch_score}\")\nprint(f\"Homogeneity Score: {homogeneity_score}\")\nprint(f\"Completeness Score: {completeness_score}\")\n</code></pre> <p>This example demonstrates how to:</p> <ol> <li>Generate sample clustering data</li> <li>Create XYData objects for use with LabChain</li> <li>Train a KMeans clustering model</li> <li>Make predictions on the dataset</li> <li>Initialize and compute various clustering metrics</li> <li>Print the evaluation results</li> </ol>"},{"location":"api/plugins/metrics/clustering/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Multiple Metrics: Use multiple metrics to get a comprehensive view of your clustering model's performance. Different metrics capture different aspects of clustering quality.</p> </li> <li> <p>Ground Truth: When available, use metrics that compare against ground truth labels (like NMI, ARI) for a more robust evaluation.</p> </li> <li> <p>Internal Metrics: When ground truth is not available, rely on internal metrics like Silhouette Score and Calinski-Harabasz Index.</p> </li> <li> <p>Interpretation: Remember that the interpretation of these metrics can depend on the specific characteristics of your dataset and the clustering algorithm used.</p> </li> <li> <p>Visualization: Complement these metrics with visualization techniques to get a better understanding of your clustering results.</p> </li> <li> <p>Parameter Tuning: Use these metrics to guide the tuning of your clustering algorithm's parameters (e.g., number of clusters).</p> </li> <li> <p>Stability: Consider evaluating the stability of your clustering results by running the algorithm multiple times with different initializations.</p> </li> <li> <p>Domain Knowledge: Always interpret these metrics in the context of your domain knowledge and the specific goals of your clustering task.</p> </li> </ol>"},{"location":"api/plugins/metrics/clustering/#conclusion","title":"Conclusion","text":"<p>The Clustering Metrics module in LabChain provides essential tools for evaluating the performance of clustering models. By using these metrics in combination with other LabChain components, you can gain valuable insights into your model's strengths and weaknesses. The example demonstrates how easy it is to compute and interpret these metrics within the LabChain ecosystem, enabling you to make informed decisions about your clustering models.</p>"},{"location":"api/plugins/metrics/coherence/","title":"Coherence Metrics","text":""},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.NPMI","title":"<code>NPMI</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Normalized Pointwise Mutual Information (NPMI) coherence metric for topic modeling evaluation.</p> <p>This class calculates the NPMI coherence score, which measures the coherence of topics based on the normalized pointwise mutual information of word pairs.</p> Key Features <ul> <li>Measures topic coherence using normalized pointwise mutual information</li> <li>Suitable for evaluating topic models</li> <li>Handles input data as pandas DataFrames</li> </ul> Usage <p>The NPMI metric can be used to evaluate topic modeling results:</p> <pre><code>from framework3.plugins.metrics.coherence import NPMI\nfrom framework3.base.base_types import XYData\nimport pandas as pd\nimport numpy as np\n\n# Assuming you have a DataFrame 'df' with your document-term matrix\nx_data = XYData(value=df)\ny_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\nnpmi_metric = NPMI()\nscore = npmi_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\nprint(f\"NPMI Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray: Calculate the NPMI coherence score.</p> Note <p>This metric requires the input data to be a pandas DataFrame. Ensure that your data is properly formatted before using this metric.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>@Container.bind()\nclass NPMI(BaseMetric):\n    \"\"\"\n    Normalized Pointwise Mutual Information (NPMI) coherence metric for topic modeling evaluation.\n\n    This class calculates the NPMI coherence score, which measures the coherence of topics based on\n    the normalized pointwise mutual information of word pairs.\n\n    Key Features:\n        - Measures topic coherence using normalized pointwise mutual information\n        - Suitable for evaluating topic models\n        - Handles input data as pandas DataFrames\n\n    Usage:\n        The NPMI metric can be used to evaluate topic modeling results:\n\n        ```python\n        from framework3.plugins.metrics.coherence import NPMI\n        from framework3.base.base_types import XYData\n        import pandas as pd\n        import numpy as np\n\n        # Assuming you have a DataFrame 'df' with your document-term matrix\n        x_data = XYData(value=df)\n        y_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\n        npmi_metric = NPMI()\n        score = npmi_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\n        print(f\"NPMI Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the NPMI coherence score.\n\n    Note:\n        This metric requires the input data to be a pandas DataFrame. Ensure that your data\n        is properly formatted before using this metric.\n    \"\"\"\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: XYData | None,\n        y_pred: XYData,\n        **kwargs: Unpack[CoherenceEvaluateKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the NPMI coherence score.\n\n        This method computes the NPMI coherence score for the given topics.\n\n        Args:\n            x_data (XYData): The input data, expected to be a pandas DataFrame.\n            y_true (XYData | None): Not used for this metric, but required by the interface.\n            y_pred (XYData): The predicted topics, typically a list of lists of words.\n            **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n                - f_vocab (list): The vocabulary of the corpus.\n                - topk (int): The number of top words to consider for each topic (default: 10).\n                - processes (int): The number of processes to use for parallel computation (default: 1).\n\n        Returns:\n            Float | np.ndarray: The NPMI coherence score.\n\n        Raises:\n            Exception: If x_data is not a pandas DataFrame.\n\n        Note:\n            This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n        \"\"\"\n        f_vocab = kwargs.get(\"f_vocab\")\n        topk = kwargs.get(\"topk\", 10)\n        processes = kwargs.get(\"processes\", 1)\n        coherence = Coherence(\n            f_vocab=f_vocab, topk=topk, processes=processes, measure=\"c_npmi\"\n        )\n        if isinstance(x_data.value, pd.DataFrame):\n            return coherence.evaluate(df=x_data.value, predicted=y_pred)\n        else:\n            raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.NPMI.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the NPMI coherence score.</p> <p>This method computes the NPMI coherence score for the given topics.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data, expected to be a pandas DataFrame.</p> required <code>y_true</code> <code>XYData | None</code> <p>Not used for this metric, but required by the interface.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted topics, typically a list of lists of words.</p> required <code>**kwargs</code> <code>Unpack[EvaluateKwargs]</code> <p>Additional keyword arguments: - f_vocab (list): The vocabulary of the corpus. - topk (int): The number of top words to consider for each topic (default: 10). - processes (int): The number of processes to use for parallel computation (default: 1).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The NPMI coherence score.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If x_data is not a pandas DataFrame.</p> Note <p>This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: XYData | None,\n    y_pred: XYData,\n    **kwargs: Unpack[CoherenceEvaluateKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the NPMI coherence score.\n\n    This method computes the NPMI coherence score for the given topics.\n\n    Args:\n        x_data (XYData): The input data, expected to be a pandas DataFrame.\n        y_true (XYData | None): Not used for this metric, but required by the interface.\n        y_pred (XYData): The predicted topics, typically a list of lists of words.\n        **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n            - f_vocab (list): The vocabulary of the corpus.\n            - topk (int): The number of top words to consider for each topic (default: 10).\n            - processes (int): The number of processes to use for parallel computation (default: 1).\n\n    Returns:\n        Float | np.ndarray: The NPMI coherence score.\n\n    Raises:\n        Exception: If x_data is not a pandas DataFrame.\n\n    Note:\n        This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n    \"\"\"\n    f_vocab = kwargs.get(\"f_vocab\")\n    topk = kwargs.get(\"topk\", 10)\n    processes = kwargs.get(\"processes\", 1)\n    coherence = Coherence(\n        f_vocab=f_vocab, topk=topk, processes=processes, measure=\"c_npmi\"\n    )\n    if isinstance(x_data.value, pd.DataFrame):\n        return coherence.evaluate(df=x_data.value, predicted=y_pred)\n    else:\n        raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.UCI","title":"<code>UCI</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>UCI coherence metric for topic modeling evaluation.</p> <p>This class calculates the UCI coherence score, which is based on pointwise mutual information (PMI) of all word pairs in a topic.</p> Key Features <ul> <li>Measures topic coherence using pointwise mutual information of word pairs</li> <li>Suitable for evaluating topic models</li> <li>Handles input data as pandas DataFrames</li> </ul> Usage <p>The UCI metric can be used to evaluate topic modeling results:</p> <pre><code>from framework3.plugins.metrics.coherence import UCI\nfrom framework3.base.base_types import XYData\nimport pandas as pd\nimport numpy as np\n\n# Assuming you have a DataFrame 'df' with your document-term matrix\nx_data = XYData(value=df)\ny_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\nuci_metric = UCI()\nscore = uci_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\nprint(f\"UCI Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray: Calculate the UCI coherence score.</p> Note <p>This metric requires the input data to be a pandas DataFrame. Ensure that your data is properly formatted before using this metric.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>@Container.bind()\nclass UCI(BaseMetric):\n    \"\"\"\n    UCI coherence metric for topic modeling evaluation.\n\n    This class calculates the UCI coherence score, which is based on pointwise mutual information (PMI)\n    of all word pairs in a topic.\n\n    Key Features:\n        - Measures topic coherence using pointwise mutual information of word pairs\n        - Suitable for evaluating topic models\n        - Handles input data as pandas DataFrames\n\n    Usage:\n        The UCI metric can be used to evaluate topic modeling results:\n\n        ```python\n        from framework3.plugins.metrics.coherence import UCI\n        from framework3.base.base_types import XYData\n        import pandas as pd\n        import numpy as np\n\n        # Assuming you have a DataFrame 'df' with your document-term matrix\n        x_data = XYData(value=df)\n        y_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\n        uci_metric = UCI()\n        score = uci_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\n        print(f\"UCI Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the UCI coherence score.\n\n    Note:\n        This metric requires the input data to be a pandas DataFrame. Ensure that your data\n        is properly formatted before using this metric.\n    \"\"\"\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: XYData | None,\n        y_pred: XYData,\n        **kwargs: Unpack[CoherenceEvaluateKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the UCI coherence score.\n\n        This method computes the UCI coherence score for the given topics.\n\n        Args:\n            x_data (XYData): The input data, expected to be a pandas DataFrame.\n            y_true (XYData | None): Not used for this metric, but required by the interface.\n            y_pred (XYData): The predicted topics, typically a list of lists of words.\n            **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n                - f_vocab (list): The vocabulary of the corpus.\n                - topk (int): The number of top words to consider for each topic (default: 10).\n                - processes (int): The number of processes to use for parallel computation (default: 1).\n\n        Returns:\n            Float | np.ndarray: The UCI coherence score.\n\n        Raises:\n            Exception: If x_data is not a pandas DataFrame.\n\n        Note:\n            This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n        \"\"\"\n        f_vocab = kwargs.get(\"f_vocab\")\n        topk = cast(int, kwargs.get(\"topk\", 10))\n        processes = cast(int, kwargs.get(\"processes\", 1))\n        coherence = Coherence(\n            f_vocab=f_vocab, topk=topk, processes=processes, measure=\"c_uci\"\n        )\n        if isinstance(x_data.value, pd.DataFrame):\n            return coherence.evaluate(df=x_data.value, predicted=y_pred)\n        else:\n            raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.UCI.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the UCI coherence score.</p> <p>This method computes the UCI coherence score for the given topics.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data, expected to be a pandas DataFrame.</p> required <code>y_true</code> <code>XYData | None</code> <p>Not used for this metric, but required by the interface.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted topics, typically a list of lists of words.</p> required <code>**kwargs</code> <code>Unpack[EvaluateKwargs]</code> <p>Additional keyword arguments: - f_vocab (list): The vocabulary of the corpus. - topk (int): The number of top words to consider for each topic (default: 10). - processes (int): The number of processes to use for parallel computation (default: 1).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The UCI coherence score.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If x_data is not a pandas DataFrame.</p> Note <p>This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: XYData | None,\n    y_pred: XYData,\n    **kwargs: Unpack[CoherenceEvaluateKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the UCI coherence score.\n\n    This method computes the UCI coherence score for the given topics.\n\n    Args:\n        x_data (XYData): The input data, expected to be a pandas DataFrame.\n        y_true (XYData | None): Not used for this metric, but required by the interface.\n        y_pred (XYData): The predicted topics, typically a list of lists of words.\n        **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n            - f_vocab (list): The vocabulary of the corpus.\n            - topk (int): The number of top words to consider for each topic (default: 10).\n            - processes (int): The number of processes to use for parallel computation (default: 1).\n\n    Returns:\n        Float | np.ndarray: The UCI coherence score.\n\n    Raises:\n        Exception: If x_data is not a pandas DataFrame.\n\n    Note:\n        This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n    \"\"\"\n    f_vocab = kwargs.get(\"f_vocab\")\n    topk = cast(int, kwargs.get(\"topk\", 10))\n    processes = cast(int, kwargs.get(\"processes\", 1))\n    coherence = Coherence(\n        f_vocab=f_vocab, topk=topk, processes=processes, measure=\"c_uci\"\n    )\n    if isinstance(x_data.value, pd.DataFrame):\n        return coherence.evaluate(df=x_data.value, predicted=y_pred)\n    else:\n        raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.UMASS","title":"<code>UMASS</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>UMass coherence metric for topic modeling evaluation.</p> <p>This class calculates the UMass coherence score, which is based on document co-occurrence counts and a sliding window.</p> Key Features <ul> <li>Measures topic coherence using document co-occurrence</li> <li>Suitable for evaluating topic models</li> <li>Handles input data as pandas DataFrames</li> </ul> Usage <p>The UMASS metric can be used to evaluate topic modeling results:</p> <pre><code>from framework3.plugins.metrics.coherence import UMASS\nfrom framework3.base.base_types import XYData\nimport pandas as pd\nimport numpy as np\n\n# Assuming you have a DataFrame 'df' with your document-term matrix\nx_data = XYData(value=df)\ny_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\numass_metric = UMASS()\nscore = umass_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\nprint(f\"UMass Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray: Calculate the UMass coherence score.</p> Note <p>This metric requires the input data to be a pandas DataFrame. Ensure that your data is properly formatted before using this metric.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>@Container.bind()\nclass UMASS(BaseMetric):\n    \"\"\"\n    UMass coherence metric for topic modeling evaluation.\n\n    This class calculates the UMass coherence score, which is based on document co-occurrence counts\n    and a sliding window.\n\n    Key Features:\n        - Measures topic coherence using document co-occurrence\n        - Suitable for evaluating topic models\n        - Handles input data as pandas DataFrames\n\n    Usage:\n        The UMASS metric can be used to evaluate topic modeling results:\n\n        ```python\n        from framework3.plugins.metrics.coherence import UMASS\n        from framework3.base.base_types import XYData\n        import pandas as pd\n        import numpy as np\n\n        # Assuming you have a DataFrame 'df' with your document-term matrix\n        x_data = XYData(value=df)\n        y_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\n        umass_metric = UMASS()\n        score = umass_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\n        print(f\"UMass Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the UMass coherence score.\n\n    Note:\n        This metric requires the input data to be a pandas DataFrame. Ensure that your data\n        is properly formatted before using this metric.\n    \"\"\"\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: XYData | None,\n        y_pred: XYData,\n        **kwargs: Unpack[CoherenceEvaluateKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the UMass coherence score.\n\n        This method computes the UMass coherence score for the given topics.\n\n        Args:\n            x_data (XYData): The input data, expected to be a pandas DataFrame.\n            y_true (XYData | None): Not used for this metric, but required by the interface.\n            y_pred (XYData): The predicted topics, typically a list of lists of words.\n            **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n                - f_vocab (list): The vocabulary of the corpus.\n                - topk (int): The number of top words to consider for each topic (default: 10).\n                - processes (int): The number of processes to use for parallel computation (default: 1).\n\n        Returns:\n            Float | np.ndarray: The UMass coherence score.\n\n        Raises:\n            Exception: If x_data is not a pandas DataFrame.\n\n        Note:\n            This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n        \"\"\"\n        f_vocab = kwargs.get(\"f_vocab\")\n        topk = kwargs.get(\"topk\", 10)\n        processes = kwargs.get(\"processes\", 1)\n        coherence = Coherence(\n            f_vocab=f_vocab, topk=topk, processes=processes, measure=\"u_mass\"\n        )\n        if isinstance(x_data.value, pd.DataFrame):\n            return coherence.evaluate(df=x_data.value, predicted=y_pred)\n        else:\n            raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.UMASS.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the UMass coherence score.</p> <p>This method computes the UMass coherence score for the given topics.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data, expected to be a pandas DataFrame.</p> required <code>y_true</code> <code>XYData | None</code> <p>Not used for this metric, but required by the interface.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted topics, typically a list of lists of words.</p> required <code>**kwargs</code> <code>Unpack[EvaluateKwargs]</code> <p>Additional keyword arguments: - f_vocab (list): The vocabulary of the corpus. - topk (int): The number of top words to consider for each topic (default: 10). - processes (int): The number of processes to use for parallel computation (default: 1).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The UMass coherence score.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If x_data is not a pandas DataFrame.</p> Note <p>This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: XYData | None,\n    y_pred: XYData,\n    **kwargs: Unpack[CoherenceEvaluateKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the UMass coherence score.\n\n    This method computes the UMass coherence score for the given topics.\n\n    Args:\n        x_data (XYData): The input data, expected to be a pandas DataFrame.\n        y_true (XYData | None): Not used for this metric, but required by the interface.\n        y_pred (XYData): The predicted topics, typically a list of lists of words.\n        **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n            - f_vocab (list): The vocabulary of the corpus.\n            - topk (int): The number of top words to consider for each topic (default: 10).\n            - processes (int): The number of processes to use for parallel computation (default: 1).\n\n    Returns:\n        Float | np.ndarray: The UMass coherence score.\n\n    Raises:\n        Exception: If x_data is not a pandas DataFrame.\n\n    Note:\n        This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n    \"\"\"\n    f_vocab = kwargs.get(\"f_vocab\")\n    topk = kwargs.get(\"topk\", 10)\n    processes = kwargs.get(\"processes\", 1)\n    coherence = Coherence(\n        f_vocab=f_vocab, topk=topk, processes=processes, measure=\"u_mass\"\n    )\n    if isinstance(x_data.value, pd.DataFrame):\n        return coherence.evaluate(df=x_data.value, predicted=y_pred)\n    else:\n        raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.V","title":"<code>V</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>V-measure coherence metric for topic modeling evaluation.</p> <p>This class calculates the V-measure coherence score, which is based on a combination of homogeneity and completeness.</p> Key Features <ul> <li>Measures topic coherence using a combination of homogeneity and completeness</li> <li>Suitable for evaluating topic models</li> <li>Handles input data as pandas DataFrames</li> </ul> Usage <p>The V-measure metric can be used to evaluate topic modeling results:</p> <pre><code>from framework3.plugins.metrics.coherence import V\nfrom framework3.base.base_types import XYData\nimport pandas as pd\nimport numpy as np\n\n# Assuming you have a DataFrame 'df' with your document-term matrix\nx_data = XYData(value=df)\ny_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\nv_metric = V()\nscore = v_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\nprint(f\"V-measure Score: {score}\")\n</code></pre> <p>Methods:</p> Name Description <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray: Calculate the V-measure coherence score.</p> Note <p>This metric requires the input data to be a pandas DataFrame. Ensure that your data is properly formatted before using this metric.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>@Container.bind()\nclass V(BaseMetric):\n    \"\"\"\n    V-measure coherence metric for topic modeling evaluation.\n\n    This class calculates the V-measure coherence score, which is based on a combination of\n    homogeneity and completeness.\n\n    Key Features:\n        - Measures topic coherence using a combination of homogeneity and completeness\n        - Suitable for evaluating topic models\n        - Handles input data as pandas DataFrames\n\n    Usage:\n        The V-measure metric can be used to evaluate topic modeling results:\n\n        ```python\n        from framework3.plugins.metrics.coherence import V\n        from framework3.base.base_types import XYData\n        import pandas as pd\n        import numpy as np\n\n        # Assuming you have a DataFrame 'df' with your document-term matrix\n        x_data = XYData(value=df)\n        y_pred = np.array([['word1', 'word2', 'word3'], ['word4', 'word5', 'word6']])  # Example topics\n\n        v_metric = V()\n        score = v_metric.evaluate(x_data, None, y_pred, f_vocab=df.columns)\n        print(f\"V-measure Score: {score}\")\n        ```\n\n    Methods:\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData, **kwargs) -&gt; Float | np.ndarray:\n            Calculate the V-measure coherence score.\n\n    Note:\n        This metric requires the input data to be a pandas DataFrame. Ensure that your data\n        is properly formatted before using this metric.\n    \"\"\"\n\n    def evaluate(\n        self,\n        x_data: XYData,\n        y_true: XYData | None,\n        y_pred: XYData,\n        **kwargs: Unpack[CoherenceEvaluateKwargs],\n    ) -&gt; Float | np.ndarray:\n        \"\"\"\n        Calculate the V-measure coherence score.\n\n        This method computes the V-measure coherence score for the given topics.\n\n        Args:\n            x_data (XYData): The input data, expected to be a pandas DataFrame.\n            y_true (XYData | None): Not used for this metric, but required by the interface.\n            y_pred (XYData): The predicted topics, typically a list of lists of words.\n            **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n                - f_vocab (list): The vocabulary of the corpus.\n                - topk (int): The number of top words to consider for each topic (default: 10).\n                - processes (int): The number of processes to use for parallel computation (default: 1).\n\n        Returns:\n            Float | np.ndarray: The V-measure coherence score.\n\n        Raises:\n            Exception: If x_data is not a pandas DataFrame.\n\n        Note:\n            This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n        \"\"\"\n        f_vocab = kwargs.get(\"f_vocab\")\n        topk: int = cast(int, kwargs.get(\"topk\", 10))\n        processes: int = cast(int, kwargs.get(\"processes\", 1))\n        coherence = Coherence(\n            f_vocab=f_vocab, topk=topk, processes=processes, measure=\"c_v\"\n        )\n        if isinstance(x_data.value, pd.DataFrame):\n            return coherence.evaluate(df=x_data.value, predicted=y_pred)\n        else:\n            raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#framework3.plugins.metrics.coherence.V.evaluate","title":"<code>evaluate(x_data, y_true, y_pred, **kwargs)</code>","text":"<p>Calculate the V-measure coherence score.</p> <p>This method computes the V-measure coherence score for the given topics.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data, expected to be a pandas DataFrame.</p> required <code>y_true</code> <code>XYData | None</code> <p>Not used for this metric, but required by the interface.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted topics, typically a list of lists of words.</p> required <code>**kwargs</code> <code>Unpack[EvaluateKwargs]</code> <p>Additional keyword arguments: - f_vocab (list): The vocabulary of the corpus. - topk (int): The number of top words to consider for each topic (default: 10). - processes (int): The number of processes to use for parallel computation (default: 1).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Float | ndarray</code> <p>Float | np.ndarray: The V-measure coherence score.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If x_data is not a pandas DataFrame.</p> Note <p>This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.</p> Source code in <code>framework3/plugins/metrics/coherence.py</code> <pre><code>def evaluate(\n    self,\n    x_data: XYData,\n    y_true: XYData | None,\n    y_pred: XYData,\n    **kwargs: Unpack[CoherenceEvaluateKwargs],\n) -&gt; Float | np.ndarray:\n    \"\"\"\n    Calculate the V-measure coherence score.\n\n    This method computes the V-measure coherence score for the given topics.\n\n    Args:\n        x_data (XYData): The input data, expected to be a pandas DataFrame.\n        y_true (XYData | None): Not used for this metric, but required by the interface.\n        y_pred (XYData): The predicted topics, typically a list of lists of words.\n        **kwargs (Unpack[EvaluateKwargs]): Additional keyword arguments:\n            - f_vocab (list): The vocabulary of the corpus.\n            - topk (int): The number of top words to consider for each topic (default: 10).\n            - processes (int): The number of processes to use for parallel computation (default: 1).\n\n    Returns:\n        Float | np.ndarray: The V-measure coherence score.\n\n    Raises:\n        Exception: If x_data is not a pandas DataFrame.\n\n    Note:\n        This method uses the Coherence class from framework3.plugins.metrics.utils.coherence internally.\n    \"\"\"\n    f_vocab = kwargs.get(\"f_vocab\")\n    topk: int = cast(int, kwargs.get(\"topk\", 10))\n    processes: int = cast(int, kwargs.get(\"processes\", 1))\n    coherence = Coherence(\n        f_vocab=f_vocab, topk=topk, processes=processes, measure=\"c_v\"\n    )\n    if isinstance(x_data.value, pd.DataFrame):\n        return coherence.evaluate(df=x_data.value, predicted=y_pred)\n    else:\n        raise Exception(\"x_data must be a pandas DataFrame\")\n</code></pre>"},{"location":"api/plugins/metrics/coherence/#overview","title":"Overview","text":"<p>The Coherence Metrics module in LabChain provides evaluation metrics specifically designed for assessing the quality and interpretability of topic models. These metrics help in understanding how well the generated topics represent the underlying themes in a corpus of documents.</p>"},{"location":"api/plugins/metrics/coherence/#available-coherence-metrics","title":"Available Coherence Metrics","text":""},{"location":"api/plugins/metrics/coherence/#npmi-normalized-pointwise-mutual-information","title":"NPMI (Normalized Pointwise Mutual Information)","text":"<p>The NPMI coherence metric is implemented in the <code>NPMI</code> class. It measures the semantic similarity between high-scoring words in the topic based on normalized pointwise mutual information.</p>"},{"location":"api/plugins/metrics/coherence/#usage","title":"Usage","text":"<pre><code>from framework3.plugins.metrics.coherence import NPMI\nfrom framework3.base.base_types import XYData\n\nnpmi_metric = NPMI(measure='c_npmi', topk=10, processes=1)\nscore = npmi_metric.evaluate(df, predicted)\n</code></pre> <p>Parameters: - <code>measure</code>: The coherence measure to use. Default is 'c_npmi'. - <code>topk</code>: The number of top words to consider for each topic. Default is 10. - <code>processes</code>: The number of processes to use for parallel computation. Default is 1.</p>"},{"location":"api/plugins/metrics/coherence/#how-coherence-metrics-work","title":"How Coherence Metrics Work","text":"<p>Coherence metrics typically work by:</p> <ol> <li>Extracting the top N words from each topic.</li> <li>Calculating the semantic similarity between these words based on their co-occurrence in the corpus.</li> <li>Aggregating these similarities to produce a single coherence score for each topic or for the entire model.</li> </ol> <p>The NPMI metric specifically:</p> <ol> <li>Calculates the pointwise mutual information (PMI) between pairs of words.</li> <li>Normalizes the PMI scores to account for the frequency of individual words.</li> <li>Averages these normalized scores to produce the final coherence score.</li> </ol>"},{"location":"api/plugins/metrics/coherence/#comprehensive-example-evaluating-a-topic-model","title":"Comprehensive Example: Evaluating a Topic Model","text":"<p>Here's an example of how to use the NPMI coherence metric to evaluate a topic model:</p> <pre><code>import pandas as pd\nfrom framework3.plugins.filters.topic_modeling.lda import LDAPlugin\nfrom framework3.plugins.metrics.coherence import NPMI\nfrom framework3.base.base_types import XYData\n\n# Assume we have a DataFrame 'df' with a 'text' column containing our documents\ndf = pd.DataFrame({'text': [\"This is document 1\", \"This is document 2\", \"Another document here\"]})\n\n# Create XYData object\nX_data = XYData(_hash='X_data', _path='/tmp', _value=df['text'].values)\n\n# Create and fit the LDA model\nlda_model = LDAPlugin(n_components=5, random_state=42)\nlda_model.fit(X_data)\n\n# Get topic-word distributions\ntopic_word_dist = lda_model.get_topic_word_dist()\n\n# Initialize NPMI metric\nnpmi_metric = NPMI(measure='c_npmi', topk=10, processes=1)\n\n# Evaluate coherence\ncoherence_score = npmi_metric.evaluate(df, (None, topic_word_dist, None))\n\nprint(f\"NPMI Coherence Score: {coherence_score}\")\n</code></pre> <p>This example demonstrates how to:</p> <ol> <li>Prepare your text data</li> <li>Create XYData objects for use with LabChain</li> <li>Train an LDA topic model</li> <li>Extract topic-word distributions</li> <li>Initialize and compute the NPMI coherence metric</li> <li>Print the evaluation result</li> </ol>"},{"location":"api/plugins/metrics/coherence/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Multiple Runs: Topic modeling algorithms often have a random component. Run your model multiple times and average the coherence scores for more stable results.</p> </li> <li> <p>Number of Topics: Use coherence metrics to help determine the optimal number of topics for your model. Try different numbers of topics and compare their coherence scores.</p> </li> <li> <p>Preprocessing: The quality of your preprocessing can significantly affect coherence scores. Ensure your text is properly cleaned and tokenized.</p> </li> <li> <p>Interpretation: Remember that while higher coherence scores generally indicate better topic quality, they should be interpreted in conjunction with qualitative analysis of the topics.</p> </li> <li> <p>Comparison: Use coherence metrics to compare different topic modeling approaches (e.g., LDA vs. NMF) on the same dataset.</p> </li> <li> <p>Domain Knowledge: Always interpret coherence scores in the context of your domain knowledge and the specific goals of your topic modeling task.</p> </li> <li> <p>Visualization: Complement coherence metrics with visualization techniques (like pyLDAvis) to get a better understanding of your topic model results.</p> </li> <li> <p>Parameter Tuning: Use coherence scores to guide the tuning of your topic model's parameters (e.g., alpha and beta in LDA).</p> </li> </ol>"},{"location":"api/plugins/metrics/coherence/#conclusion","title":"Conclusion","text":"<p>The Coherence Metrics module in LabChain provides essential tools for evaluating the quality of topic models. By using these metrics in combination with other LabChain components, you can gain valuable insights into your model's performance and interpretability. The example demonstrates how easy it is to compute and interpret these metrics within the LabChain ecosystem, enabling you to make informed decisions about your topic modeling approach.</p>"},{"location":"api/plugins/optimizers/grid_optimizer/","title":"GridOptimizer","text":""},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer","title":"<code>framework3.plugins.optimizer.grid_optimizer</code>","text":""},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer","title":"<code>GridOptimizer</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>Grid search optimizer for hyperparameter tuning.</p> <p>This class implements a grid search for hyperparameter optimization. It exhaustively searches through a specified parameter grid to find the best combination of hyperparameters.</p> Key Features <ul> <li>Supports various types of hyperparameters (categorical, numerical)</li> <li>Performs an exhaustive search over the specified parameter grid</li> <li>Evaluates each parameter combination on the entire dataset</li> <li>Integrates with the Framework3 pipeline system</li> </ul> Usage <p>The GridOptimizer can be used to optimize hyperparameters of a machine learning pipeline:</p> <pre><code>from framework3.plugins.optimizer import GridOptimizer\nfrom framework3.base import XYData\n\n# Assuming you have a pipeline and data\npipeline = ...\nx_data = XYData(...)\ny_data = XYData(...)\n\noptimizer = GridOptimizer(scoring=some_metric)\noptimizer.optimize(pipeline)\noptimizer.fit(x_data, y_data)\n\nbest_pipeline = optimizer.pipeline\n</code></pre> <p>Attributes:</p> Name Type Description <code>scoring</code> <code>BaseMetric</code> <p>The scoring metric to use for evaluation.</p> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be optimized.</p> <code>best_params</code> <code>Dict[str, Any]</code> <p>The best parameters found during the search.</p> <code>best_score</code> <code>float</code> <p>The best score achieved during the search.</p> <code>_grid</code> <code>Dict[str, Any]</code> <p>The parameter grid for the search.</p> <code>_results</code> <code>DataFrame | None</code> <p>DataFrame containing all evaluation results.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>BaseFilter): Set up the optimization process for a given pipeline.</p> <code>fit</code> <p>XYData, y: Optional[XYData]) -&gt; None | float: Perform the grid search and fit the best pipeline.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the best pipeline found.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Evaluate the optimized pipeline.</p> Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>class GridOptimizer(BaseOptimizer):\n    \"\"\"\n    Grid search optimizer for hyperparameter tuning.\n\n    This class implements a grid search for hyperparameter optimization.\n    It exhaustively searches through a specified parameter grid to find the best combination of hyperparameters.\n\n    Key Features:\n        - Supports various types of hyperparameters (categorical, numerical)\n        - Performs an exhaustive search over the specified parameter grid\n        - Evaluates each parameter combination on the entire dataset\n        - Integrates with the Framework3 pipeline system\n\n    Usage:\n        The GridOptimizer can be used to optimize hyperparameters of a machine learning pipeline:\n\n        ```python\n        from framework3.plugins.optimizer import GridOptimizer\n        from framework3.base import XYData\n\n        # Assuming you have a pipeline and data\n        pipeline = ...\n        x_data = XYData(...)\n        y_data = XYData(...)\n\n        optimizer = GridOptimizer(scoring=some_metric)\n        optimizer.optimize(pipeline)\n        optimizer.fit(x_data, y_data)\n\n        best_pipeline = optimizer.pipeline\n        ```\n\n    Attributes:\n        scoring (BaseMetric): The scoring metric to use for evaluation.\n        pipeline (BaseFilter | None): The pipeline to be optimized.\n        best_params (Dict[str, Any]): The best parameters found during the search.\n        best_score (float): The best score achieved during the search.\n        _grid (Dict[str, Any]): The parameter grid for the search.\n        _results (pd.DataFrame | None): DataFrame containing all evaluation results.\n\n    Methods:\n        optimize(pipeline: BaseFilter): Set up the optimization process for a given pipeline.\n        fit(x: XYData, y: Optional[XYData]) -&gt; None | float: Perform the grid search and fit the best pipeline.\n        predict(x: XYData) -&gt; XYData: Make predictions using the best pipeline found.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Evaluate the optimized pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        scorer: BaseMetric,\n        pipeline: BaseFilter | None = None,\n    ):\n        super().__init__(pipeline=pipeline, scorer=scorer)\n        self.scorer: BaseMetric = scorer\n        self.pipeline: Optional[BaseFilter] = None\n\n        self.best_params: Dict[str, Any] = {}\n        self.best_score: float = float(\"-inf\")\n        self._grid: Dict[str, Any] = {}\n        self._results: pd.DataFrame | None = None\n\n    def get_grid(self, aux: Dict[str, Any], grid: Dict[str, Any]):\n        \"\"\"\n        Recursively process the grid configuration of a pipeline or filter.\n\n        This method traverses the configuration dictionary and extracts the grid parameters.\n\n        Args:\n            aux (Dict[str, Any]): The configuration dictionary to process.\n            grid (Dict[str, Any]): The grid to populate with hyperparameters.\n\n        Note:\n            This method modifies the input dictionary in-place.\n        \"\"\"\n        match aux[\"params\"]:\n            case {\"filters\": filters, **r}:\n                for filter_config in filters:\n                    self.get_grid(filter_config, grid)\n            case {\"pipeline\": pipeline, **r}:  # noqa: F841\n                self.get_grid(pipeline, grid)\n            case {\"filter\": cached_filter, **r}:  # noqa: F841\n                self.get_grid(cached_filter, grid)\n            case _:\n                if \"_grid\" in aux:\n                    grid[aux[\"clazz\"]] = aux[\"_grid\"]\n\n    def get_from_grid(self, aux: Dict[str, Any], config: Dict[str, Any]):\n        \"\"\"\n        Recursively process the grid configuration of a pipeline or filter.\n\n        This method traverses the configuration dictionary and applies the grid parameters.\n\n        Args:\n            aux (Dict[str, Any]): The configuration dictionary to process.\n            config (Dict[str, Any]): The configuration to apply.\n\n        Note:\n            This method modifies the input dictionary in-place.\n        \"\"\"\n        match aux[\"params\"]:\n            case {\"filters\": filters, **r}:\n                for filter_config in filters:\n                    self.get_from_grid(filter_config, config)\n            case {\"pipeline\": pipeline, **r}:  # noqa: F841\n                self.get_from_grid(pipeline, config)\n            case {\"filter\": cached_filter, **r}:  # noqa: F841\n                self.get_from_grid(cached_filter, config)\n            case p_params:\n                if \"_grid\" in aux:\n                    for param, value in aux[\"_grid\"].items():\n                        p_params.update({param: config[aux[\"clazz\"]][param]})\n\n    def nested_product(self, d: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        def recurse(current) -&gt; List[Dict[str, Any]]:\n            if isinstance(current, dict):\n                keys, values = zip(*[(k, recurse(v)) for k, v in current.items()])\n                return [dict(zip(keys, v)) for v in itertools.product(*values)]\n            elif isinstance(current, list):\n                return current\n            else:\n                return [current]\n\n        return recurse(d)\n\n    def optimize(self, pipeline: BaseFilter):\n        \"\"\"\n        Set up the optimization process for a given pipeline.\n\n        This method prepares the pipeline for grid search optimization.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be optimized.\n        \"\"\"\n        self.pipeline = pipeline\n        self.pipeline.verbose(False)\n\n    def fit(self, x: XYData, y: Optional[XYData]):\n        \"\"\"\n        Perform the grid search and fit the best pipeline.\n\n        This method runs the grid search optimization process and fits the best found pipeline.\n\n        Args:\n            x (XYData): The input features.\n            y (Optional[XYData]): The target values (if applicable).\n\n        Returns:\n            None | float: None if the pipeline is fitted successfully, or the best score if available.\n\n        Raises:\n            ValueError: If the pipeline is not defined before fitting.\n        \"\"\"\n        if self.pipeline is None:\n            raise ValueError(\"Pipeline must be set before fitting\")\n\n        dumped_pipeline = self.pipeline.item_dump(include=[\"_grid\"])\n\n        self.get_grid(dumped_pipeline, self._grid)\n\n        print(self._grid)\n\n        results = []\n        combinations = self.nested_product(self._grid)\n\n        for param_dict in combinations:\n            self.get_from_grid(dumped_pipeline, param_dict)\n\n            pipeline: BaseFilter = cast(\n                BaseFilter, BasePlugin.build_from_dump(dumped_pipeline, Container.pif)\n            )\n\n            pipeline.verbose(False)\n\n            match pipeline.fit(x, y):\n                case None:\n                    losses = pipeline.evaluate(x, y, pipeline.predict(x))\n\n                    score = losses.get(self.scorer.__class__.__name__, 0.0)\n\n                case float() as score:\n                    pass\n                case _:\n                    raise ValueError(\"Unexpected return type from pipeline.fit()\")\n\n            results.append({**param_dict, \"score\": float(score)})\n\n        # Create DataFrame with all combinations and scores\n        self._results = pd.DataFrame(results)\n        self._results = self._results.sort_values(\n            \"score\", ascending=not self.scorer.higher_better\n        )\n\n        self.best_params = self._results.iloc[0].drop(\"score\").to_dict()\n\n        self.get_from_grid(dumped_pipeline, self.best_params)\n\n        self.pipeline = cast(\n            BaseFilter, BasePlugin.build_from_dump(dumped_pipeline, Container.pif)\n        )\n\n        return self.pipeline.unwrap().fit(x, y)\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the best pipeline found.\n\n        Args:\n            x (XYData): The input features.\n\n        Returns:\n            XYData: The predictions.\n\n        Raises:\n            ValueError: If the pipeline is not fitted before predicting.\n        \"\"\"\n        if self.pipeline is not None:\n            return self.pipeline.predict(x)\n        else:\n            raise ValueError(\"Pipeline must be fitted before predicting\")\n\n    def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n        \"\"\"\n        Start the pipeline execution.\n\n        Args:\n            x (XYData): Input data for fitting.\n            y (XYData | None): Target data for fitting.\n            X_ (XYData | None): Data for prediction (if different from x).\n\n        Returns:\n            XYData | None: Prediction results if X_ is provided, else None.\n\n        Raises:\n            ValueError: If the pipeline has not been fitted.\n        \"\"\"\n        if self.pipeline is not None:\n            return self.pipeline.start(x, y, X_)\n        else:\n            raise ValueError(\"Pipeline must be fitted before starting\")\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the optimized pipeline.\n\n        Args:\n            x_data (XYData): Input data.\n            y_true (XYData | None): True target data.\n            y_pred (XYData): Predicted target data.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation results.\n        \"\"\"\n        return (\n            self.pipeline.evaluate(x_data, y_true, y_pred)\n            if self.pipeline is not None\n            else {}\n        )\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.best_params","title":"<code>best_params = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.best_score","title":"<code>best_score = float('-inf')</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.pipeline","title":"<code>pipeline = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.scorer","title":"<code>scorer = scorer</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.__init__","title":"<code>__init__(scorer, pipeline=None)</code>","text":"Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def __init__(\n    self,\n    scorer: BaseMetric,\n    pipeline: BaseFilter | None = None,\n):\n    super().__init__(pipeline=pipeline, scorer=scorer)\n    self.scorer: BaseMetric = scorer\n    self.pipeline: Optional[BaseFilter] = None\n\n    self.best_params: Dict[str, Any] = {}\n    self.best_score: float = float(\"-inf\")\n    self._grid: Dict[str, Any] = {}\n    self._results: pd.DataFrame | None = None\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the optimized pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>Input data.</p> required <code>y_true</code> <code>XYData | None</code> <p>True target data.</p> required <code>y_pred</code> <code>XYData</code> <p>Predicted target data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation results.</p> Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the optimized pipeline.\n\n    Args:\n        x_data (XYData): Input data.\n        y_true (XYData | None): True target data.\n        y_pred (XYData): Predicted target data.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results.\n    \"\"\"\n    return (\n        self.pipeline.evaluate(x_data, y_true, y_pred)\n        if self.pipeline is not None\n        else {}\n    )\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.fit","title":"<code>fit(x, y)</code>","text":"<p>Perform the grid search and fit the best pipeline.</p> <p>This method runs the grid search optimization process and fits the best found pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target values (if applicable).</p> required <p>Returns:</p> Type Description <p>None | float: None if the pipeline is fitted successfully, or the best score if available.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not defined before fitting.</p> Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def fit(self, x: XYData, y: Optional[XYData]):\n    \"\"\"\n    Perform the grid search and fit the best pipeline.\n\n    This method runs the grid search optimization process and fits the best found pipeline.\n\n    Args:\n        x (XYData): The input features.\n        y (Optional[XYData]): The target values (if applicable).\n\n    Returns:\n        None | float: None if the pipeline is fitted successfully, or the best score if available.\n\n    Raises:\n        ValueError: If the pipeline is not defined before fitting.\n    \"\"\"\n    if self.pipeline is None:\n        raise ValueError(\"Pipeline must be set before fitting\")\n\n    dumped_pipeline = self.pipeline.item_dump(include=[\"_grid\"])\n\n    self.get_grid(dumped_pipeline, self._grid)\n\n    print(self._grid)\n\n    results = []\n    combinations = self.nested_product(self._grid)\n\n    for param_dict in combinations:\n        self.get_from_grid(dumped_pipeline, param_dict)\n\n        pipeline: BaseFilter = cast(\n            BaseFilter, BasePlugin.build_from_dump(dumped_pipeline, Container.pif)\n        )\n\n        pipeline.verbose(False)\n\n        match pipeline.fit(x, y):\n            case None:\n                losses = pipeline.evaluate(x, y, pipeline.predict(x))\n\n                score = losses.get(self.scorer.__class__.__name__, 0.0)\n\n            case float() as score:\n                pass\n            case _:\n                raise ValueError(\"Unexpected return type from pipeline.fit()\")\n\n        results.append({**param_dict, \"score\": float(score)})\n\n    # Create DataFrame with all combinations and scores\n    self._results = pd.DataFrame(results)\n    self._results = self._results.sort_values(\n        \"score\", ascending=not self.scorer.higher_better\n    )\n\n    self.best_params = self._results.iloc[0].drop(\"score\").to_dict()\n\n    self.get_from_grid(dumped_pipeline, self.best_params)\n\n    self.pipeline = cast(\n        BaseFilter, BasePlugin.build_from_dump(dumped_pipeline, Container.pif)\n    )\n\n    return self.pipeline.unwrap().fit(x, y)\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.get_from_grid","title":"<code>get_from_grid(aux, config)</code>","text":"<p>Recursively process the grid configuration of a pipeline or filter.</p> <p>This method traverses the configuration dictionary and applies the grid parameters.</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to process.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>The configuration to apply.</p> required Note <p>This method modifies the input dictionary in-place.</p> Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def get_from_grid(self, aux: Dict[str, Any], config: Dict[str, Any]):\n    \"\"\"\n    Recursively process the grid configuration of a pipeline or filter.\n\n    This method traverses the configuration dictionary and applies the grid parameters.\n\n    Args:\n        aux (Dict[str, Any]): The configuration dictionary to process.\n        config (Dict[str, Any]): The configuration to apply.\n\n    Note:\n        This method modifies the input dictionary in-place.\n    \"\"\"\n    match aux[\"params\"]:\n        case {\"filters\": filters, **r}:\n            for filter_config in filters:\n                self.get_from_grid(filter_config, config)\n        case {\"pipeline\": pipeline, **r}:  # noqa: F841\n            self.get_from_grid(pipeline, config)\n        case {\"filter\": cached_filter, **r}:  # noqa: F841\n            self.get_from_grid(cached_filter, config)\n        case p_params:\n            if \"_grid\" in aux:\n                for param, value in aux[\"_grid\"].items():\n                    p_params.update({param: config[aux[\"clazz\"]][param]})\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.get_grid","title":"<code>get_grid(aux, grid)</code>","text":"<p>Recursively process the grid configuration of a pipeline or filter.</p> <p>This method traverses the configuration dictionary and extracts the grid parameters.</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to process.</p> required <code>grid</code> <code>Dict[str, Any]</code> <p>The grid to populate with hyperparameters.</p> required Note <p>This method modifies the input dictionary in-place.</p> Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def get_grid(self, aux: Dict[str, Any], grid: Dict[str, Any]):\n    \"\"\"\n    Recursively process the grid configuration of a pipeline or filter.\n\n    This method traverses the configuration dictionary and extracts the grid parameters.\n\n    Args:\n        aux (Dict[str, Any]): The configuration dictionary to process.\n        grid (Dict[str, Any]): The grid to populate with hyperparameters.\n\n    Note:\n        This method modifies the input dictionary in-place.\n    \"\"\"\n    match aux[\"params\"]:\n        case {\"filters\": filters, **r}:\n            for filter_config in filters:\n                self.get_grid(filter_config, grid)\n        case {\"pipeline\": pipeline, **r}:  # noqa: F841\n            self.get_grid(pipeline, grid)\n        case {\"filter\": cached_filter, **r}:  # noqa: F841\n            self.get_grid(cached_filter, grid)\n        case _:\n            if \"_grid\" in aux:\n                grid[aux[\"clazz\"]] = aux[\"_grid\"]\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.nested_product","title":"<code>nested_product(d)</code>","text":"Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def nested_product(self, d: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    def recurse(current) -&gt; List[Dict[str, Any]]:\n        if isinstance(current, dict):\n            keys, values = zip(*[(k, recurse(v)) for k, v in current.items()])\n            return [dict(zip(keys, v)) for v in itertools.product(*values)]\n        elif isinstance(current, list):\n            return current\n        else:\n            return [current]\n\n    return recurse(d)\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.optimize","title":"<code>optimize(pipeline)</code>","text":"<p>Set up the optimization process for a given pipeline.</p> <p>This method prepares the pipeline for grid search optimization.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be optimized.</p> required Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def optimize(self, pipeline: BaseFilter):\n    \"\"\"\n    Set up the optimization process for a given pipeline.\n\n    This method prepares the pipeline for grid search optimization.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be optimized.\n    \"\"\"\n    self.pipeline = pipeline\n    self.pipeline.verbose(False)\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the best pipeline found.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predictions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not fitted before predicting.</p> Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the best pipeline found.\n\n    Args:\n        x (XYData): The input features.\n\n    Returns:\n        XYData: The predictions.\n\n    Raises:\n        ValueError: If the pipeline is not fitted before predicting.\n    \"\"\"\n    if self.pipeline is not None:\n        return self.pipeline.predict(x)\n    else:\n        raise ValueError(\"Pipeline must be fitted before predicting\")\n</code></pre>"},{"location":"api/plugins/optimizers/grid_optimizer/#framework3.plugins.optimizer.grid_optimizer.GridOptimizer.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the pipeline execution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>Input data for fitting.</p> required <code>y</code> <code>XYData | None</code> <p>Target data for fitting.</p> required <code>X_</code> <code>XYData | None</code> <p>Data for prediction (if different from x).</p> required <p>Returns:</p> Type Description <code>XYData | None</code> <p>XYData | None: Prediction results if X_ is provided, else None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline has not been fitted.</p> Source code in <code>framework3/plugins/optimizer/grid_optimizer.py</code> <pre><code>def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n    \"\"\"\n    Start the pipeline execution.\n\n    Args:\n        x (XYData): Input data for fitting.\n        y (XYData | None): Target data for fitting.\n        X_ (XYData | None): Data for prediction (if different from x).\n\n    Returns:\n        XYData | None: Prediction results if X_ is provided, else None.\n\n    Raises:\n        ValueError: If the pipeline has not been fitted.\n    \"\"\"\n    if self.pipeline is not None:\n        return self.pipeline.start(x, y, X_)\n    else:\n        raise ValueError(\"Pipeline must be fitted before starting\")\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/","title":"OptunaOptimizer","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer","title":"<code>framework3.plugins.optimizer.optuna_optimizer</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer","title":"<code>OptunaOptimizer</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>Optuna-based optimizer for hyperparameter tuning.</p> <p>This class implements hyperparameter optimization using the Optuna framework. It allows for efficient searching of hyperparameter spaces for machine learning models.</p> Key Features <ul> <li>Supports various types of hyperparameters (categorical, integer, float)</li> <li>Allows for customizable optimization direction (minimize or maximize)</li> <li>Can resume previous studies or start new ones</li> <li>Integrates with the Framework3 pipeline system</li> </ul> Usage <p>The OptunaOptimizer can be used to optimize hyperparameters of a machine learning pipeline:</p> <pre><code>from framework3.plugins.optimizer import OptunaOptimizer\nfrom framework3.base import XYData\n\n# Assuming you have a pipeline and data\npipeline = ...\nx_data = XYData(...)\ny_data = XYData(...)\n\noptimizer = OptunaOptimizer(direction=\"minimize\", n_trials=100)\noptimizer.optimize(pipeline)\noptimizer.fit(x_data, y_data)\n\nbest_pipeline = optimizer.pipeline\n</code></pre> <p>Attributes:</p> Name Type Description <code>direction</code> <code>str</code> <p>The direction of optimization (\"minimize\" or \"maximize\").</p> <code>n_trials</code> <code>int</code> <p>The number of trials for the optimization process.</p> <code>load_if_exists</code> <code>bool</code> <p>Whether to load an existing study if one exists.</p> <code>reset_study</code> <code>bool</code> <p>Whether to reset the study before optimization.</p> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be optimized.</p> <code>study_name</code> <code>str | None</code> <p>The name of the Optuna study.</p> <code>storage</code> <code>str | None</code> <p>The storage URL for the Optuna study.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>BaseFilter): Set up the optimization process for a given pipeline.</p> <code>fit</code> <p>XYData, y: XYData | None): Perform the optimization and fit the best pipeline.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the optimized pipeline.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Evaluate the optimized pipeline.</p> Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>@Container.bind()\nclass OptunaOptimizer(BaseOptimizer):\n    \"\"\"\n    Optuna-based optimizer for hyperparameter tuning.\n\n    This class implements hyperparameter optimization using the Optuna framework.\n    It allows for efficient searching of hyperparameter spaces for machine learning models.\n\n    Key Features:\n        - Supports various types of hyperparameters (categorical, integer, float)\n        - Allows for customizable optimization direction (minimize or maximize)\n        - Can resume previous studies or start new ones\n        - Integrates with the Framework3 pipeline system\n\n    Usage:\n        The OptunaOptimizer can be used to optimize hyperparameters of a machine learning pipeline:\n\n        ```python\n        from framework3.plugins.optimizer import OptunaOptimizer\n        from framework3.base import XYData\n\n        # Assuming you have a pipeline and data\n        pipeline = ...\n        x_data = XYData(...)\n        y_data = XYData(...)\n\n        optimizer = OptunaOptimizer(direction=\"minimize\", n_trials=100)\n        optimizer.optimize(pipeline)\n        optimizer.fit(x_data, y_data)\n\n        best_pipeline = optimizer.pipeline\n        ```\n\n    Attributes:\n        direction (str): The direction of optimization (\"minimize\" or \"maximize\").\n        n_trials (int): The number of trials for the optimization process.\n        load_if_exists (bool): Whether to load an existing study if one exists.\n        reset_study (bool): Whether to reset the study before optimization.\n        pipeline (BaseFilter | None): The pipeline to be optimized.\n        study_name (str | None): The name of the Optuna study.\n        storage (str | None): The storage URL for the Optuna study.\n\n    Methods:\n        optimize(pipeline: BaseFilter): Set up the optimization process for a given pipeline.\n        fit(x: XYData, y: XYData | None): Perform the optimization and fit the best pipeline.\n        predict(x: XYData) -&gt; XYData: Make predictions using the optimized pipeline.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Evaluate the optimized pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        direction: str,\n        n_trials: int = 2,\n        load_if_exists: bool = False,\n        reset_study: bool = False,\n        pipeline: BaseFilter | None = None,\n        study_name: str | None = None,\n        storage: str | None = None,\n    ):\n        \"\"\"\n        Initialize the OptunaOptimizer.\n\n        Args:\n            direction (str): The direction of optimization (\"minimize\" or \"maximize\").\n            n_trials (int): The number of trials for the optimization process.\n            load_if_exists (bool): Whether to load an existing study if one exists.\n            reset_study (bool): Whether to reset the study before optimization.\n            pipeline (BaseFilter | None): The pipeline to be optimized.\n            study_name (str | None): The name of the Optuna study.\n            storage (str | None): The storage URL for the Optuna study.\n        \"\"\"\n        super().__init__(direction=direction, study_name=study_name, storage=storage)\n        self.direction = direction\n        self.study_name = study_name\n        self.storage = storage\n        self.pipeline = pipeline\n        self.n_trials = n_trials\n        self.load_if_exists = load_if_exists\n        self.reset_study = reset_study\n\n    def optimize(self, pipeline: BaseFilter):\n        \"\"\"\n        Set up the optimization process for a given pipeline.\n\n        This method prepares the Optuna study for optimization.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be optimized.\n        \"\"\"\n        self.pipeline = pipeline\n        self.pipeline._verbose = False\n\n        if (\n            self.reset_study\n            and self.study_name is not None\n            and self.storage is not None\n        ):\n            studies = optuna.get_all_study_summaries(storage=self.storage)\n\n            if self.study_name in [study.study_name for study in studies]:\n                optuna.delete_study(study_name=self.study_name, storage=self.storage)\n\n        self._study = optuna.create_study(\n            study_name=self.study_name,\n            direction=self.direction,\n            storage=self.storage,\n            load_if_exists=self.load_if_exists,\n        )\n\n    def get_grid(self, aux: Dict[str, Any], f: Callable):\n        \"\"\"\n        Recursively process the grid configuration of a pipeline or filter.\n\n        This method traverses the configuration dictionary and applies the provided\n        callable to each grid parameter.\n\n        Args:\n            aux (Dict[str, Any]): The configuration dictionary to process.\n            f (Callable): A function to apply to each grid parameter.\n\n        Note:\n            This method modifies the input dictionary in-place.\n        \"\"\"\n        match aux[\"params\"]:\n            case {\"filters\": filters, **r}:\n                for filter_config in filters:\n                    self.get_grid(filter_config, f)\n            case {\"pipeline\": pipeline, **r}:  # noqa: F841\n                self.get_grid(pipeline, f)\n            case {\"filter\": cached_filter, **r}:  # noqa: F841\n                self.get_grid(cached_filter, f)\n            case p_params:\n                if \"_grid\" in aux:\n                    for param, value in aux[\"_grid\"].items():\n                        value = f(param, value)\n                        p_params.update({param: value})\n\n    def build_pipeline(\n        self, dumped_pipeline: Dict[str, Any], f: Callable\n    ) -&gt; BaseFilter:\n        \"\"\"\n        Build a pipeline from a dumped configuration.\n\n        This method processes the dumped pipeline configuration, applies the provided\n        callable to the grid parameters, and constructs a new BaseFilter object.\n\n        Args:\n            dumped_pipeline (Dict[str, Any]): The dumped pipeline configuration.\n            f (Callable): A function to apply to each grid parameter.\n\n        Returns:\n            BaseFilter: The constructed pipeline.\n\n        Note:\n            This method uses the Container.pif for dependency injection when building\n            the pipeline components.\n        \"\"\"\n        self.get_grid(dumped_pipeline, f)\n\n        pipeline: BaseFilter = cast(\n            BaseFilter, BasePlugin.build_from_dump(dumped_pipeline, Container.pif)\n        )\n        return pipeline\n\n    def fit(self, x: XYData, y: XYData | None = None):\n        \"\"\"\n        Perform the optimization and fit the best pipeline.\n\n        This method runs the Optuna optimization process and fits the best found pipeline.\n\n        Args:\n            x (XYData): The input features.\n            y (XYData | None): The target values (if applicable).\n\n        Raises:\n            ValueError: If the pipeline is not defined before fitting.\n        \"\"\"\n        self._print_acction(\"Fitting with OptunaOptimizer...\")\n        if self._verbose:\n            print(self.pipeline)\n\n        if self.pipeline is not None:\n            dumped_pipeline = self.pipeline.item_dump(include=[\"_grid\"])\n            print(dumped_pipeline)\n\n            def objective(trial) -&gt; Union[float, Sequence[float]]:\n                def matcher(k, v):\n                    match v:\n                        case list():\n                            return trial.suggest_categorical(k, v)\n                        case dict():\n                            if type(v[\"low\"]) is int and type(v[\"high\"]) is int:\n                                return trial.suggest_int(k, v[\"low\"], v[\"high\"])\n                            elif type(v[\"low\"]) is float and type(v[\"high\"]) is float:\n                                return trial.suggest_float(k, v[\"low\"], v[\"high\"])\n                            else:\n                                raise ValueError(\n                                    f\"Inconsistent types in tuple: {k}: {v}\"\n                                )\n                        case (min_v, max_v):\n                            if type(min_v) is int and type(max_v) is int:\n                                return trial.suggest_int(k, min_v, max_v)\n                            elif type(min_v) is float and type(max_v) is float:\n                                return trial.suggest_float(k, min_v, max_v)\n                            else:\n                                raise ValueError(\n                                    f\"Inconsistent types in tuple: {k}: {v}\"\n                                )\n                        case _:\n                            raise ValueError(f\"Unsupported type in grid: {k}: {v}\")\n\n                pipeline: BaseFilter = self.build_pipeline(dumped_pipeline, matcher)\n                pipeline.verbose(False)\n\n                match pipeline.fit(x, y):\n                    case None:\n                        return float(\n                            next(\n                                iter(\n                                    pipeline.evaluate(\n                                        x, y, pipeline.predict(x)\n                                    ).values()\n                                )\n                            )\n                        )\n                    case float() as loss:\n                        return loss\n                    case _:\n                        raise ValueError(\"Unsupported type in pipeline.fit\")\n\n            self._study.optimize(\n                objective, n_trials=self.n_trials, show_progress_bar=True\n            )\n\n            best_params = self._study.best_params\n            if best_params:\n                print(f\"Best params: {best_params}\")\n                pipeline = self.build_pipeline(\n                    dumped_pipeline, lambda k, _: best_params[k]\n                ).unwrap()\n                pipeline.fit(x, y)\n                self.pipeline = pipeline\n            else:\n                self.pipeline.unwrap().fit(x, y)\n        else:\n            raise ValueError(\"Pipeline must be defined before fitting\")\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the optimized pipeline.\n\n        Args:\n            x (XYData): The input features.\n\n        Returns:\n            XYData: The predictions.\n\n        Raises:\n            ValueError: If the pipeline is not fitted before predicting.\n        \"\"\"\n        self._print_acction(\"Predicting with OptunaOptimizer...\")\n        if self._verbose:\n            print(self.pipeline)\n\n        if self.pipeline is not None:\n            return self.pipeline.predict(x)\n        else:\n            raise ValueError(\"Pipeline must be fitted before predicting\")\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the optimized pipeline.\n\n        Args:\n            x_data (XYData): The input features.\n            y_true (XYData | None): The true target values (if applicable).\n            y_pred (XYData): The predicted values.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing evaluation metrics.\n\n        Raises:\n            ValueError: If the pipeline is not fitted before evaluating.\n        \"\"\"\n        if self.pipeline is not None:\n            return self.pipeline.evaluate(x_data, y_true, y_pred)\n        else:\n            raise ValueError(\"Pipeline must be fitted before evaluating\")\n\n    def start(\n        self, x: XYData, y: XYData | None, X_: XYData | None\n    ) -&gt; XYData | None: ...\n\n    def finish(self) -&gt; None: ...\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.direction","title":"<code>direction = direction</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.load_if_exists","title":"<code>load_if_exists = load_if_exists</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.n_trials","title":"<code>n_trials = n_trials</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.pipeline","title":"<code>pipeline = pipeline</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.reset_study","title":"<code>reset_study = reset_study</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.storage","title":"<code>storage = storage</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.study_name","title":"<code>study_name = study_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.__init__","title":"<code>__init__(direction, n_trials=2, load_if_exists=False, reset_study=False, pipeline=None, study_name=None, storage=None)</code>","text":"<p>Initialize the OptunaOptimizer.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>str</code> <p>The direction of optimization (\"minimize\" or \"maximize\").</p> required <code>n_trials</code> <code>int</code> <p>The number of trials for the optimization process.</p> <code>2</code> <code>load_if_exists</code> <code>bool</code> <p>Whether to load an existing study if one exists.</p> <code>False</code> <code>reset_study</code> <code>bool</code> <p>Whether to reset the study before optimization.</p> <code>False</code> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be optimized.</p> <code>None</code> <code>study_name</code> <code>str | None</code> <p>The name of the Optuna study.</p> <code>None</code> <code>storage</code> <code>str | None</code> <p>The storage URL for the Optuna study.</p> <code>None</code> Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def __init__(\n    self,\n    direction: str,\n    n_trials: int = 2,\n    load_if_exists: bool = False,\n    reset_study: bool = False,\n    pipeline: BaseFilter | None = None,\n    study_name: str | None = None,\n    storage: str | None = None,\n):\n    \"\"\"\n    Initialize the OptunaOptimizer.\n\n    Args:\n        direction (str): The direction of optimization (\"minimize\" or \"maximize\").\n        n_trials (int): The number of trials for the optimization process.\n        load_if_exists (bool): Whether to load an existing study if one exists.\n        reset_study (bool): Whether to reset the study before optimization.\n        pipeline (BaseFilter | None): The pipeline to be optimized.\n        study_name (str | None): The name of the Optuna study.\n        storage (str | None): The storage URL for the Optuna study.\n    \"\"\"\n    super().__init__(direction=direction, study_name=study_name, storage=storage)\n    self.direction = direction\n    self.study_name = study_name\n    self.storage = storage\n    self.pipeline = pipeline\n    self.n_trials = n_trials\n    self.load_if_exists = load_if_exists\n    self.reset_study = reset_study\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.build_pipeline","title":"<code>build_pipeline(dumped_pipeline, f)</code>","text":"<p>Build a pipeline from a dumped configuration.</p> <p>This method processes the dumped pipeline configuration, applies the provided callable to the grid parameters, and constructs a new BaseFilter object.</p> <p>Parameters:</p> Name Type Description Default <code>dumped_pipeline</code> <code>Dict[str, Any]</code> <p>The dumped pipeline configuration.</p> required <code>f</code> <code>Callable</code> <p>A function to apply to each grid parameter.</p> required <p>Returns:</p> Name Type Description <code>BaseFilter</code> <code>BaseFilter</code> <p>The constructed pipeline.</p> Note <p>This method uses the Container.pif for dependency injection when building the pipeline components.</p> Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def build_pipeline(\n    self, dumped_pipeline: Dict[str, Any], f: Callable\n) -&gt; BaseFilter:\n    \"\"\"\n    Build a pipeline from a dumped configuration.\n\n    This method processes the dumped pipeline configuration, applies the provided\n    callable to the grid parameters, and constructs a new BaseFilter object.\n\n    Args:\n        dumped_pipeline (Dict[str, Any]): The dumped pipeline configuration.\n        f (Callable): A function to apply to each grid parameter.\n\n    Returns:\n        BaseFilter: The constructed pipeline.\n\n    Note:\n        This method uses the Container.pif for dependency injection when building\n        the pipeline components.\n    \"\"\"\n    self.get_grid(dumped_pipeline, f)\n\n    pipeline: BaseFilter = cast(\n        BaseFilter, BasePlugin.build_from_dump(dumped_pipeline, Container.pif)\n    )\n    return pipeline\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the optimized pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input features.</p> required <code>y_true</code> <code>XYData | None</code> <p>The true target values (if applicable).</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing evaluation metrics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not fitted before evaluating.</p> Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the optimized pipeline.\n\n    Args:\n        x_data (XYData): The input features.\n        y_true (XYData | None): The true target values (if applicable).\n        y_pred (XYData): The predicted values.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing evaluation metrics.\n\n    Raises:\n        ValueError: If the pipeline is not fitted before evaluating.\n    \"\"\"\n    if self.pipeline is not None:\n        return self.pipeline.evaluate(x_data, y_true, y_pred)\n    else:\n        raise ValueError(\"Pipeline must be fitted before evaluating\")\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.finish","title":"<code>finish()</code>","text":"Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def finish(self) -&gt; None: ...\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.fit","title":"<code>fit(x, y=None)</code>","text":"<p>Perform the optimization and fit the best pipeline.</p> <p>This method runs the Optuna optimization process and fits the best found pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>XYData | None</code> <p>The target values (if applicable).</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not defined before fitting.</p> Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def fit(self, x: XYData, y: XYData | None = None):\n    \"\"\"\n    Perform the optimization and fit the best pipeline.\n\n    This method runs the Optuna optimization process and fits the best found pipeline.\n\n    Args:\n        x (XYData): The input features.\n        y (XYData | None): The target values (if applicable).\n\n    Raises:\n        ValueError: If the pipeline is not defined before fitting.\n    \"\"\"\n    self._print_acction(\"Fitting with OptunaOptimizer...\")\n    if self._verbose:\n        print(self.pipeline)\n\n    if self.pipeline is not None:\n        dumped_pipeline = self.pipeline.item_dump(include=[\"_grid\"])\n        print(dumped_pipeline)\n\n        def objective(trial) -&gt; Union[float, Sequence[float]]:\n            def matcher(k, v):\n                match v:\n                    case list():\n                        return trial.suggest_categorical(k, v)\n                    case dict():\n                        if type(v[\"low\"]) is int and type(v[\"high\"]) is int:\n                            return trial.suggest_int(k, v[\"low\"], v[\"high\"])\n                        elif type(v[\"low\"]) is float and type(v[\"high\"]) is float:\n                            return trial.suggest_float(k, v[\"low\"], v[\"high\"])\n                        else:\n                            raise ValueError(\n                                f\"Inconsistent types in tuple: {k}: {v}\"\n                            )\n                    case (min_v, max_v):\n                        if type(min_v) is int and type(max_v) is int:\n                            return trial.suggest_int(k, min_v, max_v)\n                        elif type(min_v) is float and type(max_v) is float:\n                            return trial.suggest_float(k, min_v, max_v)\n                        else:\n                            raise ValueError(\n                                f\"Inconsistent types in tuple: {k}: {v}\"\n                            )\n                    case _:\n                        raise ValueError(f\"Unsupported type in grid: {k}: {v}\")\n\n            pipeline: BaseFilter = self.build_pipeline(dumped_pipeline, matcher)\n            pipeline.verbose(False)\n\n            match pipeline.fit(x, y):\n                case None:\n                    return float(\n                        next(\n                            iter(\n                                pipeline.evaluate(\n                                    x, y, pipeline.predict(x)\n                                ).values()\n                            )\n                        )\n                    )\n                case float() as loss:\n                    return loss\n                case _:\n                    raise ValueError(\"Unsupported type in pipeline.fit\")\n\n        self._study.optimize(\n            objective, n_trials=self.n_trials, show_progress_bar=True\n        )\n\n        best_params = self._study.best_params\n        if best_params:\n            print(f\"Best params: {best_params}\")\n            pipeline = self.build_pipeline(\n                dumped_pipeline, lambda k, _: best_params[k]\n            ).unwrap()\n            pipeline.fit(x, y)\n            self.pipeline = pipeline\n        else:\n            self.pipeline.unwrap().fit(x, y)\n    else:\n        raise ValueError(\"Pipeline must be defined before fitting\")\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.get_grid","title":"<code>get_grid(aux, f)</code>","text":"<p>Recursively process the grid configuration of a pipeline or filter.</p> <p>This method traverses the configuration dictionary and applies the provided callable to each grid parameter.</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to process.</p> required <code>f</code> <code>Callable</code> <p>A function to apply to each grid parameter.</p> required Note <p>This method modifies the input dictionary in-place.</p> Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def get_grid(self, aux: Dict[str, Any], f: Callable):\n    \"\"\"\n    Recursively process the grid configuration of a pipeline or filter.\n\n    This method traverses the configuration dictionary and applies the provided\n    callable to each grid parameter.\n\n    Args:\n        aux (Dict[str, Any]): The configuration dictionary to process.\n        f (Callable): A function to apply to each grid parameter.\n\n    Note:\n        This method modifies the input dictionary in-place.\n    \"\"\"\n    match aux[\"params\"]:\n        case {\"filters\": filters, **r}:\n            for filter_config in filters:\n                self.get_grid(filter_config, f)\n        case {\"pipeline\": pipeline, **r}:  # noqa: F841\n            self.get_grid(pipeline, f)\n        case {\"filter\": cached_filter, **r}:  # noqa: F841\n            self.get_grid(cached_filter, f)\n        case p_params:\n            if \"_grid\" in aux:\n                for param, value in aux[\"_grid\"].items():\n                    value = f(param, value)\n                    p_params.update({param: value})\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.optimize","title":"<code>optimize(pipeline)</code>","text":"<p>Set up the optimization process for a given pipeline.</p> <p>This method prepares the Optuna study for optimization.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be optimized.</p> required Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def optimize(self, pipeline: BaseFilter):\n    \"\"\"\n    Set up the optimization process for a given pipeline.\n\n    This method prepares the Optuna study for optimization.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be optimized.\n    \"\"\"\n    self.pipeline = pipeline\n    self.pipeline._verbose = False\n\n    if (\n        self.reset_study\n        and self.study_name is not None\n        and self.storage is not None\n    ):\n        studies = optuna.get_all_study_summaries(storage=self.storage)\n\n        if self.study_name in [study.study_name for study in studies]:\n            optuna.delete_study(study_name=self.study_name, storage=self.storage)\n\n    self._study = optuna.create_study(\n        study_name=self.study_name,\n        direction=self.direction,\n        storage=self.storage,\n        load_if_exists=self.load_if_exists,\n    )\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the optimized pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predictions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not fitted before predicting.</p> Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the optimized pipeline.\n\n    Args:\n        x (XYData): The input features.\n\n    Returns:\n        XYData: The predictions.\n\n    Raises:\n        ValueError: If the pipeline is not fitted before predicting.\n    \"\"\"\n    self._print_acction(\"Predicting with OptunaOptimizer...\")\n    if self._verbose:\n        print(self.pipeline)\n\n    if self.pipeline is not None:\n        return self.pipeline.predict(x)\n    else:\n        raise ValueError(\"Pipeline must be fitted before predicting\")\n</code></pre>"},{"location":"api/plugins/optimizers/optuna_optimizer/#framework3.plugins.optimizer.optuna_optimizer.OptunaOptimizer.start","title":"<code>start(x, y, X_)</code>","text":"Source code in <code>framework3/plugins/optimizer/optuna_optimizer.py</code> <pre><code>def start(\n    self, x: XYData, y: XYData | None, X_: XYData | None\n) -&gt; XYData | None: ...\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/","title":"SklearnOptimizer","text":""},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer","title":"<code>framework3.plugins.optimizer.sklearn_optimizer</code>","text":""},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.__all__","title":"<code>__all__ = ['SklearnOptimizer']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer","title":"<code>SklearnOptimizer</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>Sklearn-based optimizer for hyperparameter tuning using GridSearchCV.</p> <p>This class implements hyperparameter optimization using scikit-learn's GridSearchCV. It allows for efficient searching of hyperparameter spaces for machine learning models within the Framework3 pipeline system.</p> Key Features <ul> <li>Supports various types of hyperparameters (categorical, numerical)</li> <li>Integrates with scikit-learn's GridSearchCV for exhaustive search</li> <li>Allows for customizable scoring metrics</li> <li>Integrates with the Framework3 pipeline system</li> </ul> Usage <p>The SklearnOptimizer can be used to optimize hyperparameters of a machine learning pipeline:</p> <pre><code>from framework3.plugins.optimizer import SklearnOptimizer\nfrom framework3.base import XYData\n\n# Assuming you have a pipeline and data\npipeline = ...\nx_data = XYData(...)\ny_data = XYData(...)\n\noptimizer = SklearnOptimizer(scoring='accuracy', cv=5)\noptimizer.optimize(pipeline)\noptimizer.fit(x_data, y_data)\n\nbest_pipeline = optimizer.pipeline\n</code></pre> <p>Attributes:</p> Name Type Description <code>scoring</code> <code>str | Callable | Tuple | Dict</code> <p>The scoring metric for GridSearchCV.</p> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be optimized.</p> <code>cv</code> <code>int</code> <p>The number of cross-validation folds.</p> <code>_grid</code> <code>Dict</code> <p>The parameter grid for GridSearchCV.</p> <code>_filters</code> <code>List[Tuple[str, SkWrapper]]</code> <p>The list of pipeline steps.</p> <code>_pipeline</code> <code>Pipeline</code> <p>The scikit-learn Pipeline object.</p> <code>_clf</code> <code>GridSearchCV</code> <p>The GridSearchCV object.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>BaseFilter): Set up the optimization process for a given pipeline.</p> <code>fit</code> <p>XYData, y: Optional[XYData]) -&gt; None | float: Fit the GridSearchCV object to the given data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the best estimator found by GridSearchCV.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Evaluate the optimized pipeline.</p> Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>@Container.bind()\nclass SklearnOptimizer(BaseOptimizer):\n    \"\"\"\n    Sklearn-based optimizer for hyperparameter tuning using GridSearchCV.\n\n    This class implements hyperparameter optimization using scikit-learn's GridSearchCV.\n    It allows for efficient searching of hyperparameter spaces for machine learning models\n    within the Framework3 pipeline system.\n\n    Key Features:\n        - Supports various types of hyperparameters (categorical, numerical)\n        - Integrates with scikit-learn's GridSearchCV for exhaustive search\n        - Allows for customizable scoring metrics\n        - Integrates with the Framework3 pipeline system\n\n    Usage:\n        The SklearnOptimizer can be used to optimize hyperparameters of a machine learning pipeline:\n\n        ```python\n        from framework3.plugins.optimizer import SklearnOptimizer\n        from framework3.base import XYData\n\n        # Assuming you have a pipeline and data\n        pipeline = ...\n        x_data = XYData(...)\n        y_data = XYData(...)\n\n        optimizer = SklearnOptimizer(scoring='accuracy', cv=5)\n        optimizer.optimize(pipeline)\n        optimizer.fit(x_data, y_data)\n\n        best_pipeline = optimizer.pipeline\n        ```\n\n    Attributes:\n        scoring (str | Callable | Tuple | Dict): The scoring metric for GridSearchCV.\n        pipeline (BaseFilter | None): The pipeline to be optimized.\n        cv (int): The number of cross-validation folds.\n        _grid (Dict): The parameter grid for GridSearchCV.\n        _filters (List[Tuple[str, SkWrapper]]): The list of pipeline steps.\n        _pipeline (Pipeline): The scikit-learn Pipeline object.\n        _clf (GridSearchCV): The GridSearchCV object.\n\n    Methods:\n        optimize(pipeline: BaseFilter): Set up the optimization process for a given pipeline.\n        fit(x: XYData, y: Optional[XYData]) -&gt; None | float: Fit the GridSearchCV object to the given data.\n        predict(x: XYData) -&gt; XYData: Make predictions using the best estimator found by GridSearchCV.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Evaluate the optimized pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        scoring: str | Callable | Tuple | Dict,\n        pipeline: BaseFilter | None = None,\n        cv: int = 2,\n        n_jobs: int | None = None,\n    ):\n        \"\"\"\n        Initialize the SklearnOptimizer.\n\n        Args:\n            scoring (str | Callable | Tuple | Dict): Strategy to evaluate the performance of the cross-validated model.\n            pipeline (BaseFilter | None): The pipeline to be optimized. Defaults to None.\n            cv (int): Determines the cross-validation splitting strategy. Defaults to 2.\n        \"\"\"\n\n        super().__init__(\n            scoring=scoring,\n            cv=cv,\n            pipeline=pipeline,\n        )\n        self.pipeline = pipeline\n        self.n_jobs = n_jobs\n        self._grid = {}\n\n    def get_grid(self, aux: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Recursively process the grid configuration of a pipeline or filter.\n\n        This method traverses the configuration dictionary and builds the parameter grid\n        for GridSearchCV.\n\n        Args:\n            aux (Dict[str, Any]): The configuration dictionary to process.\n\n        Note:\n            This method modifies the _grid attribute in-place.\n        \"\"\"\n        match aux[\"params\"]:\n            case {\"filters\": filters, **r}:\n                for filter_config in filters:\n                    self.get_grid(filter_config)\n            case {\"pipeline\": pipeline, **r}:  # noqa: F841\n                self.get_grid(pipeline)\n            case _:\n                if \"_grid\" in aux:\n                    for param, value in aux[\"_grid\"].items():\n                        if type(value) is list:\n                            self._grid[f'{aux[\"clazz\"]}__{param}'] = value\n                        else:\n                            self._grid[f'{aux[\"clazz\"]}__{param}'] = [value]\n\n    def optimize(self, pipeline: BaseFilter):\n        \"\"\"\n        Set up the optimization process for a given pipeline.\n\n        This method prepares the GridSearchCV object for optimization.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be optimized.\n        \"\"\"\n        self.pipeline = pipeline\n        self.pipeline.verbose(False)\n        self._filters = list(\n            map(lambda x: (x.__name__, SkWrapper(x)), self.pipeline.get_types())\n        )\n\n        dumped_pipeline = self.pipeline.item_dump(include=[\"_grid\"])\n        self.get_grid(dumped_pipeline)\n\n        self._pipeline = Pipeline(self._filters)\n\n        self._clf: GridSearchCV = GridSearchCV(\n            estimator=self._pipeline,\n            param_grid=self._grid,\n            scoring=self.scoring,\n            cv=self.cv,\n            n_jobs=self.n_jobs,\n            verbose=10,\n        )\n\n    def start(\n        self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n    ) -&gt; Optional[XYData]:\n        \"\"\"\n        Start the pipeline execution.\n\n        This method fits the optimizer and makes predictions if X_ is provided.\n\n        Args:\n            x (XYData): Input data for fitting.\n            y (Optional[XYData]): Target data for fitting.\n            X_ (Optional[XYData]): Data for prediction (if different from x).\n\n        Returns:\n            Optional[XYData]: Prediction results if X_ is provided, else None.\n\n        Raises:\n            Exception: If an error occurs during pipeline execution.\n        \"\"\"\n        try:\n            self.fit(x, y)\n            if X_ is not None:\n                return self.predict(X_)\n            else:\n                return self.predict(x)\n        except Exception as e:\n            print(f\"Error during pipeline execution: {e}\")\n            raise e\n\n    def fit(self, x: XYData, y: Optional[XYData]) -&gt; None | float:\n        \"\"\"\n        Fit the GridSearchCV object to the given data.\n\n        This method performs the grid search and prints the results.\n\n        Args:\n            x (XYData): The input features.\n            y (Optional[XYData]): The target values.\n\n        Returns:\n            None | float: The best score achieved during the grid search.\n        \"\"\"\n        self._clf.fit(x.value, y.value if y is not None else None)\n        results = self._clf.cv_results_\n        results_df = (\n            pd.DataFrame(results)\n            .iloc[:, 4:]\n            .sort_values(\"mean_test_score\", ascending=False)\n        )\n        print(results_df)\n        return self._clf.best_score_  # type: ignore\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the best estimator found by GridSearchCV.\n\n        Args:\n            x (XYData): The input features.\n\n        Returns:\n            XYData: The predicted values wrapped in an XYData object.\n        \"\"\"\n        return XYData.mock(self._clf.predict(x.value))  # type: ignore\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the optimized pipeline.\n\n        This method applies each metric in the pipeline to the predicted and true values,\n        and includes the best score from GridSearchCV.\n\n        Args:\n            x_data (XYData): Input data.\n            y_true (XYData | None): True target data.\n            y_pred (XYData): Predicted target data.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation results for each metric\n                            and the best score from GridSearchCV.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; evaluation = optimizer.evaluate(x_test, y_test, predictions)\n            &gt;&gt;&gt; print(evaluation)\n            {'F1Score': 0.85, 'best_score': 0.87}\n            ```\n        \"\"\"\n        if self.pipeline is None:\n            raise Exception(\"No pipeline set for evaluation.\")\n\n        results = self.pipeline.evaluate(x_data, y_true, y_pred)\n        results[\"best_score\"] = self._clf.best_score_  # type: ignore\n        return results\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.n_jobs","title":"<code>n_jobs = n_jobs</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.pipeline","title":"<code>pipeline = pipeline</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.__init__","title":"<code>__init__(scoring, pipeline=None, cv=2, n_jobs=None)</code>","text":"<p>Initialize the SklearnOptimizer.</p> <p>Parameters:</p> Name Type Description Default <code>scoring</code> <code>str | Callable | Tuple | Dict</code> <p>Strategy to evaluate the performance of the cross-validated model.</p> required <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be optimized. Defaults to None.</p> <code>None</code> <code>cv</code> <code>int</code> <p>Determines the cross-validation splitting strategy. Defaults to 2.</p> <code>2</code> Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>def __init__(\n    self,\n    scoring: str | Callable | Tuple | Dict,\n    pipeline: BaseFilter | None = None,\n    cv: int = 2,\n    n_jobs: int | None = None,\n):\n    \"\"\"\n    Initialize the SklearnOptimizer.\n\n    Args:\n        scoring (str | Callable | Tuple | Dict): Strategy to evaluate the performance of the cross-validated model.\n        pipeline (BaseFilter | None): The pipeline to be optimized. Defaults to None.\n        cv (int): Determines the cross-validation splitting strategy. Defaults to 2.\n    \"\"\"\n\n    super().__init__(\n        scoring=scoring,\n        cv=cv,\n        pipeline=pipeline,\n    )\n    self.pipeline = pipeline\n    self.n_jobs = n_jobs\n    self._grid = {}\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the optimized pipeline.</p> <p>This method applies each metric in the pipeline to the predicted and true values, and includes the best score from GridSearchCV.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>Input data.</p> required <code>y_true</code> <code>XYData | None</code> <p>True target data.</p> required <code>y_pred</code> <code>XYData</code> <p>Predicted target data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation results for each metric             and the best score from GridSearchCV.</p> Example <pre><code>&gt;&gt;&gt; evaluation = optimizer.evaluate(x_test, y_test, predictions)\n&gt;&gt;&gt; print(evaluation)\n{'F1Score': 0.85, 'best_score': 0.87}\n</code></pre> Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the optimized pipeline.\n\n    This method applies each metric in the pipeline to the predicted and true values,\n    and includes the best score from GridSearchCV.\n\n    Args:\n        x_data (XYData): Input data.\n        y_true (XYData | None): True target data.\n        y_pred (XYData): Predicted target data.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results for each metric\n                        and the best score from GridSearchCV.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; evaluation = optimizer.evaluate(x_test, y_test, predictions)\n        &gt;&gt;&gt; print(evaluation)\n        {'F1Score': 0.85, 'best_score': 0.87}\n        ```\n    \"\"\"\n    if self.pipeline is None:\n        raise Exception(\"No pipeline set for evaluation.\")\n\n    results = self.pipeline.evaluate(x_data, y_true, y_pred)\n    results[\"best_score\"] = self._clf.best_score_  # type: ignore\n    return results\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.fit","title":"<code>fit(x, y)</code>","text":"<p>Fit the GridSearchCV object to the given data.</p> <p>This method performs the grid search and prints the results.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>None | float</code> <p>None | float: The best score achieved during the grid search.</p> Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>def fit(self, x: XYData, y: Optional[XYData]) -&gt; None | float:\n    \"\"\"\n    Fit the GridSearchCV object to the given data.\n\n    This method performs the grid search and prints the results.\n\n    Args:\n        x (XYData): The input features.\n        y (Optional[XYData]): The target values.\n\n    Returns:\n        None | float: The best score achieved during the grid search.\n    \"\"\"\n    self._clf.fit(x.value, y.value if y is not None else None)\n    results = self._clf.cv_results_\n    results_df = (\n        pd.DataFrame(results)\n        .iloc[:, 4:]\n        .sort_values(\"mean_test_score\", ascending=False)\n    )\n    print(results_df)\n    return self._clf.best_score_  # type: ignore\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.get_grid","title":"<code>get_grid(aux)</code>","text":"<p>Recursively process the grid configuration of a pipeline or filter.</p> <p>This method traverses the configuration dictionary and builds the parameter grid for GridSearchCV.</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to process.</p> required Note <p>This method modifies the _grid attribute in-place.</p> Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>def get_grid(self, aux: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Recursively process the grid configuration of a pipeline or filter.\n\n    This method traverses the configuration dictionary and builds the parameter grid\n    for GridSearchCV.\n\n    Args:\n        aux (Dict[str, Any]): The configuration dictionary to process.\n\n    Note:\n        This method modifies the _grid attribute in-place.\n    \"\"\"\n    match aux[\"params\"]:\n        case {\"filters\": filters, **r}:\n            for filter_config in filters:\n                self.get_grid(filter_config)\n        case {\"pipeline\": pipeline, **r}:  # noqa: F841\n            self.get_grid(pipeline)\n        case _:\n            if \"_grid\" in aux:\n                for param, value in aux[\"_grid\"].items():\n                    if type(value) is list:\n                        self._grid[f'{aux[\"clazz\"]}__{param}'] = value\n                    else:\n                        self._grid[f'{aux[\"clazz\"]}__{param}'] = [value]\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.optimize","title":"<code>optimize(pipeline)</code>","text":"<p>Set up the optimization process for a given pipeline.</p> <p>This method prepares the GridSearchCV object for optimization.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be optimized.</p> required Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>def optimize(self, pipeline: BaseFilter):\n    \"\"\"\n    Set up the optimization process for a given pipeline.\n\n    This method prepares the GridSearchCV object for optimization.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be optimized.\n    \"\"\"\n    self.pipeline = pipeline\n    self.pipeline.verbose(False)\n    self._filters = list(\n        map(lambda x: (x.__name__, SkWrapper(x)), self.pipeline.get_types())\n    )\n\n    dumped_pipeline = self.pipeline.item_dump(include=[\"_grid\"])\n    self.get_grid(dumped_pipeline)\n\n    self._pipeline = Pipeline(self._filters)\n\n    self._clf: GridSearchCV = GridSearchCV(\n        estimator=self._pipeline,\n        param_grid=self._grid,\n        scoring=self.scoring,\n        cv=self.cv,\n        n_jobs=self.n_jobs,\n        verbose=10,\n    )\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the best estimator found by GridSearchCV.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predicted values wrapped in an XYData object.</p> Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the best estimator found by GridSearchCV.\n\n    Args:\n        x (XYData): The input features.\n\n    Returns:\n        XYData: The predicted values wrapped in an XYData object.\n    \"\"\"\n    return XYData.mock(self._clf.predict(x.value))  # type: ignore\n</code></pre>"},{"location":"api/plugins/optimizers/sklearn_optimizer/#framework3.plugins.optimizer.sklearn_optimizer.SklearnOptimizer.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the pipeline execution.</p> <p>This method fits the optimizer and makes predictions if X_ is provided.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>Input data for fitting.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Target data for fitting.</p> required <code>X_</code> <code>Optional[XYData]</code> <p>Data for prediction (if different from x).</p> required <p>Returns:</p> Type Description <code>Optional[XYData]</code> <p>Optional[XYData]: Prediction results if X_ is provided, else None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during pipeline execution.</p> Source code in <code>framework3/plugins/optimizer/sklearn_optimizer.py</code> <pre><code>def start(\n    self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n) -&gt; Optional[XYData]:\n    \"\"\"\n    Start the pipeline execution.\n\n    This method fits the optimizer and makes predictions if X_ is provided.\n\n    Args:\n        x (XYData): Input data for fitting.\n        y (Optional[XYData]): Target data for fitting.\n        X_ (Optional[XYData]): Data for prediction (if different from x).\n\n    Returns:\n        Optional[XYData]: Prediction results if X_ is provided, else None.\n\n    Raises:\n        Exception: If an error occurs during pipeline execution.\n    \"\"\"\n    try:\n        self.fit(x, y)\n        if X_ is not None:\n            return self.predict(X_)\n        else:\n            return self.predict(x)\n    except Exception as e:\n        print(f\"Error during pipeline execution: {e}\")\n        raise e\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/","title":"WandbOptimizer","text":""},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer","title":"<code>framework3.plugins.optimizer.wandb_optimizer</code>","text":""},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.__all__","title":"<code>__all__ = ['WandbOptimizer']</code>  <code>module-attribute</code>","text":""},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer","title":"<code>WandbOptimizer</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>Weights &amp; Biases (wandb) based optimizer for hyperparameter tuning.</p> <p>This class implements hyperparameter optimization using Weights &amp; Biases' sweep functionality. It allows for efficient searching of hyperparameter spaces for machine learning models within the Framework3 pipeline system.</p> Key Features <ul> <li>Integrates with Weights &amp; Biases for distributed hyperparameter optimization</li> <li>Supports various types of hyperparameters</li> <li>Allows for customizable scoring metrics</li> <li>Integrates with the Framework3 pipeline system</li> </ul> Usage <p>The WandbOptimizer can be used to optimize hyperparameters of a machine learning pipeline:</p> <pre><code>from framework3.plugins.optimizer import WandbOptimizer\nfrom framework3.base import XYData, F1\n\n# Assuming you have a pipeline and data\npipeline = ...\nx_data = XYData(...)\ny_data = XYData(...)\n\noptimizer = WandbOptimizer(project=\"my_project\", scorer=F1(), pipeline=pipeline)\noptimizer.fit(x_data, y_data)\n\nbest_pipeline = optimizer.pipeline\n</code></pre> <p>Attributes:</p> Name Type Description <code>project</code> <code>str</code> <p>The name of the Weights &amp; Biases project.</p> <code>scorer</code> <code>BaseMetric</code> <p>The scoring metric for evaluation.</p> <code>sweep_id</code> <code>str | None</code> <p>The ID of the Weights &amp; Biases sweep.</p> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be optimized.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>BaseFilter) -&gt; None: Set up the optimization process for a given pipeline.</p> <code>fit</code> <p>XYData, y: XYData | None) -&gt; None: Perform the hyperparameter optimization.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the best pipeline found.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Evaluate the optimized pipeline.</p> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>@Container.bind()\nclass WandbOptimizer(BaseOptimizer):\n    \"\"\"\n    Weights &amp; Biases (wandb) based optimizer for hyperparameter tuning.\n\n    This class implements hyperparameter optimization using Weights &amp; Biases' sweep functionality.\n    It allows for efficient searching of hyperparameter spaces for machine learning models\n    within the Framework3 pipeline system.\n\n    Key Features:\n        - Integrates with Weights &amp; Biases for distributed hyperparameter optimization\n        - Supports various types of hyperparameters\n        - Allows for customizable scoring metrics\n        - Integrates with the Framework3 pipeline system\n\n    Usage:\n        The WandbOptimizer can be used to optimize hyperparameters of a machine learning pipeline:\n\n        ```python\n        from framework3.plugins.optimizer import WandbOptimizer\n        from framework3.base import XYData, F1\n\n        # Assuming you have a pipeline and data\n        pipeline = ...\n        x_data = XYData(...)\n        y_data = XYData(...)\n\n        optimizer = WandbOptimizer(project=\"my_project\", scorer=F1(), pipeline=pipeline)\n        optimizer.fit(x_data, y_data)\n\n        best_pipeline = optimizer.pipeline\n        ```\n\n    Attributes:\n        project (str): The name of the Weights &amp; Biases project.\n        scorer (BaseMetric): The scoring metric for evaluation.\n        sweep_id (str | None): The ID of the Weights &amp; Biases sweep.\n        pipeline (BaseFilter | None): The pipeline to be optimized.\n\n    Methods:\n        optimize(pipeline: BaseFilter) -&gt; None: Set up the optimization process for a given pipeline.\n        fit(x: XYData, y: XYData | None) -&gt; None: Perform the hyperparameter optimization.\n        predict(x: XYData) -&gt; XYData: Make predictions using the best pipeline found.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Evaluate the optimized pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        project: str,\n        scorer: BaseMetric,\n        pipeline: BaseFilter | None = None,\n        sweep_id: str | None = None,\n    ):\n        \"\"\"\n        Initialize the WandbOptimizer.\n\n        Args:\n            project (str): The name of the Weights &amp; Biases project.\n            scorer (BaseMetric): The scoring metric for evaluation.\n            pipeline (BaseFilter | None): The pipeline to be optimized. Defaults to None.\n            sweep_id (str | None): The ID of an existing Weights &amp; Biases sweep. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.project = project\n        self.scorer = scorer\n        self.sweep_id = sweep_id\n        self.pipeline = pipeline\n\n    def optimize(self, pipeline: BaseFilter) -&gt; None:\n        \"\"\"\n        Set up the optimization process for a given pipeline.\n\n        This method prepares the pipeline for optimization by Weights &amp; Biases.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be optimized.\n        \"\"\"\n        self.pipeline = pipeline\n        self.pipeline.verbose(False)\n\n    def get_grid(self, aux: Dict[str, Any], config: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Recursively process the grid configuration of a pipeline or filter.\n\n        This method traverses the configuration dictionary and updates the parameters\n        based on the Weights &amp; Biases configuration.\n\n        Args:\n            aux (Dict[str, Any]): The configuration dictionary to process.\n            config (Dict[str, Any]): The Weights &amp; Biases configuration.\n\n        Note:\n            This method modifies the input dictionary in-place.\n        \"\"\"\n        match aux[\"params\"]:\n            case {\"filters\": filters, **r}:\n                for filter_config in filters:\n                    self.get_grid(filter_config, config)\n            case {\"pipeline\": pipeline, **r}:  # noqa: F841\n                self.get_grid(pipeline, config)\n            case {\"filter\": cached_filter, **r}:  # noqa: F841\n                self.get_grid(cached_filter, config)\n            case p_params:\n                if \"_grid\" in aux:\n                    for param, value in aux[\"_grid\"].items():\n                        p_params.update({param: config[aux[\"clazz\"]][param]})\n\n    def exec(\n        self, config: Dict[str, Any], x: XYData, y: XYData | None = None\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Execute a single run of the pipeline with a given configuration.\n\n        This method is called by the Weights &amp; Biases agent for each hyperparameter configuration.\n\n        Args:\n            config (Dict[str, Any]): The hyperparameter configuration to test.\n            x (XYData): The input features.\n            y (XYData | None): The target values.\n\n        Returns:\n            Dict[str, float]: A dictionary containing the score for the current configuration.\n\n        Raises:\n            ValueError: If the pipeline is not properly configured or returns unexpected results.\n        \"\"\"\n        if self.pipeline is None and self.sweep_id is None or self.project == \"\":\n            raise ValueError(\"Either pipeline or sweep_id must be provided\")\n\n        self.get_grid(config[\"pipeline\"], config[\"filters\"])\n\n        pipeline: BaseFilter = cast(\n            BaseFilter, BasePlugin.build_from_dump(config[\"pipeline\"], Container.pif)\n        )\n\n        pipeline.verbose(False)\n\n        match pipeline.fit(x, y):\n            case None:\n                losses = pipeline.evaluate(x, y, pipeline.predict(x))\n\n                loss = losses.get(self.scorer.__class__.__name__, 0.0)\n\n                return {self.scorer.__class__.__name__: float(loss)}\n            case float() as loss:\n                return {self.scorer.__class__.__name__: loss}\n            case _:\n                raise ValueError(\"Unexpected return type from pipeline.fit()\")\n\n    def fit(self, x: XYData, y: XYData | None = None) -&gt; None:\n        \"\"\"\n        Perform the hyperparameter optimization.\n\n        This method creates a Weights &amp; Biases sweep if necessary, runs the optimization,\n        and fits the best pipeline found.\n\n        Args:\n            x (XYData): The input features.\n            y (XYData | None): The target values.\n\n        Raises:\n            ValueError: If neither pipeline nor sweep_id is provided.\n        \"\"\"\n        if self.sweep_id is None and self.pipeline is not None:\n            self.sweep_id = WandbSweepManager().create_sweep(\n                self.pipeline, self.project, scorer=self.scorer, x=x, y=y\n            )\n\n        if self.sweep_id is not None:\n            sweep = WandbSweepManager().get_sweep(self.project, self.sweep_id)\n            sweep_state = sweep.state.lower()\n            if sweep_state not in (\"finished\", \"cancelled\", \"crashed\"):\n                WandbAgent()(\n                    self.sweep_id, self.project, lambda config: self.exec(config, x, y)\n                )\n        else:\n            raise ValueError(\"Either pipeline or sweep_id must be provided\")\n\n        winner = WandbSweepManager().get_best_config(\n            self.project, self.sweep_id, self.scorer.__class__.__name__\n        )\n\n        print(winner)\n\n        self.get_grid(winner[\"pipeline\"], winner[\"filters\"])\n        self.pipeline = cast(\n            BaseFilter, BasePlugin.build_from_dump(winner[\"pipeline\"], Container.pif)\n        )\n\n        self.pipeline.unwrap().fit(x, y)\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the best pipeline found.\n\n        Args:\n            x (XYData): The input features.\n\n        Returns:\n            XYData: The predicted values.\n\n        Raises:\n            ValueError: If the pipeline has not been fitted.\n        \"\"\"\n        if self.pipeline is not None:\n            return self.pipeline.predict(x)\n        else:\n            raise ValueError(\"Pipeline must be fitted before predicting\")\n\n    def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n        \"\"\"\n        Start the pipeline execution.\n\n        Args:\n            x (XYData): Input data for fitting.\n            y (XYData | None): Target data for fitting.\n            X_ (XYData | None): Data for prediction (if different from x).\n\n        Returns:\n            XYData | None: Prediction results if X_ is provided, else None.\n\n        Raises:\n            ValueError: If the pipeline has not been fitted.\n        \"\"\"\n        if self.pipeline is not None:\n            return self.pipeline.start(x, y, X_)\n        else:\n            raise ValueError(\"Pipeline must be fitted before starting\")\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the optimized pipeline.\n\n        Args:\n            x_data (XYData): Input data.\n            y_true (XYData | None): True target data.\n            y_pred (XYData): Predicted target data.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation results.\n        \"\"\"\n        return (\n            self.pipeline.evaluate(x_data, y_true, y_pred)\n            if self.pipeline is not None\n            else {}\n        )\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.pipeline","title":"<code>pipeline = pipeline</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.project","title":"<code>project = project</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.scorer","title":"<code>scorer = scorer</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.sweep_id","title":"<code>sweep_id = sweep_id</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.__init__","title":"<code>__init__(project, scorer, pipeline=None, sweep_id=None)</code>","text":"<p>Initialize the WandbOptimizer.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>The name of the Weights &amp; Biases project.</p> required <code>scorer</code> <code>BaseMetric</code> <p>The scoring metric for evaluation.</p> required <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be optimized. Defaults to None.</p> <code>None</code> <code>sweep_id</code> <code>str | None</code> <p>The ID of an existing Weights &amp; Biases sweep. Defaults to None.</p> <code>None</code> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def __init__(\n    self,\n    project: str,\n    scorer: BaseMetric,\n    pipeline: BaseFilter | None = None,\n    sweep_id: str | None = None,\n):\n    \"\"\"\n    Initialize the WandbOptimizer.\n\n    Args:\n        project (str): The name of the Weights &amp; Biases project.\n        scorer (BaseMetric): The scoring metric for evaluation.\n        pipeline (BaseFilter | None): The pipeline to be optimized. Defaults to None.\n        sweep_id (str | None): The ID of an existing Weights &amp; Biases sweep. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self.project = project\n    self.scorer = scorer\n    self.sweep_id = sweep_id\n    self.pipeline = pipeline\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the optimized pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>Input data.</p> required <code>y_true</code> <code>XYData | None</code> <p>True target data.</p> required <code>y_pred</code> <code>XYData</code> <p>Predicted target data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation results.</p> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the optimized pipeline.\n\n    Args:\n        x_data (XYData): Input data.\n        y_true (XYData | None): True target data.\n        y_pred (XYData): Predicted target data.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results.\n    \"\"\"\n    return (\n        self.pipeline.evaluate(x_data, y_true, y_pred)\n        if self.pipeline is not None\n        else {}\n    )\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.exec","title":"<code>exec(config, x, y=None)</code>","text":"<p>Execute a single run of the pipeline with a given configuration.</p> <p>This method is called by the Weights &amp; Biases agent for each hyperparameter configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The hyperparameter configuration to test.</p> required <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>XYData | None</code> <p>The target values.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary containing the score for the current configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not properly configured or returns unexpected results.</p> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def exec(\n    self, config: Dict[str, Any], x: XYData, y: XYData | None = None\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Execute a single run of the pipeline with a given configuration.\n\n    This method is called by the Weights &amp; Biases agent for each hyperparameter configuration.\n\n    Args:\n        config (Dict[str, Any]): The hyperparameter configuration to test.\n        x (XYData): The input features.\n        y (XYData | None): The target values.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the score for the current configuration.\n\n    Raises:\n        ValueError: If the pipeline is not properly configured or returns unexpected results.\n    \"\"\"\n    if self.pipeline is None and self.sweep_id is None or self.project == \"\":\n        raise ValueError(\"Either pipeline or sweep_id must be provided\")\n\n    self.get_grid(config[\"pipeline\"], config[\"filters\"])\n\n    pipeline: BaseFilter = cast(\n        BaseFilter, BasePlugin.build_from_dump(config[\"pipeline\"], Container.pif)\n    )\n\n    pipeline.verbose(False)\n\n    match pipeline.fit(x, y):\n        case None:\n            losses = pipeline.evaluate(x, y, pipeline.predict(x))\n\n            loss = losses.get(self.scorer.__class__.__name__, 0.0)\n\n            return {self.scorer.__class__.__name__: float(loss)}\n        case float() as loss:\n            return {self.scorer.__class__.__name__: loss}\n        case _:\n            raise ValueError(\"Unexpected return type from pipeline.fit()\")\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.fit","title":"<code>fit(x, y=None)</code>","text":"<p>Perform the hyperparameter optimization.</p> <p>This method creates a Weights &amp; Biases sweep if necessary, runs the optimization, and fits the best pipeline found.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>XYData | None</code> <p>The target values.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither pipeline nor sweep_id is provided.</p> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def fit(self, x: XYData, y: XYData | None = None) -&gt; None:\n    \"\"\"\n    Perform the hyperparameter optimization.\n\n    This method creates a Weights &amp; Biases sweep if necessary, runs the optimization,\n    and fits the best pipeline found.\n\n    Args:\n        x (XYData): The input features.\n        y (XYData | None): The target values.\n\n    Raises:\n        ValueError: If neither pipeline nor sweep_id is provided.\n    \"\"\"\n    if self.sweep_id is None and self.pipeline is not None:\n        self.sweep_id = WandbSweepManager().create_sweep(\n            self.pipeline, self.project, scorer=self.scorer, x=x, y=y\n        )\n\n    if self.sweep_id is not None:\n        sweep = WandbSweepManager().get_sweep(self.project, self.sweep_id)\n        sweep_state = sweep.state.lower()\n        if sweep_state not in (\"finished\", \"cancelled\", \"crashed\"):\n            WandbAgent()(\n                self.sweep_id, self.project, lambda config: self.exec(config, x, y)\n            )\n    else:\n        raise ValueError(\"Either pipeline or sweep_id must be provided\")\n\n    winner = WandbSweepManager().get_best_config(\n        self.project, self.sweep_id, self.scorer.__class__.__name__\n    )\n\n    print(winner)\n\n    self.get_grid(winner[\"pipeline\"], winner[\"filters\"])\n    self.pipeline = cast(\n        BaseFilter, BasePlugin.build_from_dump(winner[\"pipeline\"], Container.pif)\n    )\n\n    self.pipeline.unwrap().fit(x, y)\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.get_grid","title":"<code>get_grid(aux, config)</code>","text":"<p>Recursively process the grid configuration of a pipeline or filter.</p> <p>This method traverses the configuration dictionary and updates the parameters based on the Weights &amp; Biases configuration.</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to process.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>The Weights &amp; Biases configuration.</p> required Note <p>This method modifies the input dictionary in-place.</p> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def get_grid(self, aux: Dict[str, Any], config: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Recursively process the grid configuration of a pipeline or filter.\n\n    This method traverses the configuration dictionary and updates the parameters\n    based on the Weights &amp; Biases configuration.\n\n    Args:\n        aux (Dict[str, Any]): The configuration dictionary to process.\n        config (Dict[str, Any]): The Weights &amp; Biases configuration.\n\n    Note:\n        This method modifies the input dictionary in-place.\n    \"\"\"\n    match aux[\"params\"]:\n        case {\"filters\": filters, **r}:\n            for filter_config in filters:\n                self.get_grid(filter_config, config)\n        case {\"pipeline\": pipeline, **r}:  # noqa: F841\n            self.get_grid(pipeline, config)\n        case {\"filter\": cached_filter, **r}:  # noqa: F841\n            self.get_grid(cached_filter, config)\n        case p_params:\n            if \"_grid\" in aux:\n                for param, value in aux[\"_grid\"].items():\n                    p_params.update({param: config[aux[\"clazz\"]][param]})\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.optimize","title":"<code>optimize(pipeline)</code>","text":"<p>Set up the optimization process for a given pipeline.</p> <p>This method prepares the pipeline for optimization by Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be optimized.</p> required Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def optimize(self, pipeline: BaseFilter) -&gt; None:\n    \"\"\"\n    Set up the optimization process for a given pipeline.\n\n    This method prepares the pipeline for optimization by Weights &amp; Biases.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be optimized.\n    \"\"\"\n    self.pipeline = pipeline\n    self.pipeline.verbose(False)\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the best pipeline found.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predicted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline has not been fitted.</p> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the best pipeline found.\n\n    Args:\n        x (XYData): The input features.\n\n    Returns:\n        XYData: The predicted values.\n\n    Raises:\n        ValueError: If the pipeline has not been fitted.\n    \"\"\"\n    if self.pipeline is not None:\n        return self.pipeline.predict(x)\n    else:\n        raise ValueError(\"Pipeline must be fitted before predicting\")\n</code></pre>"},{"location":"api/plugins/optimizers/wandb_optimizer/#framework3.plugins.optimizer.wandb_optimizer.WandbOptimizer.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the pipeline execution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>Input data for fitting.</p> required <code>y</code> <code>XYData | None</code> <p>Target data for fitting.</p> required <code>X_</code> <code>XYData | None</code> <p>Data for prediction (if different from x).</p> required <p>Returns:</p> Type Description <code>XYData | None</code> <p>XYData | None: Prediction results if X_ is provided, else None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline has not been fitted.</p> Source code in <code>framework3/plugins/optimizer/wandb_optimizer.py</code> <pre><code>def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n    \"\"\"\n    Start the pipeline execution.\n\n    Args:\n        x (XYData): Input data for fitting.\n        y (XYData | None): Target data for fitting.\n        X_ (XYData | None): Data for prediction (if different from x).\n\n    Returns:\n        XYData | None: Prediction results if X_ is provided, else None.\n\n    Raises:\n        ValueError: If the pipeline has not been fitted.\n    \"\"\"\n    if self.pipeline is not None:\n        return self.pipeline.start(x, y, X_)\n    else:\n        raise ValueError(\"Pipeline must be fitted before starting\")\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/","title":"HPCPipeline","text":"<p>The <code>HPCPipeline</code> class is part of the <code>framework3.plugins.pipelines.parallel</code> module and is designed to facilitate high-performance computing (HPC) tasks within the framework. This pipeline is optimized for parallel processing, allowing for efficient execution of complex workflows across distributed systems.</p>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#module-contents","title":"Module Contents","text":""},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline","title":"<code>framework3.plugins.pipelines.parallel.parallel_hpc_pipeline</code>","text":""},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline","title":"<code>HPCPipeline</code>","text":"<p>               Bases: <code>ParallelPipeline</code></p> <p>High Performance Computing Pipeline using MapReduce for parallel feature extraction.</p> <p>This pipeline applies a sequence of filters to the input data using a MapReduce approach, enabling parallel processing and potentially improved performance on large datasets.</p> Key Features <ul> <li>Parallel processing of filters using MapReduce</li> <li>Scalable to large datasets</li> <li>Configurable number of partitions for optimization</li> </ul> Usage <pre><code>from framework3.plugins.pipelines.parallel import HPCPipeline\nfrom framework3.base import XYData\n\nfilters = [Filter1(), Filter2(), Filter3()]\npipeline = HPCPipeline(filters, app_name=\"MyApp\", master=\"local[*]\", numSlices=8)\n\nx_data = XYData(...)\ny_data = XYData(...)\n\npipeline.fit(x_data, y_data)\npredictions = pipeline.predict(x_data)\n</code></pre> <p>Attributes:</p> Name Type Description <code>filters</code> <code>Sequence[BaseFilter]</code> <p>A sequence of filters to be applied to the input data.</p> <code>numSlices</code> <code>int</code> <p>The number of partitions to use in the MapReduce process.</p> <code>app_name</code> <code>str</code> <p>The name of the Spark application.</p> <code>master</code> <code>str</code> <p>The Spark master URL.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData]): Fit the filters in parallel using MapReduce.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the fitted filters in parallel.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Evaluate the pipeline using provided metrics.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>@Container.bind()\nclass HPCPipeline(ParallelPipeline):\n    \"\"\"\n    High Performance Computing Pipeline using MapReduce for parallel feature extraction.\n\n    This pipeline applies a sequence of filters to the input data using a MapReduce approach,\n    enabling parallel processing and potentially improved performance on large datasets.\n\n    Key Features:\n        - Parallel processing of filters using MapReduce\n        - Scalable to large datasets\n        - Configurable number of partitions for optimization\n\n    Usage:\n        ```python\n        from framework3.plugins.pipelines.parallel import HPCPipeline\n        from framework3.base import XYData\n\n        filters = [Filter1(), Filter2(), Filter3()]\n        pipeline = HPCPipeline(filters, app_name=\"MyApp\", master=\"local[*]\", numSlices=8)\n\n        x_data = XYData(...)\n        y_data = XYData(...)\n\n        pipeline.fit(x_data, y_data)\n        predictions = pipeline.predict(x_data)\n        ```\n\n    Attributes:\n        filters (Sequence[BaseFilter]): A sequence of filters to be applied to the input data.\n        numSlices (int): The number of partitions to use in the MapReduce process.\n        app_name (str): The name of the Spark application.\n        master (str): The Spark master URL.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData]): Fit the filters in parallel using MapReduce.\n        predict(x: XYData) -&gt; XYData: Make predictions using the fitted filters in parallel.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Evaluate the pipeline using provided metrics.\n    \"\"\"\n\n    def __init__(\n        self,\n        filters: Sequence[BaseFilter],\n        app_name: str,\n        master: str = \"local\",\n        numSlices: int = 4,\n    ):\n        \"\"\"\n        Initialize the HPCPipeline.\n\n        Args:\n            filters (Sequence[BaseFilter]): A sequence of filters to be applied to the input data.\n            app_name (str): The name of the Spark application.\n            master (str, optional): The Spark master URL. Defaults to \"local\".\n            numSlices (int, optional): The number of partitions to use in the MapReduce process. Defaults to 4.\n        \"\"\"\n        super().__init__(filters=filters)\n        self.filters = filters\n        self.numSlices = numSlices\n        self.app_name = app_name\n        self.master = master\n\n    def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n        \"\"\"\n        Start the pipeline execution by fitting the model and making predictions.\n\n        This method initiates the pipeline process by fitting the model to the input data\n        and then making predictions.\n\n        Args:\n            x (XYData): The input data for fitting and prediction.\n            y (XYData | None): The target data for fitting, if available.\n            X_ (XYData | None): Additional input data for prediction, if different from x.\n\n        Returns:\n            XYData | None: The predictions made by the pipeline, or None if an error occurs.\n\n        Raises:\n            Exception: If an error occurs during the fitting or prediction process.\n        \"\"\"\n        try:\n            self.fit(x, y)\n            return self.predict(x)\n        except Exception as e:\n            # Handle the exception appropriately\n            raise e\n\n    def fit(self, x: XYData, y: Optional[XYData]):\n        \"\"\"\n        Fit the filters in the pipeline to the input data using MapReduce.\n\n        This method applies the fit operation to all filters in parallel using MapReduce,\n        allowing for efficient processing of large datasets.\n\n        Args:\n            x (XYData): The input data to fit the filters on.\n            y (XYData | None, optional): The target data, if available.\n\n        Note:\n            This method updates the filters in place with their trained versions.\n        \"\"\"\n\n        def fit_function(filter):\n            try:\n                filter.fit(deepcopy(x), y)\n            except NotTrainableFilterError:\n                filter.init()\n            return filter\n\n        spark = PySparkMapReduce(self.app_name, self.master)\n        # Apply fit in parallel to the filters\n        rdd = spark.map(self.filters, fit_function, numSlices=self.numSlices)\n        # Update the filters with the trained versions\n        self.filters = rdd.collect()\n        spark.stop()\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted filters in parallel.\n\n        This method applies the predict operation to all filters in parallel using MapReduce,\n        then combines the results into a single prediction.\n\n        Args:\n            x (XYData): The input data to make predictions on.\n\n        Returns:\n            XYData: The combined predictions from all filters.\n\n        Note:\n            The predictions from each filter are stacked horizontally to form the final output.\n        \"\"\"\n\n        def predict_function(filter: BaseFilter) -&gt; VData:\n            result: XYData = filter.predict(x)\n            m_hash, _ = filter._get_model_key(x._hash)\n            return XYData.ensure_dim(result.value)\n\n        # Apply predict in parallel to the filters\n        spark = PySparkMapReduce(self.app_name, self.master)\n        spark.map(self.filters, predict_function, numSlices=self.numSlices)\n        aux = spark.reduce(lambda x, y: np.hstack([x, y]))\n        spark.stop()\n        # Reduce the results\n        return XYData.mock(aux)\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the pipeline using the provided metrics.\n\n        This method applies each metric in the pipeline to the predicted and true values,\n        returning a dictionary of evaluation results.\n\n        Args:\n            x_data (XYData): Input data used for evaluation.\n            y_true (XYData | None): True target data, if available.\n            y_pred (XYData): Predicted target data.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation results for each metric.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; evaluation = pipeline.evaluate(x_test, y_test, predictions)\n            &gt;&gt;&gt; print(evaluation)\n            {'F1Score': 0.85, 'Accuracy': 0.90}\n            ```\n        \"\"\"\n        results = {}\n        for metric in self.metrics:\n            results[metric.__class__.__name__] = metric.evaluate(x_data, y_true, y_pred)\n        return results\n\n    def log_metrics(self):\n        \"\"\"\n        Log metrics for the pipeline.\n\n        This method can be implemented to log any relevant metrics during the pipeline's execution.\n        \"\"\"\n        # Implement metric logging if necessary\n        pass\n\n    def finish(self):\n        \"\"\"\n        Finish the pipeline's execution.\n\n        This method is called to perform any necessary cleanup or finalization steps.\n        \"\"\"\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline.__init__","title":"<code>__init__(filters, app_name, master='local', numSlices=4)</code>","text":"<p>Initialize the HPCPipeline.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Sequence[BaseFilter]</code> <p>A sequence of filters to be applied to the input data.</p> required <code>app_name</code> <code>str</code> <p>The name of the Spark application.</p> required <code>master</code> <code>str</code> <p>The Spark master URL. Defaults to \"local\".</p> <code>'local'</code> <code>numSlices</code> <code>int</code> <p>The number of partitions to use in the MapReduce process. Defaults to 4.</p> <code>4</code> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>def __init__(\n    self,\n    filters: Sequence[BaseFilter],\n    app_name: str,\n    master: str = \"local\",\n    numSlices: int = 4,\n):\n    \"\"\"\n    Initialize the HPCPipeline.\n\n    Args:\n        filters (Sequence[BaseFilter]): A sequence of filters to be applied to the input data.\n        app_name (str): The name of the Spark application.\n        master (str, optional): The Spark master URL. Defaults to \"local\".\n        numSlices (int, optional): The number of partitions to use in the MapReduce process. Defaults to 4.\n    \"\"\"\n    super().__init__(filters=filters)\n    self.filters = filters\n    self.numSlices = numSlices\n    self.app_name = app_name\n    self.master = master\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the pipeline using the provided metrics.</p> <p>This method applies each metric in the pipeline to the predicted and true values, returning a dictionary of evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>Input data used for evaluation.</p> required <code>y_true</code> <code>XYData | None</code> <p>True target data, if available.</p> required <code>y_pred</code> <code>XYData</code> <p>Predicted target data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation results for each metric.</p> Example <pre><code>&gt;&gt;&gt; evaluation = pipeline.evaluate(x_test, y_test, predictions)\n&gt;&gt;&gt; print(evaluation)\n{'F1Score': 0.85, 'Accuracy': 0.90}\n</code></pre> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the pipeline using the provided metrics.\n\n    This method applies each metric in the pipeline to the predicted and true values,\n    returning a dictionary of evaluation results.\n\n    Args:\n        x_data (XYData): Input data used for evaluation.\n        y_true (XYData | None): True target data, if available.\n        y_pred (XYData): Predicted target data.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results for each metric.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; evaluation = pipeline.evaluate(x_test, y_test, predictions)\n        &gt;&gt;&gt; print(evaluation)\n        {'F1Score': 0.85, 'Accuracy': 0.90}\n        ```\n    \"\"\"\n    results = {}\n    for metric in self.metrics:\n        results[metric.__class__.__name__] = metric.evaluate(x_data, y_true, y_pred)\n    return results\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline.finish","title":"<code>finish()</code>","text":"<p>Finish the pipeline's execution.</p> <p>This method is called to perform any necessary cleanup or finalization steps.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>def finish(self):\n    \"\"\"\n    Finish the pipeline's execution.\n\n    This method is called to perform any necessary cleanup or finalization steps.\n    \"\"\"\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline.fit","title":"<code>fit(x, y)</code>","text":"<p>Fit the filters in the pipeline to the input data using MapReduce.</p> <p>This method applies the fit operation to all filters in parallel using MapReduce, allowing for efficient processing of large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data to fit the filters on.</p> required <code>y</code> <code>XYData | None</code> <p>The target data, if available.</p> required Note <p>This method updates the filters in place with their trained versions.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>def fit(self, x: XYData, y: Optional[XYData]):\n    \"\"\"\n    Fit the filters in the pipeline to the input data using MapReduce.\n\n    This method applies the fit operation to all filters in parallel using MapReduce,\n    allowing for efficient processing of large datasets.\n\n    Args:\n        x (XYData): The input data to fit the filters on.\n        y (XYData | None, optional): The target data, if available.\n\n    Note:\n        This method updates the filters in place with their trained versions.\n    \"\"\"\n\n    def fit_function(filter):\n        try:\n            filter.fit(deepcopy(x), y)\n        except NotTrainableFilterError:\n            filter.init()\n        return filter\n\n    spark = PySparkMapReduce(self.app_name, self.master)\n    # Apply fit in parallel to the filters\n    rdd = spark.map(self.filters, fit_function, numSlices=self.numSlices)\n    # Update the filters with the trained versions\n    self.filters = rdd.collect()\n    spark.stop()\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline.log_metrics","title":"<code>log_metrics()</code>","text":"<p>Log metrics for the pipeline.</p> <p>This method can be implemented to log any relevant metrics during the pipeline's execution.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>def log_metrics(self):\n    \"\"\"\n    Log metrics for the pipeline.\n\n    This method can be implemented to log any relevant metrics during the pipeline's execution.\n    \"\"\"\n    # Implement metric logging if necessary\n    pass\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted filters in parallel.</p> <p>This method applies the predict operation to all filters in parallel using MapReduce, then combines the results into a single prediction.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data to make predictions on.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The combined predictions from all filters.</p> Note <p>The predictions from each filter are stacked horizontally to form the final output.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted filters in parallel.\n\n    This method applies the predict operation to all filters in parallel using MapReduce,\n    then combines the results into a single prediction.\n\n    Args:\n        x (XYData): The input data to make predictions on.\n\n    Returns:\n        XYData: The combined predictions from all filters.\n\n    Note:\n        The predictions from each filter are stacked horizontally to form the final output.\n    \"\"\"\n\n    def predict_function(filter: BaseFilter) -&gt; VData:\n        result: XYData = filter.predict(x)\n        m_hash, _ = filter._get_model_key(x._hash)\n        return XYData.ensure_dim(result.value)\n\n    # Apply predict in parallel to the filters\n    spark = PySparkMapReduce(self.app_name, self.master)\n    spark.map(self.filters, predict_function, numSlices=self.numSlices)\n    aux = spark.reduce(lambda x, y: np.hstack([x, y]))\n    spark.stop()\n    # Reduce the results\n    return XYData.mock(aux)\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#framework3.plugins.pipelines.parallel.parallel_hpc_pipeline.HPCPipeline.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the pipeline execution by fitting the model and making predictions.</p> <p>This method initiates the pipeline process by fitting the model to the input data and then making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data for fitting and prediction.</p> required <code>y</code> <code>XYData | None</code> <p>The target data for fitting, if available.</p> required <code>X_</code> <code>XYData | None</code> <p>Additional input data for prediction, if different from x.</p> required <p>Returns:</p> Type Description <code>XYData | None</code> <p>XYData | None: The predictions made by the pipeline, or None if an error occurs.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the fitting or prediction process.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_hpc_pipeline.py</code> <pre><code>def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n    \"\"\"\n    Start the pipeline execution by fitting the model and making predictions.\n\n    This method initiates the pipeline process by fitting the model to the input data\n    and then making predictions.\n\n    Args:\n        x (XYData): The input data for fitting and prediction.\n        y (XYData | None): The target data for fitting, if available.\n        X_ (XYData | None): Additional input data for prediction, if different from x.\n\n    Returns:\n        XYData | None: The predictions made by the pipeline, or None if an error occurs.\n\n    Raises:\n        Exception: If an error occurs during the fitting or prediction process.\n    \"\"\"\n    try:\n        self.fit(x, y)\n        return self.predict(x)\n    except Exception as e:\n        # Handle the exception appropriately\n        raise e\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#class-hierarchy","title":"Class Hierarchy","text":"<ul> <li>HPCPipeline</li> </ul>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#hpcpipeline_1","title":"HPCPipeline","text":"<p><code>HPCPipeline</code> extends <code>BasePipeline</code> and provides functionality for executing tasks in parallel, leveraging HPC resources to improve performance and scalability.</p>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#key-methods","title":"Key Methods:","text":"<ul> <li><code>init()</code>: Initializes the pipeline, setting up necessary resources and configurations for HPC execution.</li> <li><code>log_metrics()</code>: Logs relevant metrics during the pipeline's execution to monitor performance and resource utilization.</li> </ul>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#creating-and-using-hpcpipeline","title":"Creating and Using HPCPipeline","text":"<pre><code>from framework3.plugins.pipelines.parallel.hpc_pipeline import HPCPipeline\n\n# Initialize the HPCPipeline\nhpc_pipeline = HPCPipeline()\n\n# Perform necessary setup\nhpc_pipeline.init()\n\n# Execute the pipeline\n# (Assuming tasks and configurations are defined elsewhere)\nhpc_pipeline.execute()\n\n# Log metrics\nhpc_pipeline.log_metrics()\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Ensure that your HPC environment is properly configured and accessible before initializing the pipeline.</li> <li>Define tasks and dependencies clearly to maximize parallel execution efficiency.</li> <li>Monitor resource utilization and adjust configurations as needed to optimize performance.</li> </ol>"},{"location":"api/plugins/pipelines/parallel/hpc_pipeline/#conclusion","title":"Conclusion","text":"<p><code>HPCPipeline</code> provides a robust solution for executing high-performance computing tasks within the <code>LabChain</code> ecosystem. By following the best practices and examples provided, you can effectively leverage HPC resources to enhance your machine learning workflows.</p>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/","title":"MonoPipeline","text":"<p>The <code>MonoPipeline</code> class is part of the <code>framework3.plugins.pipelines.parallel</code> module and is designed to run multiple pipelines in parallel, combining their outputs to create new features. This pipeline is ideal for scenarios where different models or transformations need to be applied simultaneously to the same dataset.</p>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#module-contents","title":"Module Contents","text":""},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline","title":"<code>framework3.plugins.pipelines.parallel.parallel_mono_pipeline</code>","text":""},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline.MonoPipeline","title":"<code>MonoPipeline</code>","text":"<p>               Bases: <code>ParallelPipeline</code></p> <p>A pipeline that combines multiple filters in parallel and constructs new features from their outputs.</p> <p>This pipeline allows for simultaneous execution of multiple filters on the same input data, and then combines their outputs to create new features. It's particularly useful for feature engineering and ensemble methods.</p> Key Features <ul> <li>Parallel execution of multiple filters</li> <li>Combination of filter outputs for feature construction</li> <li>Support for evaluation metrics</li> </ul> Usage <pre><code>from framework3.plugins.pipelines.parallel import MonoPipeline\nfrom framework3.plugins.filters.transformation import PCAPlugin\nfrom framework3.plugins.filters.classification import KnnFilter\nfrom framework3.plugins.metrics import F1Score\nfrom framework3.base import XYData\n\npipeline = MonoPipeline(\n    filters=[\n        PCAPlugin(n_components=2),\n        KnnFilter(n_neighbors=3)\n    ],\n    metrics=[F1Score()]\n)\n\nx_train = XYData(...)\ny_train = XYData(...)\nx_test = XYData(...)\ny_test = XYData(...)\n\npipeline.fit(x_train, y_train)\npredictions = pipeline.predict(x_test)\nevaluation = pipeline.evaluate(x_test, y_test, predictions)\nprint(evaluation)\n</code></pre> <p>Attributes:</p> Name Type Description <code>filters</code> <code>Sequence[BaseFilter]</code> <p>A sequence of filters to be applied in parallel.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]: Fit all filters in parallel.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using all filters in parallel.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Evaluate the pipeline using provided metrics.</p> <code>combine_features</code> <p>list[XYData]) -&gt; XYData: Combine features from all filter outputs.</p> <code>start</code> <p>XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None: Start the pipeline execution.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_mono_pipeline.py</code> <pre><code>@Container.bind()\nclass MonoPipeline(ParallelPipeline):\n    \"\"\"\n    A pipeline that combines multiple filters in parallel and constructs new features from their outputs.\n\n    This pipeline allows for simultaneous execution of multiple filters on the same input data,\n    and then combines their outputs to create new features. It's particularly useful for\n    feature engineering and ensemble methods.\n\n    Key Features:\n        - Parallel execution of multiple filters\n        - Combination of filter outputs for feature construction\n        - Support for evaluation metrics\n\n    Usage:\n        ```python\n        from framework3.plugins.pipelines.parallel import MonoPipeline\n        from framework3.plugins.filters.transformation import PCAPlugin\n        from framework3.plugins.filters.classification import KnnFilter\n        from framework3.plugins.metrics import F1Score\n        from framework3.base import XYData\n\n        pipeline = MonoPipeline(\n            filters=[\n                PCAPlugin(n_components=2),\n                KnnFilter(n_neighbors=3)\n            ],\n            metrics=[F1Score()]\n        )\n\n        x_train = XYData(...)\n        y_train = XYData(...)\n        x_test = XYData(...)\n        y_test = XYData(...)\n\n        pipeline.fit(x_train, y_train)\n        predictions = pipeline.predict(x_test)\n        evaluation = pipeline.evaluate(x_test, y_test, predictions)\n        print(evaluation)\n        ```\n\n    Attributes:\n        filters (Sequence[BaseFilter]): A sequence of filters to be applied in parallel.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None) -&gt; Optional[float]:\n            Fit all filters in parallel.\n        predict(x: XYData) -&gt; XYData: Make predictions using all filters in parallel.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Evaluate the pipeline using provided metrics.\n        combine_features(pipeline_outputs: list[XYData]) -&gt; XYData:\n            Combine features from all filter outputs.\n        start(x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n            Start the pipeline execution.\n    \"\"\"\n\n    def __init__(self, filters: Sequence[BaseFilter]):\n        \"\"\"\n        Initialize the MonoPipeline.\n\n        Args:\n            filters (Sequence[BaseFilter]): A sequence of filters to be applied in parallel.\n        \"\"\"\n        super().__init__(filters=filters)\n        self.filters = filters\n\n    def fit(\n        self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Fit all filters in the pipeline to the input data in parallel.\n\n        This method applies the fit operation to each filter in the pipeline\n        using the provided input data.\n\n        Args:\n            x (XYData): The input data to fit the filters on.\n            y (XYData | None): The target data, if available.\n            evaluator (BaseMetric | None, optional): An evaluator metric, if needed. Defaults to None.\n\n        Returns:\n            Optional[float]: The mean of the losses returned by the filters, if any.\n\n        Note:\n            Filters that raise NotTrainableFilterError will be initialized instead of fitted.\n        \"\"\"\n        losses = []\n        for f in self.filters:\n            try:\n                losses.append(f.fit(deepcopy(x), y))\n            except NotTrainableFilterError:\n                f.init()\n\n        # filtre los valores None\n\n        match list(filter(lambda x: x is not None, losses)):\n            case []:\n                return None\n            case lss:\n                return float(np.mean(lss))\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using all filters in parallel and combine their outputs.\n\n        This method applies the predict operation to each filter in the pipeline\n        and then combines the outputs using the combine_features method.\n\n        Args:\n            x (XYData): The input data to make predictions on.\n\n        Returns:\n            XYData: The combined predictions from all filters.\n        \"\"\"\n        outputs: List[XYData] = [filter.predict(deepcopy(x)) for filter in self.filters]\n        return self.combine_features(outputs)\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the pipeline using the provided metrics.\n\n        This method applies each metric in the pipeline to the predicted and true values,\n        returning a dictionary of evaluation results.\n\n        Args:\n            x_data (XYData): Input data used for evaluation.\n            y_true (XYData | None): True target data, if available.\n            y_pred (XYData): Predicted target data.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation results for each metric.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; evaluation = pipeline.evaluate(x_test, y_test, predictions)\n            &gt;&gt;&gt; print(evaluation)\n            {'F1Score': 0.85, 'Accuracy': 0.90}\n            ```\n        \"\"\"\n        results = {}\n        for metric in self.metrics:\n            results[metric.__class__.__name__] = metric.evaluate(x_data, y_true, y_pred)\n        return results\n\n    @staticmethod\n    def combine_features(pipeline_outputs: list[XYData]) -&gt; XYData:\n        \"\"\"\n        Combine features from all filter outputs.\n\n        This method concatenates the features from all filter outputs along the last axis.\n\n        Args:\n            pipeline_outputs (List[XYData]): List of outputs from each filter.\n\n        Returns:\n            XYData: Combined output with concatenated features.\n\n        Note:\n            This method assumes that all filter outputs can be concatenated along the last axis.\n            Ensure that your filters produce compatible outputs.\n        \"\"\"\n        return XYData.concat(\n            [XYData.ensure_dim(output.value) for output in pipeline_outputs], axis=-1\n        )\n\n    def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n        \"\"\"\n        Start the pipeline execution by fitting the model and making predictions.\n\n        This method initiates the pipeline process by fitting the model to the input data\n        and then making predictions.\n\n        Args:\n            x (XYData): The input data for fitting and prediction.\n            y (XYData | None): The target data for fitting, if available.\n            X_ (XYData | None): Additional input data for prediction, if different from x.\n\n        Returns:\n            XYData | None: The predictions made by the pipeline, or None if an error occurs.\n\n        Raises:\n            Exception: If an error occurs during the fitting or prediction process.\n        \"\"\"\n        try:\n            self.fit(x, y)\n            if X_ is not None:\n                return self.predict(X_)\n            else:\n                return self.predict(x)\n        except Exception as e:\n            print(f\"Error during pipeline execution: {e}\")\n            raise e\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline.MonoPipeline.__init__","title":"<code>__init__(filters)</code>","text":"<p>Initialize the MonoPipeline.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Sequence[BaseFilter]</code> <p>A sequence of filters to be applied in parallel.</p> required Source code in <code>framework3/plugins/pipelines/parallel/parallel_mono_pipeline.py</code> <pre><code>def __init__(self, filters: Sequence[BaseFilter]):\n    \"\"\"\n    Initialize the MonoPipeline.\n\n    Args:\n        filters (Sequence[BaseFilter]): A sequence of filters to be applied in parallel.\n    \"\"\"\n    super().__init__(filters=filters)\n    self.filters = filters\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline.MonoPipeline.combine_features","title":"<code>combine_features(pipeline_outputs)</code>  <code>staticmethod</code>","text":"<p>Combine features from all filter outputs.</p> <p>This method concatenates the features from all filter outputs along the last axis.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_outputs</code> <code>List[XYData]</code> <p>List of outputs from each filter.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>Combined output with concatenated features.</p> Note <p>This method assumes that all filter outputs can be concatenated along the last axis. Ensure that your filters produce compatible outputs.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_mono_pipeline.py</code> <pre><code>@staticmethod\ndef combine_features(pipeline_outputs: list[XYData]) -&gt; XYData:\n    \"\"\"\n    Combine features from all filter outputs.\n\n    This method concatenates the features from all filter outputs along the last axis.\n\n    Args:\n        pipeline_outputs (List[XYData]): List of outputs from each filter.\n\n    Returns:\n        XYData: Combined output with concatenated features.\n\n    Note:\n        This method assumes that all filter outputs can be concatenated along the last axis.\n        Ensure that your filters produce compatible outputs.\n    \"\"\"\n    return XYData.concat(\n        [XYData.ensure_dim(output.value) for output in pipeline_outputs], axis=-1\n    )\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline.MonoPipeline.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the pipeline using the provided metrics.</p> <p>This method applies each metric in the pipeline to the predicted and true values, returning a dictionary of evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>Input data used for evaluation.</p> required <code>y_true</code> <code>XYData | None</code> <p>True target data, if available.</p> required <code>y_pred</code> <code>XYData</code> <p>Predicted target data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation results for each metric.</p> Example <pre><code>&gt;&gt;&gt; evaluation = pipeline.evaluate(x_test, y_test, predictions)\n&gt;&gt;&gt; print(evaluation)\n{'F1Score': 0.85, 'Accuracy': 0.90}\n</code></pre> Source code in <code>framework3/plugins/pipelines/parallel/parallel_mono_pipeline.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the pipeline using the provided metrics.\n\n    This method applies each metric in the pipeline to the predicted and true values,\n    returning a dictionary of evaluation results.\n\n    Args:\n        x_data (XYData): Input data used for evaluation.\n        y_true (XYData | None): True target data, if available.\n        y_pred (XYData): Predicted target data.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results for each metric.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; evaluation = pipeline.evaluate(x_test, y_test, predictions)\n        &gt;&gt;&gt; print(evaluation)\n        {'F1Score': 0.85, 'Accuracy': 0.90}\n        ```\n    \"\"\"\n    results = {}\n    for metric in self.metrics:\n        results[metric.__class__.__name__] = metric.evaluate(x_data, y_true, y_pred)\n    return results\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline.MonoPipeline.fit","title":"<code>fit(x, y, evaluator=None)</code>","text":"<p>Fit all filters in the pipeline to the input data in parallel.</p> <p>This method applies the fit operation to each filter in the pipeline using the provided input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data to fit the filters on.</p> required <code>y</code> <code>XYData | None</code> <p>The target data, if available.</p> required <code>evaluator</code> <code>BaseMetric | None</code> <p>An evaluator metric, if needed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The mean of the losses returned by the filters, if any.</p> Note <p>Filters that raise NotTrainableFilterError will be initialized instead of fitted.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_mono_pipeline.py</code> <pre><code>def fit(\n    self, x: XYData, y: Optional[XYData], evaluator: BaseMetric | None = None\n) -&gt; Optional[float]:\n    \"\"\"\n    Fit all filters in the pipeline to the input data in parallel.\n\n    This method applies the fit operation to each filter in the pipeline\n    using the provided input data.\n\n    Args:\n        x (XYData): The input data to fit the filters on.\n        y (XYData | None): The target data, if available.\n        evaluator (BaseMetric | None, optional): An evaluator metric, if needed. Defaults to None.\n\n    Returns:\n        Optional[float]: The mean of the losses returned by the filters, if any.\n\n    Note:\n        Filters that raise NotTrainableFilterError will be initialized instead of fitted.\n    \"\"\"\n    losses = []\n    for f in self.filters:\n        try:\n            losses.append(f.fit(deepcopy(x), y))\n        except NotTrainableFilterError:\n            f.init()\n\n    # filtre los valores None\n\n    match list(filter(lambda x: x is not None, losses)):\n        case []:\n            return None\n        case lss:\n            return float(np.mean(lss))\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline.MonoPipeline.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using all filters in parallel and combine their outputs.</p> <p>This method applies the predict operation to each filter in the pipeline and then combines the outputs using the combine_features method.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data to make predictions on.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The combined predictions from all filters.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_mono_pipeline.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using all filters in parallel and combine their outputs.\n\n    This method applies the predict operation to each filter in the pipeline\n    and then combines the outputs using the combine_features method.\n\n    Args:\n        x (XYData): The input data to make predictions on.\n\n    Returns:\n        XYData: The combined predictions from all filters.\n    \"\"\"\n    outputs: List[XYData] = [filter.predict(deepcopy(x)) for filter in self.filters]\n    return self.combine_features(outputs)\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#framework3.plugins.pipelines.parallel.parallel_mono_pipeline.MonoPipeline.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the pipeline execution by fitting the model and making predictions.</p> <p>This method initiates the pipeline process by fitting the model to the input data and then making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data for fitting and prediction.</p> required <code>y</code> <code>XYData | None</code> <p>The target data for fitting, if available.</p> required <code>X_</code> <code>XYData | None</code> <p>Additional input data for prediction, if different from x.</p> required <p>Returns:</p> Type Description <code>XYData | None</code> <p>XYData | None: The predictions made by the pipeline, or None if an error occurs.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the fitting or prediction process.</p> Source code in <code>framework3/plugins/pipelines/parallel/parallel_mono_pipeline.py</code> <pre><code>def start(self, x: XYData, y: XYData | None, X_: XYData | None) -&gt; XYData | None:\n    \"\"\"\n    Start the pipeline execution by fitting the model and making predictions.\n\n    This method initiates the pipeline process by fitting the model to the input data\n    and then making predictions.\n\n    Args:\n        x (XYData): The input data for fitting and prediction.\n        y (XYData | None): The target data for fitting, if available.\n        X_ (XYData | None): Additional input data for prediction, if different from x.\n\n    Returns:\n        XYData | None: The predictions made by the pipeline, or None if an error occurs.\n\n    Raises:\n        Exception: If an error occurs during the fitting or prediction process.\n    \"\"\"\n    try:\n        self.fit(x, y)\n        if X_ is not None:\n            return self.predict(X_)\n        else:\n            return self.predict(x)\n    except Exception as e:\n        print(f\"Error during pipeline execution: {e}\")\n        raise e\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#class-hierarchy","title":"Class Hierarchy","text":"<ul> <li>MonoPipeline</li> </ul>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#monopipeline_1","title":"MonoPipeline","text":"<p><code>MonoPipeline</code> extends <code>ParallelPipeline</code> and provides functionality for executing multiple pipelines in parallel, combining their outputs to enhance feature sets.</p>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#key-methods","title":"Key Methods:","text":"<ul> <li><code>__init__(filters: Sequence[BaseFilter])</code>: Initializes the pipeline with a sequence of filters to be applied in parallel.</li> <li><code>start(x: XYData, y: XYData|None, X_: XYData|None) -&gt; XYData|None</code>: Starts the pipeline execution, fitting the data and making predictions.</li> <li><code>fit(x: XYData, y: XYData|None = None)</code>: Fits all pipelines in parallel using the provided input data.</li> <li><code>predict(x: XYData) -&gt; XYData</code>: Runs predictions on all pipelines in parallel and combines their outputs.</li> <li><code>evaluate(x_data: XYData, y_true: XYData|None, y_pred: XYData) -&gt; Dict[str, Any]</code>: Evaluates the pipeline using the provided metrics.</li> <li><code>combine_features(pipeline_outputs: list[XYData]) -&gt; XYData</code>: Combines features from all pipeline outputs.</li> </ul>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#creating-and-using-monopipeline","title":"Creating and Using MonoPipeline","text":"<pre><code>from framework3.plugins.pipelines.parallel.mono_pipeline import MonoPipeline\nfrom framework3.base import XYData\n\n# Define filters (assuming filters are defined elsewhere)\nfilters = [Filter1(), Filter2()]\n\n# Initialize the MonoPipeline\nmono_pipeline = MonoPipeline(filters=filters)\n\n# Example data\nx_data = XYData(_hash='x_data', _path='/tmp', _value=[[1, 2], [3, 4]])\ny_data = XYData(_hash='y_data', _path='/tmp', _value=[0, 1])\n\n# Start the pipeline\nresults = mono_pipeline.start(x_data, y_data, None)\n\n# Evaluate the pipeline\nevaluation_results = mono_pipeline.evaluate(x_data, y_data, results)\nprint(evaluation_results)\n</code></pre>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Ensure that the filters used in the pipeline are compatible with the input data.</li> <li>Define a clear combiner function if the default concatenation does not meet your needs.</li> <li>Monitor the performance and resource utilization to optimize parallel execution.</li> </ol>"},{"location":"api/plugins/pipelines/parallel/mono_pipeline/#conclusion","title":"Conclusion","text":"<p><code>MonoPipeline</code> provides a flexible and efficient way to execute multiple pipelines in parallel, enhancing feature sets through combined outputs. By following the best practices and examples provided, you can effectively integrate this pipeline into your machine learning workflows.</p>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/","title":"F3Pipeline","text":""},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#framework3.plugins.pipelines.sequential.f3_pipeline.F3Pipeline","title":"<code>F3Pipeline</code>","text":"<p>               Bases: <code>SequentialPipeline</code></p> <p>A flexible sequential pipeline implementation for machine learning workflows.</p> <p>F3Pipeline allows chaining multiple filters together and applying metrics for evaluation. It supports fitting, predicting, and evaluating data through the pipeline.</p> Key Features <ul> <li>Sequential application of multiple filters</li> <li>Support for various metrics for evaluation</li> <li>Configurable data storage and logging options</li> </ul> Usage <pre><code>from framework3.plugins.pipelines.sequential import F3Pipeline\nfrom framework3.plugins.filters.transformation import PCAPlugin\nfrom framework3.plugins.filters.classification import ClassifierSVMPlugin\nfrom framework3.plugins.metrics.classification import F1, Precision, Recall\nfrom framework3.base import XYData\nimport numpy as np\n\n# Create a pipeline with PCA and SVM\npipeline = F3Pipeline(\n    filters=[\n        PCAPlugin(n_components=2),\n        ClassifierSVMPlugin(kernel='rbf', C=1.0)\n    ],\n    metrics=[F1(), Precision(), Recall()]\n)\n\n# Prepare some dummy data\nX = XYData(value=np.random.rand(100, 10))\ny = XYData(value=np.random.randint(0, 2, 100))\n\n# Fit the pipeline\npipeline.fit(X, y)\n\n# Make predictions\ny_pred = pipeline.predict(X)\n\n# Evaluate the pipeline\nresults = pipeline.evaluate(X, y, y_pred)\nprint(results)\n</code></pre> <p>Attributes:</p> Name Type Description <code>filters</code> <code>List[BaseFilter]</code> <p>List of filters to be applied in the pipeline.</p> <code>metrics</code> <code>List[BaseMetric]</code> <p>List of metrics for evaluation.</p> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing data in storage.</p> <code>store</code> <code>bool</code> <p>Whether to store intermediate results.</p> <code>log</code> <code>bool</code> <p>Whether to log pipeline operations.</p> <p>Methods:</p> Name Description <code>fit</code> <p>XYData, y: Optional[XYData]) -&gt; None | float: Fit the pipeline to the input data.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the fitted pipeline.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, float]: Evaluate the pipeline using specified metrics.</p> <code>start</code> <p>XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]: Start the pipeline execution by fitting and optionally predicting.</p> Source code in <code>framework3/plugins/pipelines/sequential/f3_pipeline.py</code> <pre><code>@Container.bind()\nclass F3Pipeline(SequentialPipeline):\n    \"\"\"\n    A flexible sequential pipeline implementation for machine learning workflows.\n\n    F3Pipeline allows chaining multiple filters together and applying metrics\n    for evaluation. It supports fitting, predicting, and evaluating data through the pipeline.\n\n    Key Features:\n        - Sequential application of multiple filters\n        - Support for various metrics for evaluation\n        - Configurable data storage and logging options\n\n    Usage:\n        ```python\n        from framework3.plugins.pipelines.sequential import F3Pipeline\n        from framework3.plugins.filters.transformation import PCAPlugin\n        from framework3.plugins.filters.classification import ClassifierSVMPlugin\n        from framework3.plugins.metrics.classification import F1, Precision, Recall\n        from framework3.base import XYData\n        import numpy as np\n\n        # Create a pipeline with PCA and SVM\n        pipeline = F3Pipeline(\n            filters=[\n                PCAPlugin(n_components=2),\n                ClassifierSVMPlugin(kernel='rbf', C=1.0)\n            ],\n            metrics=[F1(), Precision(), Recall()]\n        )\n\n        # Prepare some dummy data\n        X = XYData(value=np.random.rand(100, 10))\n        y = XYData(value=np.random.randint(0, 2, 100))\n\n        # Fit the pipeline\n        pipeline.fit(X, y)\n\n        # Make predictions\n        y_pred = pipeline.predict(X)\n\n        # Evaluate the pipeline\n        results = pipeline.evaluate(X, y, y_pred)\n        print(results)\n        ```\n\n    Attributes:\n        filters (List[BaseFilter]): List of filters to be applied in the pipeline.\n        metrics (List[BaseMetric]): List of metrics for evaluation.\n        overwrite (bool): Whether to overwrite existing data in storage.\n        store (bool): Whether to store intermediate results.\n        log (bool): Whether to log pipeline operations.\n\n    Methods:\n        fit(x: XYData, y: Optional[XYData]) -&gt; None | float: Fit the pipeline to the input data.\n        predict(x: XYData) -&gt; XYData: Make predictions using the fitted pipeline.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, float]:\n            Evaluate the pipeline using specified metrics.\n        start(x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n            Start the pipeline execution by fitting and optionally predicting.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    def __init__(\n        self,\n        filters: List[BaseFilter],\n        metrics: List[BaseMetric] = [],\n        overwrite: bool = False,\n        store: bool = False,\n        log: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the F3Pipeline.\n\n        Args:\n            filters (List[BaseFilter]): List of filters to be applied in the pipeline.\n            metrics (List[BaseMetric], optional): List of metrics for evaluation. Defaults to [].\n            overwrite (bool, optional): Whether to overwrite existing data. Defaults to False.\n            store (bool, optional): Whether to store intermediate results. Defaults to False.\n            log (bool, optional): Whether to log pipeline operations. Defaults to False.\n        \"\"\"\n        super().__init__(\n            filters=filters, metrics=metrics, overwrite=overwrite, store=store, log=log\n        )\n        self.filters: List[BaseFilter] = filters\n        self.metrics: List[BaseMetric] = metrics\n        self.overwrite = overwrite\n        self.store = store\n        self.log = log\n        # self._filters: List[BaseFilter] = []\n\n    def start(\n        self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n    ) -&gt; Optional[XYData]:\n        \"\"\"\n        Start the pipeline execution by fitting the model and making predictions.\n\n        This method initiates the pipeline process by fitting the model to the input data\n        and then making predictions.\n\n        Args:\n            x (XYData): The input data for fitting and prediction.\n            y (Optional[XYData]): The target data for fitting, if available.\n            X_ (Optional[XYData]): Additional input data for prediction, if different from x.\n\n        Returns:\n            Optional[XYData]: The predictions made by the pipeline, or None if an error occurs.\n\n        Raises:\n            Exception: If an error occurs during the fitting or prediction process.\n        \"\"\"\n        try:\n            self.fit(x, y)\n            if X_ is not None:\n                return self.predict(X_)\n            else:\n                return self.predict(x)\n        except Exception as e:\n            print(f\"Error during pipeline execution: {e}\")\n            raise e\n\n    def fit(self, x: XYData, y: Optional[XYData]) -&gt; None | float:\n        \"\"\"\n        Fit the pipeline to the input data.\n\n        This method applies each filter in the pipeline sequentially to the input data,\n        fitting each filter that requires training.\n\n        Args:\n            x (XYData): The input data to fit the pipeline on.\n            y (Optional[XYData]): The target data, if available.\n\n        Returns:\n            None | float: The loss value from the last fitted filter, if any.\n\n        Note:\n            Filters that raise NotTrainableFilterError will be initialized instead of fitted.\n        \"\"\"\n        self._print_acction(\"Fitting pipeline\")\n        loss = None\n        for filter in self.filters:\n            if self._verbose:\n                rprint(filter)\n            try:\n                loss = filter.fit(x, y)\n            except NotTrainableFilterError:\n                filter.init()  # Initialize filter\n\n            x = filter.predict(x)\n\n        return loss\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted pipeline.\n\n        This method applies each filter in the pipeline sequentially to the input data\n        to generate predictions.\n\n        Args:\n            x (XYData): The input data to make predictions on.\n\n        Returns:\n            XYData: The predictions made by the pipeline.\n        \"\"\"\n\n        self._print_acction(\"Predicting pipeline\")\n\n        for filter_ in self.filters:\n            if self._verbose:\n                rprint(filter_)\n            x = filter_.predict(x)\n\n        return x\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Evaluate the pipeline using the specified metrics.\n\n        This method applies each metric in the pipeline to the predicted and true values,\n        returning a dictionary of evaluation results.\n\n        Args:\n            x_data (XYData): The input data used for evaluation.\n            y_true (XYData | None): The true target values, if available.\n            y_pred (XYData): The predicted values.\n\n        Returns:\n            Dict[str, float]: A dictionary containing the evaluation results for each metric.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; results = pipeline.evaluate(x_test, y_test, y_pred)\n            &gt;&gt;&gt; print(results)\n            {'F1': 0.85, 'Precision': 0.80, 'Recall': 0.90}\n            ```\n        \"\"\"\n\n        self._print_acction(\"Evaluating pipeline...\")\n\n        evaluations = {}\n        for metric in self.metrics:\n            evaluations[metric.__class__.__name__] = metric.evaluate(\n                x_data, y_true, y_pred\n            )\n        return evaluations\n\n    def inner(self) -&gt; List[BaseFilter]:\n        \"\"\"\n        Get the list of filters in the pipeline.\n\n        Returns:\n            List[BaseFilter]: The list of filters in the pipeline.\n        \"\"\"\n        return self.filters\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#framework3.plugins.pipelines.sequential.f3_pipeline.F3Pipeline.__init__","title":"<code>__init__(filters, metrics=[], overwrite=False, store=False, log=False)</code>","text":"<p>Initialize the F3Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>List[BaseFilter]</code> <p>List of filters to be applied in the pipeline.</p> required <code>metrics</code> <code>List[BaseMetric]</code> <p>List of metrics for evaluation. Defaults to [].</p> <code>[]</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing data. Defaults to False.</p> <code>False</code> <code>store</code> <code>bool</code> <p>Whether to store intermediate results. Defaults to False.</p> <code>False</code> <code>log</code> <code>bool</code> <p>Whether to log pipeline operations. Defaults to False.</p> <code>False</code> Source code in <code>framework3/plugins/pipelines/sequential/f3_pipeline.py</code> <pre><code>def __init__(\n    self,\n    filters: List[BaseFilter],\n    metrics: List[BaseMetric] = [],\n    overwrite: bool = False,\n    store: bool = False,\n    log: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the F3Pipeline.\n\n    Args:\n        filters (List[BaseFilter]): List of filters to be applied in the pipeline.\n        metrics (List[BaseMetric], optional): List of metrics for evaluation. Defaults to [].\n        overwrite (bool, optional): Whether to overwrite existing data. Defaults to False.\n        store (bool, optional): Whether to store intermediate results. Defaults to False.\n        log (bool, optional): Whether to log pipeline operations. Defaults to False.\n    \"\"\"\n    super().__init__(\n        filters=filters, metrics=metrics, overwrite=overwrite, store=store, log=log\n    )\n    self.filters: List[BaseFilter] = filters\n    self.metrics: List[BaseMetric] = metrics\n    self.overwrite = overwrite\n    self.store = store\n    self.log = log\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#framework3.plugins.pipelines.sequential.f3_pipeline.F3Pipeline.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the pipeline using the specified metrics.</p> <p>This method applies each metric in the pipeline to the predicted and true values, returning a dictionary of evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input data used for evaluation.</p> required <code>y_true</code> <code>XYData | None</code> <p>The true target values, if available.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted values.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary containing the evaluation results for each metric.</p> Example <pre><code>&gt;&gt;&gt; results = pipeline.evaluate(x_test, y_test, y_pred)\n&gt;&gt;&gt; print(results)\n{'F1': 0.85, 'Precision': 0.80, 'Recall': 0.90}\n</code></pre> Source code in <code>framework3/plugins/pipelines/sequential/f3_pipeline.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate the pipeline using the specified metrics.\n\n    This method applies each metric in the pipeline to the predicted and true values,\n    returning a dictionary of evaluation results.\n\n    Args:\n        x_data (XYData): The input data used for evaluation.\n        y_true (XYData | None): The true target values, if available.\n        y_pred (XYData): The predicted values.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the evaluation results for each metric.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; results = pipeline.evaluate(x_test, y_test, y_pred)\n        &gt;&gt;&gt; print(results)\n        {'F1': 0.85, 'Precision': 0.80, 'Recall': 0.90}\n        ```\n    \"\"\"\n\n    self._print_acction(\"Evaluating pipeline...\")\n\n    evaluations = {}\n    for metric in self.metrics:\n        evaluations[metric.__class__.__name__] = metric.evaluate(\n            x_data, y_true, y_pred\n        )\n    return evaluations\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#framework3.plugins.pipelines.sequential.f3_pipeline.F3Pipeline.fit","title":"<code>fit(x, y)</code>","text":"<p>Fit the pipeline to the input data.</p> <p>This method applies each filter in the pipeline sequentially to the input data, fitting each filter that requires training.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data to fit the pipeline on.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target data, if available.</p> required <p>Returns:</p> Type Description <code>None | float</code> <p>None | float: The loss value from the last fitted filter, if any.</p> Note <p>Filters that raise NotTrainableFilterError will be initialized instead of fitted.</p> Source code in <code>framework3/plugins/pipelines/sequential/f3_pipeline.py</code> <pre><code>def fit(self, x: XYData, y: Optional[XYData]) -&gt; None | float:\n    \"\"\"\n    Fit the pipeline to the input data.\n\n    This method applies each filter in the pipeline sequentially to the input data,\n    fitting each filter that requires training.\n\n    Args:\n        x (XYData): The input data to fit the pipeline on.\n        y (Optional[XYData]): The target data, if available.\n\n    Returns:\n        None | float: The loss value from the last fitted filter, if any.\n\n    Note:\n        Filters that raise NotTrainableFilterError will be initialized instead of fitted.\n    \"\"\"\n    self._print_acction(\"Fitting pipeline\")\n    loss = None\n    for filter in self.filters:\n        if self._verbose:\n            rprint(filter)\n        try:\n            loss = filter.fit(x, y)\n        except NotTrainableFilterError:\n            filter.init()  # Initialize filter\n\n        x = filter.predict(x)\n\n    return loss\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#framework3.plugins.pipelines.sequential.f3_pipeline.F3Pipeline.inner","title":"<code>inner()</code>","text":"<p>Get the list of filters in the pipeline.</p> <p>Returns:</p> Type Description <code>List[BaseFilter]</code> <p>List[BaseFilter]: The list of filters in the pipeline.</p> Source code in <code>framework3/plugins/pipelines/sequential/f3_pipeline.py</code> <pre><code>def inner(self) -&gt; List[BaseFilter]:\n    \"\"\"\n    Get the list of filters in the pipeline.\n\n    Returns:\n        List[BaseFilter]: The list of filters in the pipeline.\n    \"\"\"\n    return self.filters\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#framework3.plugins.pipelines.sequential.f3_pipeline.F3Pipeline.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted pipeline.</p> <p>This method applies each filter in the pipeline sequentially to the input data to generate predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data to make predictions on.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predictions made by the pipeline.</p> Source code in <code>framework3/plugins/pipelines/sequential/f3_pipeline.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted pipeline.\n\n    This method applies each filter in the pipeline sequentially to the input data\n    to generate predictions.\n\n    Args:\n        x (XYData): The input data to make predictions on.\n\n    Returns:\n        XYData: The predictions made by the pipeline.\n    \"\"\"\n\n    self._print_acction(\"Predicting pipeline\")\n\n    for filter_ in self.filters:\n        if self._verbose:\n            rprint(filter_)\n        x = filter_.predict(x)\n\n    return x\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#framework3.plugins.pipelines.sequential.f3_pipeline.F3Pipeline.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the pipeline execution by fitting the model and making predictions.</p> <p>This method initiates the pipeline process by fitting the model to the input data and then making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input data for fitting and prediction.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target data for fitting, if available.</p> required <code>X_</code> <code>Optional[XYData]</code> <p>Additional input data for prediction, if different from x.</p> required <p>Returns:</p> Type Description <code>Optional[XYData]</code> <p>Optional[XYData]: The predictions made by the pipeline, or None if an error occurs.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the fitting or prediction process.</p> Source code in <code>framework3/plugins/pipelines/sequential/f3_pipeline.py</code> <pre><code>def start(\n    self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n) -&gt; Optional[XYData]:\n    \"\"\"\n    Start the pipeline execution by fitting the model and making predictions.\n\n    This method initiates the pipeline process by fitting the model to the input data\n    and then making predictions.\n\n    Args:\n        x (XYData): The input data for fitting and prediction.\n        y (Optional[XYData]): The target data for fitting, if available.\n        X_ (Optional[XYData]): Additional input data for prediction, if different from x.\n\n    Returns:\n        Optional[XYData]: The predictions made by the pipeline, or None if an error occurs.\n\n    Raises:\n        Exception: If an error occurs during the fitting or prediction process.\n    \"\"\"\n    try:\n        self.fit(x, y)\n        if X_ is not None:\n            return self.predict(X_)\n        else:\n            return self.predict(x)\n    except Exception as e:\n        print(f\"Error during pipeline execution: {e}\")\n        raise e\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#overview","title":"Overview","text":"<p>The F3Pipeline is a flexible and powerful pipeline implementation in the LabChain ecosystem. It allows you to chain multiple data processing steps, machine learning models, and evaluation metrics into a single, cohesive workflow.</p>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#key-features","title":"Key Features","text":"<ul> <li>Seamless integration of multiple plugins (filters, transformers, models)</li> <li>Built-in support for various metrics</li> <li>Caching capabilities for improved performance</li> <li>Nested pipeline support for complex workflows</li> </ul>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example of how to create and use an F3Pipeline:</p> <pre><code>from framework3.plugins.pipelines import F3Pipeline\nfrom framework3.plugins.filters.transformation import PCAPlugin\nfrom framework3.plugins.filters.classification import SVMClassifier\nfrom framework3.plugins.metrics import F1Score, Accuracy\nfrom framework3.base.base_types import XYData\nimport numpy as np\n\n# Create a pipeline\npipeline = F3Pipeline(\n    plugins=[\n        PCAPlugin(n_components=2),\n        SVMClassifier(kernel='rbf')\n    ],\n    metrics=[F1Score(), Accuracy()]\n)\n\n# Generate some dummy data\nX = XYData(value=np.random.rand(100, 10))\ny = XYData(value=np.random.randint(0, 2, 100))\n\n# Fit the pipeline\npipeline.fit(X, y)\n\n# Make predictions\ny_pred = pipeline.predict(X)\n\n# Evaluate the pipeline\nresults = pipeline.evaluate(X, y, y_pred)\nprint(results)\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#nested-pipelines","title":"Nested Pipelines","text":"<p>F3Pipeline supports nesting, allowing you to create more complex workflows:</p> <pre><code>from framework3.plugins.pipelines import F3Pipeline\nfrom framework3.plugins.filters.transformation import NormalizationPlugin\nfrom framework3.plugins.filters.feature_selection import VarianceThresholdPlugin\n\n# Create a sub-pipeline\nfeature_engineering = F3Pipeline(\n    plugins=[\n        NormalizationPlugin(),\n        VarianceThresholdPlugin(threshold=0.1)\n    ],\n    metrics=[]\n)\n\n# Create the main pipeline\nmain_pipeline = F3Pipeline(\n    plugins=[\n        feature_engineering,\n        SVMClassifier(kernel='linear')\n    ],\n    metrics=[F1Score(), Accuracy()]\n)\n\n# Use the main pipeline as before\nmain_pipeline.fit(X, y)\ny_pred = main_pipeline.predict(X)\nresults = main_pipeline.evaluate(X, y, y_pred)\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#caching","title":"Caching","text":"<p>F3Pipeline supports caching of intermediate results and fitted models for improved performance:</p> <pre><code>from framework3.plugins.filters.cached_filter import Cached\nfrom framework3.plugins.filters.transformation import PCAPlugin\n\npipeline = F3Pipeline(\n    plugins=[\n        Cached(\n            filter=PCAPlugin(n_components=2),\n            cache_data=True,\n            cache_filter=True,\n            overwrite=False\n        ),\n        SVMClassifier()\n    ],\n    metrics=[F1Score()]\n)\n\n# The PCA transformation will be cached after the first run\npipeline.fit(X, y)\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#api-reference","title":"API Reference","text":""},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#f3pipeline_1","title":"F3Pipeline","text":"<pre><code>class F3Pipeline(BasePipeline):\n    def __init__(self, plugins: List[BasePlugin], metrics: List[BaseMetric], overwrite: bool = False, store: bool = False, log: bool = False) -&gt; None:\n        \"\"\"\n        Initialize the F3Pipeline.\n\n        Args:\n            plugins (List[BasePlugin]): List of plugins to be applied in the pipeline.\n            metrics (List[BaseMetric]): List of metrics for evaluation.\n            overwrite (bool, optional): Whether to overwrite existing data. Defaults to False.\n            store (bool, optional): Whether to store intermediate results. Defaults to False.\n            log (bool, optional): Whether to log pipeline operations. Defaults to False.\n        \"\"\"\n</code></pre>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#methods","title":"Methods","text":"<ul> <li><code>fit(self, x: XYData, y: Optional[XYData])</code>: Fit the pipeline to the input data.</li> <li><code>predict(self, x: XYData) -&gt; XYData</code>: Make predictions using the fitted pipeline.</li> <li><code>evaluate(self, x_data: XYData, y_true: XYData|None, y_pred: XYData) -&gt; Dict[str, float]</code>: Evaluate the pipeline using the specified metrics.</li> </ul>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Order Matters: The order of plugins in the pipeline is crucial. Ensure that your data preprocessing steps come before your model.</p> </li> <li> <p>Caching: Use caching for computationally expensive steps, especially when you're iterating on your pipeline design.</p> </li> <li> <p>Nested Pipelines: Use nested pipelines to organize complex workflows into logical sub-components.</p> </li> <li> <p>Metrics: Include multiple relevant metrics to get a comprehensive view of your pipeline's performance.</p> </li> <li> <p>Cross-Validation: Consider using cross-validation techniques in conjunction with F3Pipeline for more robust model evaluation.</p> </li> <li> <p>Logging: Enable logging to get insights into the pipeline's operation and to help with debugging.</p> </li> <li> <p>Parameter Tuning: Use F3Pipeline in conjunction with hyperparameter tuning techniques to optimize your entire workflow.</p> </li> </ol>"},{"location":"api/plugins/pipelines/sequential/f3_pipeline/#conclusion","title":"Conclusion","text":"<p>F3Pipeline provides a powerful and flexible way to build complex data processing and machine learning workflows in LabChain. By combining multiple plugins, nested pipelines, and caching capabilities, you can create efficient and maintainable pipelines for a wide range of tasks.</p>"},{"location":"api/plugins/splitters/kfold_splitter/","title":"KFoldSplitter","text":""},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter","title":"<code>framework3.plugins.splitter.cross_validation_splitter</code>","text":""},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter","title":"<code>KFoldSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>A K-Fold cross-validation splitter for evaluating machine learning models.</p> <p>This class implements K-Fold cross-validation, which splits the dataset into K equally sized folds. The model is trained on K-1 folds and validated on the remaining fold. This process is repeated K times, with each fold serving as the validation set once.</p> Key Features <ul> <li>Configurable number of splits</li> <li>Option to shuffle data before splitting</li> <li>Supports custom pipelines for model training and evaluation</li> <li>Provides mean loss across all folds</li> </ul> Usage <pre><code>from framework3.plugins.splitter import KFoldSplitter\nfrom framework3.plugins.pipelines.sequential import F3Pipeline\nfrom framework3.base import XYData\nimport numpy as np\n\n# Create a dummy pipeline\npipeline = F3Pipeline(filters=[...], metrics=[...])\n\n# Create the KFoldSplitter\nsplitter = KFoldSplitter(n_splits=5, shuffle=True, random_state=42, pipeline=pipeline)\n\n# Prepare some dummy data\nX = XYData(value=np.random.rand(100, 10))\ny = XYData(value=np.random.randint(0, 2, 100))\n\n# Fit and evaluate the model using cross-validation\nmean_loss = splitter.fit(X, y)\nprint(f\"Mean loss across folds: {mean_loss}\")\n\n# Make predictions on new data\nX_new = XYData(value=np.random.rand(20, 10))\npredictions = splitter.predict(X_new)\n</code></pre> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>Number of folds. Must be at least 2.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>random_state</code> <code>int</code> <p>Controls the shuffling applied to the data before applying the split.</p> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be used for training and evaluation.</p> <p>Methods:</p> Name Description <code>split</code> <p>BaseFilter): Set the pipeline for the splitter.</p> <code>fit</code> <p>XYData, y: XYData | None) -&gt; Optional[float]: Perform K-Fold cross-validation.</p> <code>predict</code> <p>XYData) -&gt; XYData: Make predictions using the fitted pipeline.</p> <code>evaluate</code> <p>XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]: Evaluate the pipeline using the last fold.</p> <code>start</code> <p>XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]: Start the cross-validation process and optionally make predictions.</p> Source code in <code>framework3/plugins/splitter/cross_validation_splitter.py</code> <pre><code>@Container.bind()\nclass KFoldSplitter(BaseSplitter):\n    \"\"\"\n    A K-Fold cross-validation splitter for evaluating machine learning models.\n\n    This class implements K-Fold cross-validation, which splits the dataset into K equally sized folds.\n    The model is trained on K-1 folds and validated on the remaining fold. This process is repeated K times,\n    with each fold serving as the validation set once.\n\n    Key Features:\n        - Configurable number of splits\n        - Option to shuffle data before splitting\n        - Supports custom pipelines for model training and evaluation\n        - Provides mean loss across all folds\n\n    Usage:\n        ```python\n        from framework3.plugins.splitter import KFoldSplitter\n        from framework3.plugins.pipelines.sequential import F3Pipeline\n        from framework3.base import XYData\n        import numpy as np\n\n        # Create a dummy pipeline\n        pipeline = F3Pipeline(filters=[...], metrics=[...])\n\n        # Create the KFoldSplitter\n        splitter = KFoldSplitter(n_splits=5, shuffle=True, random_state=42, pipeline=pipeline)\n\n        # Prepare some dummy data\n        X = XYData(value=np.random.rand(100, 10))\n        y = XYData(value=np.random.randint(0, 2, 100))\n\n        # Fit and evaluate the model using cross-validation\n        mean_loss = splitter.fit(X, y)\n        print(f\"Mean loss across folds: {mean_loss}\")\n\n        # Make predictions on new data\n        X_new = XYData(value=np.random.rand(20, 10))\n        predictions = splitter.predict(X_new)\n        ```\n\n    Attributes:\n        n_splits (int): Number of folds. Must be at least 2.\n        shuffle (bool): Whether to shuffle the data before splitting.\n        random_state (int): Controls the shuffling applied to the data before applying the split.\n        pipeline (BaseFilter | None): The pipeline to be used for training and evaluation.\n\n    Methods:\n        split(pipeline: BaseFilter): Set the pipeline for the splitter.\n        fit(x: XYData, y: XYData | None) -&gt; Optional[float]: Perform K-Fold cross-validation.\n        predict(x: XYData) -&gt; XYData: Make predictions using the fitted pipeline.\n        evaluate(x_data: XYData, y_true: XYData | None, y_pred: XYData) -&gt; Dict[str, Any]:\n            Evaluate the pipeline using the last fold.\n        start(x: XYData, y: Optional[XYData], X_: Optional[XYData]) -&gt; Optional[XYData]:\n            Start the cross-validation process and optionally make predictions.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_splits: int = 5,\n        shuffle: bool = True,\n        random_state: int = 42,\n        pipeline: BaseFilter | None = None,\n        # evaluator: BaseMetric | None = None\n    ):\n        \"\"\"\n        Initialize the KFoldSplitter.\n\n        Args:\n            n_splits (int, optional): Number of folds. Must be at least 2. Defaults to 5.\n            shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n            random_state (int, optional): Controls the shuffling applied to the data before applying the split. Defaults to 42.\n            pipeline (BaseFilter | None, optional): The pipeline to be used for training and evaluation. Defaults to None.\n        \"\"\"\n        super().__init__(pipeline=pipeline)\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self._kfold = KFold(\n            n_splits=n_splits, shuffle=shuffle, random_state=random_state\n        )\n        self.pipeline = pipeline\n        # self.evaluator = evaluator\n\n    def split(self, pipeline: BaseFilter):\n        \"\"\"\n        Set the pipeline for the splitter and disable its verbosity.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be used for training and evaluation.\n        \"\"\"\n        self.pipeline = pipeline\n        self.pipeline.verbose(False)\n\n    def fit(self, x: XYData, y: XYData | None) -&gt; Optional[float]:\n        \"\"\"\n        Perform K-Fold cross-validation on the given data.\n\n        This method splits the data into K folds, trains the pipeline on K-1 folds,\n        and evaluates it on the remaining fold. This process is repeated K times.\n\n        Args:\n            x (XYData): The input features.\n            y (XYData | None): The target values.\n\n        Returns:\n            Optional[float]: The mean loss across all folds, or None if no losses were calculated.\n\n        Raises:\n            ValueError: If y is None or if the pipeline is not set.\n        \"\"\"\n        self._print_acction(\"Fitting with KFold Splitter...\")\n        if self._verbose:\n            rprint(self.pipeline)\n\n        X = x.value\n        if y is None:  # type: ignore\n            raise ValueError(\"y must be provided for KFold split\")\n\n        if self.pipeline is None:\n            raise ValueError(\"Pipeline must be fitted before splitting\")\n\n        losses = []\n        splits = self._kfold.split(X)\n        for train_idx, val_idx in tqdm(\n            splits, total=self._kfold.get_n_splits(X), disable=not self._verbose\n        ):\n            X_train = x.split(train_idx)\n            X_val = x.split(val_idx)\n            y_train = y.split(train_idx)\n            y_val = y.split(val_idx)\n\n            pipeline = cast(\n                BasePipeline,\n                BasePlugin.build_from_dump(self.pipeline.item_dump(), Container.pif),\n            )\n\n            pipeline.fit(X_train, y_train)\n\n            _y = pipeline.predict(X_val)\n\n            loss = pipeline.evaluate(X_val, y_val, _y)\n            losses.append(float(next(iter(loss.values()))))\n\n            self.clear_memory()\n\n        return float(np.mean(losses) if losses else 0.0)\n\n    def start(\n        self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n    ) -&gt; Optional[XYData]:\n        \"\"\"\n        Start the cross-validation process and optionally make predictions.\n\n        This method performs cross-validation by fitting the model and then\n        makes predictions if X_ is provided.\n\n        Args:\n            x (XYData): The input features for training.\n            y (Optional[XYData]): The target values for training.\n            X_ (Optional[XYData]): The input features for prediction, if different from x.\n\n        Returns:\n            Optional[XYData]: Prediction results if X_ is provided, else None.\n\n        Raises:\n            Exception: If an error occurs during the process.\n        \"\"\"\n        try:\n            self.fit(x, y)\n            if X_ is not None:\n                return self.predict(X_)\n            else:\n                return self.predict(x)\n        except Exception as e:\n            print(f\"Error during pipeline execution: {e}\")\n            raise e\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted pipeline.\n\n        This method uses the pipeline that was fitted during cross-validation\n        to make predictions on new data.\n\n        Args:\n            x (XYData): The input features for prediction.\n\n        Returns:\n            XYData: The predictions made by the pipeline.\n\n        Raises:\n            ValueError: If the pipeline has not been fitted.\n        \"\"\"\n        self._print_acction(\"Predicting with KFold Splitter...\")\n        if self._verbose:\n            rprint(self.pipeline)\n\n        # X = x.value\n        if self.pipeline is None:\n            raise ValueError(\"Pipeline must be fitted before prediction\")\n\n        return self.pipeline.predict(x)\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the pipeline using the provided data.\n\n        This method uses the pipeline's evaluate method to assess its performance\n        on the given data.\n\n        Args:\n            x_data (XYData): The input features.\n            y_true (XYData | None): The true target values.\n            y_pred (XYData): The predicted target values.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation metrics.\n\n        Raises:\n            ValueError: If the pipeline has not been fitted.\n        \"\"\"\n        if self.pipeline is None:\n            raise ValueError(\"Pipeline must be fitted before evaluation\")\n        return self.pipeline.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.n_splits","title":"<code>n_splits = n_splits</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.pipeline","title":"<code>pipeline = pipeline</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.random_state","title":"<code>random_state = random_state</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.shuffle","title":"<code>shuffle = shuffle</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.__init__","title":"<code>__init__(n_splits=5, shuffle=True, random_state=42, pipeline=None)</code>","text":"<p>Initialize the KFoldSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Number of folds. Must be at least 2. Defaults to 5.</p> <code>5</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting. Defaults to True.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Controls the shuffling applied to the data before applying the split. Defaults to 42.</p> <code>42</code> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be used for training and evaluation. Defaults to None.</p> <code>None</code> Source code in <code>framework3/plugins/splitter/cross_validation_splitter.py</code> <pre><code>def __init__(\n    self,\n    n_splits: int = 5,\n    shuffle: bool = True,\n    random_state: int = 42,\n    pipeline: BaseFilter | None = None,\n    # evaluator: BaseMetric | None = None\n):\n    \"\"\"\n    Initialize the KFoldSplitter.\n\n    Args:\n        n_splits (int, optional): Number of folds. Must be at least 2. Defaults to 5.\n        shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n        random_state (int, optional): Controls the shuffling applied to the data before applying the split. Defaults to 42.\n        pipeline (BaseFilter | None, optional): The pipeline to be used for training and evaluation. Defaults to None.\n    \"\"\"\n    super().__init__(pipeline=pipeline)\n    self.n_splits = n_splits\n    self.shuffle = shuffle\n    self.random_state = random_state\n    self._kfold = KFold(\n        n_splits=n_splits, shuffle=shuffle, random_state=random_state\n    )\n    self.pipeline = pipeline\n</code></pre>"},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the pipeline using the provided data.</p> <p>This method uses the pipeline's evaluate method to assess its performance on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>The input features.</p> required <code>y_true</code> <code>XYData | None</code> <p>The true target values.</p> required <code>y_pred</code> <code>XYData</code> <p>The predicted target values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation metrics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline has not been fitted.</p> Source code in <code>framework3/plugins/splitter/cross_validation_splitter.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the pipeline using the provided data.\n\n    This method uses the pipeline's evaluate method to assess its performance\n    on the given data.\n\n    Args:\n        x_data (XYData): The input features.\n        y_true (XYData | None): The true target values.\n        y_pred (XYData): The predicted target values.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation metrics.\n\n    Raises:\n        ValueError: If the pipeline has not been fitted.\n    \"\"\"\n    if self.pipeline is None:\n        raise ValueError(\"Pipeline must be fitted before evaluation\")\n    return self.pipeline.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.fit","title":"<code>fit(x, y)</code>","text":"<p>Perform K-Fold cross-validation on the given data.</p> <p>This method splits the data into K folds, trains the pipeline on K-1 folds, and evaluates it on the remaining fold. This process is repeated K times.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features.</p> required <code>y</code> <code>XYData | None</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The mean loss across all folds, or None if no losses were calculated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y is None or if the pipeline is not set.</p> Source code in <code>framework3/plugins/splitter/cross_validation_splitter.py</code> <pre><code>def fit(self, x: XYData, y: XYData | None) -&gt; Optional[float]:\n    \"\"\"\n    Perform K-Fold cross-validation on the given data.\n\n    This method splits the data into K folds, trains the pipeline on K-1 folds,\n    and evaluates it on the remaining fold. This process is repeated K times.\n\n    Args:\n        x (XYData): The input features.\n        y (XYData | None): The target values.\n\n    Returns:\n        Optional[float]: The mean loss across all folds, or None if no losses were calculated.\n\n    Raises:\n        ValueError: If y is None or if the pipeline is not set.\n    \"\"\"\n    self._print_acction(\"Fitting with KFold Splitter...\")\n    if self._verbose:\n        rprint(self.pipeline)\n\n    X = x.value\n    if y is None:  # type: ignore\n        raise ValueError(\"y must be provided for KFold split\")\n\n    if self.pipeline is None:\n        raise ValueError(\"Pipeline must be fitted before splitting\")\n\n    losses = []\n    splits = self._kfold.split(X)\n    for train_idx, val_idx in tqdm(\n        splits, total=self._kfold.get_n_splits(X), disable=not self._verbose\n    ):\n        X_train = x.split(train_idx)\n        X_val = x.split(val_idx)\n        y_train = y.split(train_idx)\n        y_val = y.split(val_idx)\n\n        pipeline = cast(\n            BasePipeline,\n            BasePlugin.build_from_dump(self.pipeline.item_dump(), Container.pif),\n        )\n\n        pipeline.fit(X_train, y_train)\n\n        _y = pipeline.predict(X_val)\n\n        loss = pipeline.evaluate(X_val, y_val, _y)\n        losses.append(float(next(iter(loss.values()))))\n\n        self.clear_memory()\n\n    return float(np.mean(losses) if losses else 0.0)\n</code></pre>"},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted pipeline.</p> <p>This method uses the pipeline that was fitted during cross-validation to make predictions on new data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features for prediction.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>The predictions made by the pipeline.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline has not been fitted.</p> Source code in <code>framework3/plugins/splitter/cross_validation_splitter.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted pipeline.\n\n    This method uses the pipeline that was fitted during cross-validation\n    to make predictions on new data.\n\n    Args:\n        x (XYData): The input features for prediction.\n\n    Returns:\n        XYData: The predictions made by the pipeline.\n\n    Raises:\n        ValueError: If the pipeline has not been fitted.\n    \"\"\"\n    self._print_acction(\"Predicting with KFold Splitter...\")\n    if self._verbose:\n        rprint(self.pipeline)\n\n    # X = x.value\n    if self.pipeline is None:\n        raise ValueError(\"Pipeline must be fitted before prediction\")\n\n    return self.pipeline.predict(x)\n</code></pre>"},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.split","title":"<code>split(pipeline)</code>","text":"<p>Set the pipeline for the splitter and disable its verbosity.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be used for training and evaluation.</p> required Source code in <code>framework3/plugins/splitter/cross_validation_splitter.py</code> <pre><code>def split(self, pipeline: BaseFilter):\n    \"\"\"\n    Set the pipeline for the splitter and disable its verbosity.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be used for training and evaluation.\n    \"\"\"\n    self.pipeline = pipeline\n    self.pipeline.verbose(False)\n</code></pre>"},{"location":"api/plugins/splitters/kfold_splitter/#framework3.plugins.splitter.cross_validation_splitter.KFoldSplitter.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the cross-validation process and optionally make predictions.</p> <p>This method performs cross-validation by fitting the model and then makes predictions if X_ is provided.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>The input features for training.</p> required <code>y</code> <code>Optional[XYData]</code> <p>The target values for training.</p> required <code>X_</code> <code>Optional[XYData]</code> <p>The input features for prediction, if different from x.</p> required <p>Returns:</p> Type Description <code>Optional[XYData]</code> <p>Optional[XYData]: Prediction results if X_ is provided, else None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the process.</p> Source code in <code>framework3/plugins/splitter/cross_validation_splitter.py</code> <pre><code>def start(\n    self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n) -&gt; Optional[XYData]:\n    \"\"\"\n    Start the cross-validation process and optionally make predictions.\n\n    This method performs cross-validation by fitting the model and then\n    makes predictions if X_ is provided.\n\n    Args:\n        x (XYData): The input features for training.\n        y (Optional[XYData]): The target values for training.\n        X_ (Optional[XYData]): The input features for prediction, if different from x.\n\n    Returns:\n        Optional[XYData]: Prediction results if X_ is provided, else None.\n\n    Raises:\n        Exception: If an error occurs during the process.\n    \"\"\"\n    try:\n        self.fit(x, y)\n        if X_ is not None:\n            return self.predict(X_)\n        else:\n            return self.predict(x)\n    except Exception as e:\n        print(f\"Error during pipeline execution: {e}\")\n        raise e\n</code></pre>"},{"location":"api/plugins/splitters/stratified_kfold_splitter/","title":"StratifiedKFoldSplitter","text":""},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter","title":"<code>framework3.plugins.splitter.stratified_cross_validation_splitter</code>","text":""},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter","title":"<code>StratifiedKFoldSplitter</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>A Stratified K-Fold cross-validation splitter for evaluating classification models.</p> <p>This class implements Stratified K-Fold cross-validation, which splits the dataset into K folds while preserving the percentage of samples for each class. It is particularly useful for imbalanced datasets.</p> Key Features <ul> <li>Preserves label distribution across folds</li> <li>Configurable number of splits</li> <li>Option to shuffle data before splitting</li> <li>Supports custom pipelines for model training and evaluation</li> <li>Provides mean loss across all folds</li> </ul> Usage <pre><code>from framework3.plugins.splitter import StratifiedKFoldSplitter\nfrom framework3.plugins.pipelines.sequential import F3Pipeline\nfrom framework3.base import XYData\nimport numpy as np\n\npipeline = F3Pipeline(filters=[...], metrics=[...])\nsplitter = StratifiedKFoldSplitter(n_splits=5, shuffle=True, random_state=42, pipeline=pipeline)\n\nX = XYData(value=np.random.rand(100, 10))\ny = XYData(value=np.random.randint(0, 2, 100))\n\nmean_loss = splitter.fit(X, y)\nprint(f\"Mean loss across folds: {mean_loss}\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>Number of folds.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline to be used for training and evaluation.</p> Source code in <code>framework3/plugins/splitter/stratified_cross_validation_splitter.py</code> <pre><code>@Container.bind()\nclass StratifiedKFoldSplitter(BaseSplitter):\n    \"\"\"\n    A Stratified K-Fold cross-validation splitter for evaluating classification models.\n\n    This class implements Stratified K-Fold cross-validation, which splits the dataset into K folds\n    while preserving the percentage of samples for each class. It is particularly useful for imbalanced datasets.\n\n    Key Features:\n        - Preserves label distribution across folds\n        - Configurable number of splits\n        - Option to shuffle data before splitting\n        - Supports custom pipelines for model training and evaluation\n        - Provides mean loss across all folds\n\n    Usage:\n        ```python\n        from framework3.plugins.splitter import StratifiedKFoldSplitter\n        from framework3.plugins.pipelines.sequential import F3Pipeline\n        from framework3.base import XYData\n        import numpy as np\n\n        pipeline = F3Pipeline(filters=[...], metrics=[...])\n        splitter = StratifiedKFoldSplitter(n_splits=5, shuffle=True, random_state=42, pipeline=pipeline)\n\n        X = XYData(value=np.random.rand(100, 10))\n        y = XYData(value=np.random.randint(0, 2, 100))\n\n        mean_loss = splitter.fit(X, y)\n        print(f\"Mean loss across folds: {mean_loss}\")\n        ```\n\n    Attributes:\n        n_splits (int): Number of folds.\n        shuffle (bool): Whether to shuffle the data before splitting.\n        random_state (int): Random seed for reproducibility.\n        pipeline (BaseFilter | None): The pipeline to be used for training and evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_splits: int = 5,\n        shuffle: bool = True,\n        random_state: int = 42,\n        pipeline: BaseFilter | None = None,\n    ):\n        \"\"\"\n        Initialize the StratifiedKFoldSplitter.\n\n        Args:\n            n_splits (int): Number of folds. Must be at least 2.\n            shuffle (bool): Whether to shuffle the data before splitting.\n            random_state (int): Controls the shuffling applied to the data before splitting.\n            pipeline (BaseFilter | None): The pipeline used for model training and evaluation.\n        \"\"\"\n        super().__init__(pipeline=pipeline)\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self._skf = StratifiedKFold(\n            n_splits=n_splits, shuffle=shuffle, random_state=random_state\n        )\n        self.pipeline = pipeline\n\n    def split(self, pipeline: BaseFilter):\n        \"\"\"\n        Set the pipeline for the splitter and disable its verbosity.\n\n        Args:\n            pipeline (BaseFilter): The pipeline used for training and evaluation.\n        \"\"\"\n        self.pipeline = pipeline\n        self.pipeline.verbose(False)\n\n    def fit(self, x: XYData, y: XYData | None) -&gt; Optional[float]:\n        \"\"\"\n        Perform Stratified K-Fold cross-validation on the given data.\n\n        Args:\n            x (XYData): Input features.\n            y (XYData | None): Target labels.\n\n        Returns:\n            Optional[float]: Mean loss across all folds.\n\n        Raises:\n            ValueError: If y is None or the pipeline is not set.\n        \"\"\"\n        self._print_acction(\"Fitting with StratifiedKFold Splitter...\")\n        if self._verbose:\n            rprint(self.pipeline)\n\n        if y is None:\n            raise ValueError(\"y must be provided for Stratified K-Fold split\")\n\n        if self.pipeline is None:\n            raise ValueError(\"Pipeline must be fitted before splitting\")\n\n        X = x.value\n        Y = y.value\n\n        losses = []\n        splits = self._skf.split(X, Y)\n        for train_idx, val_idx in tqdm(\n            splits, total=self._skf.get_n_splits(X, Y), disable=not self._verbose\n        ):\n            X_train = x.split(train_idx)\n            X_val = x.split(val_idx)\n            y_train = y.split(train_idx)\n            y_val = y.split(val_idx)\n\n            pipeline = cast(\n                BasePipeline,\n                BasePlugin.build_from_dump(self.pipeline.item_dump(), Container.pif),\n            )\n\n            pipeline.fit(X_train, y_train)\n\n            _y = pipeline.predict(X_val)\n\n            loss = pipeline.evaluate(X_val, y_val, _y)\n            losses.append(float(next(iter(loss.values()))))\n\n            self.clear_memory()\n\n        return float(np.mean(losses) if losses else 0.0)\n\n    def start(\n        self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n    ) -&gt; Optional[XYData]:\n        \"\"\"\n        Start the cross-validation process and optionally make predictions.\n\n        Args:\n            x (XYData): Input training features.\n            y (Optional[XYData]): Target labels.\n            X_ (Optional[XYData]): Optional input data for prediction.\n\n        Returns:\n            Optional[XYData]: Predictions if X_ is provided, else predictions on training data.\n\n        Raises:\n            Exception: If any error occurs during execution.\n        \"\"\"\n        try:\n            self.fit(x, y)\n            if X_ is not None:\n                return self.predict(X_)\n            else:\n                return self.predict(x)\n        except Exception as e:\n            print(f\"Error during pipeline execution: {e}\")\n            raise e\n\n    def predict(self, x: XYData) -&gt; XYData:\n        \"\"\"\n        Make predictions using the fitted pipeline.\n\n        Args:\n            x (XYData): Input data for prediction.\n\n        Returns:\n            XYData: Predictions from the trained pipeline.\n\n        Raises:\n            ValueError: If pipeline is not fitted.\n        \"\"\"\n        self._print_acction(\"Predicting with StratifiedKFold Splitter...\")\n        if self._verbose:\n            rprint(self.pipeline)\n\n        if self.pipeline is None:\n            raise ValueError(\"Pipeline must be fitted before prediction\")\n\n        return self.pipeline.predict(x)\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the pipeline using the provided data.\n\n        Args:\n            x_data (XYData): Input features.\n            y_true (XYData | None): Ground truth labels.\n            y_pred (XYData): Predictions from the pipeline.\n\n        Returns:\n            Dict[str, Any]: Evaluation metrics.\n\n        Raises:\n            ValueError: If the pipeline is not fitted.\n        \"\"\"\n        if self.pipeline is None:\n            raise ValueError(\"Pipeline must be fitted before evaluation\")\n        return self.pipeline.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.n_splits","title":"<code>n_splits = n_splits</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.pipeline","title":"<code>pipeline = pipeline</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.random_state","title":"<code>random_state = random_state</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.shuffle","title":"<code>shuffle = shuffle</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.__init__","title":"<code>__init__(n_splits=5, shuffle=True, random_state=42, pipeline=None)</code>","text":"<p>Initialize the StratifiedKFoldSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Number of folds. Must be at least 2.</p> <code>5</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Controls the shuffling applied to the data before splitting.</p> <code>42</code> <code>pipeline</code> <code>BaseFilter | None</code> <p>The pipeline used for model training and evaluation.</p> <code>None</code> Source code in <code>framework3/plugins/splitter/stratified_cross_validation_splitter.py</code> <pre><code>def __init__(\n    self,\n    n_splits: int = 5,\n    shuffle: bool = True,\n    random_state: int = 42,\n    pipeline: BaseFilter | None = None,\n):\n    \"\"\"\n    Initialize the StratifiedKFoldSplitter.\n\n    Args:\n        n_splits (int): Number of folds. Must be at least 2.\n        shuffle (bool): Whether to shuffle the data before splitting.\n        random_state (int): Controls the shuffling applied to the data before splitting.\n        pipeline (BaseFilter | None): The pipeline used for model training and evaluation.\n    \"\"\"\n    super().__init__(pipeline=pipeline)\n    self.n_splits = n_splits\n    self.shuffle = shuffle\n    self.random_state = random_state\n    self._skf = StratifiedKFold(\n        n_splits=n_splits, shuffle=shuffle, random_state=random_state\n    )\n    self.pipeline = pipeline\n</code></pre>"},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.evaluate","title":"<code>evaluate(x_data, y_true, y_pred)</code>","text":"<p>Evaluate the pipeline using the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>XYData</code> <p>Input features.</p> required <code>y_true</code> <code>XYData | None</code> <p>Ground truth labels.</p> required <code>y_pred</code> <code>XYData</code> <p>Predictions from the pipeline.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Evaluation metrics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pipeline is not fitted.</p> Source code in <code>framework3/plugins/splitter/stratified_cross_validation_splitter.py</code> <pre><code>def evaluate(\n    self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the pipeline using the provided data.\n\n    Args:\n        x_data (XYData): Input features.\n        y_true (XYData | None): Ground truth labels.\n        y_pred (XYData): Predictions from the pipeline.\n\n    Returns:\n        Dict[str, Any]: Evaluation metrics.\n\n    Raises:\n        ValueError: If the pipeline is not fitted.\n    \"\"\"\n    if self.pipeline is None:\n        raise ValueError(\"Pipeline must be fitted before evaluation\")\n    return self.pipeline.evaluate(x_data, y_true, y_pred)\n</code></pre>"},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.fit","title":"<code>fit(x, y)</code>","text":"<p>Perform Stratified K-Fold cross-validation on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>Input features.</p> required <code>y</code> <code>XYData | None</code> <p>Target labels.</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: Mean loss across all folds.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If y is None or the pipeline is not set.</p> Source code in <code>framework3/plugins/splitter/stratified_cross_validation_splitter.py</code> <pre><code>def fit(self, x: XYData, y: XYData | None) -&gt; Optional[float]:\n    \"\"\"\n    Perform Stratified K-Fold cross-validation on the given data.\n\n    Args:\n        x (XYData): Input features.\n        y (XYData | None): Target labels.\n\n    Returns:\n        Optional[float]: Mean loss across all folds.\n\n    Raises:\n        ValueError: If y is None or the pipeline is not set.\n    \"\"\"\n    self._print_acction(\"Fitting with StratifiedKFold Splitter...\")\n    if self._verbose:\n        rprint(self.pipeline)\n\n    if y is None:\n        raise ValueError(\"y must be provided for Stratified K-Fold split\")\n\n    if self.pipeline is None:\n        raise ValueError(\"Pipeline must be fitted before splitting\")\n\n    X = x.value\n    Y = y.value\n\n    losses = []\n    splits = self._skf.split(X, Y)\n    for train_idx, val_idx in tqdm(\n        splits, total=self._skf.get_n_splits(X, Y), disable=not self._verbose\n    ):\n        X_train = x.split(train_idx)\n        X_val = x.split(val_idx)\n        y_train = y.split(train_idx)\n        y_val = y.split(val_idx)\n\n        pipeline = cast(\n            BasePipeline,\n            BasePlugin.build_from_dump(self.pipeline.item_dump(), Container.pif),\n        )\n\n        pipeline.fit(X_train, y_train)\n\n        _y = pipeline.predict(X_val)\n\n        loss = pipeline.evaluate(X_val, y_val, _y)\n        losses.append(float(next(iter(loss.values()))))\n\n        self.clear_memory()\n\n    return float(np.mean(losses) if losses else 0.0)\n</code></pre>"},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the fitted pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>Input data for prediction.</p> required <p>Returns:</p> Name Type Description <code>XYData</code> <code>XYData</code> <p>Predictions from the trained pipeline.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pipeline is not fitted.</p> Source code in <code>framework3/plugins/splitter/stratified_cross_validation_splitter.py</code> <pre><code>def predict(self, x: XYData) -&gt; XYData:\n    \"\"\"\n    Make predictions using the fitted pipeline.\n\n    Args:\n        x (XYData): Input data for prediction.\n\n    Returns:\n        XYData: Predictions from the trained pipeline.\n\n    Raises:\n        ValueError: If pipeline is not fitted.\n    \"\"\"\n    self._print_acction(\"Predicting with StratifiedKFold Splitter...\")\n    if self._verbose:\n        rprint(self.pipeline)\n\n    if self.pipeline is None:\n        raise ValueError(\"Pipeline must be fitted before prediction\")\n\n    return self.pipeline.predict(x)\n</code></pre>"},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.split","title":"<code>split(pipeline)</code>","text":"<p>Set the pipeline for the splitter and disable its verbosity.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline used for training and evaluation.</p> required Source code in <code>framework3/plugins/splitter/stratified_cross_validation_splitter.py</code> <pre><code>def split(self, pipeline: BaseFilter):\n    \"\"\"\n    Set the pipeline for the splitter and disable its verbosity.\n\n    Args:\n        pipeline (BaseFilter): The pipeline used for training and evaluation.\n    \"\"\"\n    self.pipeline = pipeline\n    self.pipeline.verbose(False)\n</code></pre>"},{"location":"api/plugins/splitters/stratified_kfold_splitter/#framework3.plugins.splitter.stratified_cross_validation_splitter.StratifiedKFoldSplitter.start","title":"<code>start(x, y, X_)</code>","text":"<p>Start the cross-validation process and optionally make predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>XYData</code> <p>Input training features.</p> required <code>y</code> <code>Optional[XYData]</code> <p>Target labels.</p> required <code>X_</code> <code>Optional[XYData]</code> <p>Optional input data for prediction.</p> required <p>Returns:</p> Type Description <code>Optional[XYData]</code> <p>Optional[XYData]: Predictions if X_ is provided, else predictions on training data.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during execution.</p> Source code in <code>framework3/plugins/splitter/stratified_cross_validation_splitter.py</code> <pre><code>def start(\n    self, x: XYData, y: Optional[XYData], X_: Optional[XYData]\n) -&gt; Optional[XYData]:\n    \"\"\"\n    Start the cross-validation process and optionally make predictions.\n\n    Args:\n        x (XYData): Input training features.\n        y (Optional[XYData]): Target labels.\n        X_ (Optional[XYData]): Optional input data for prediction.\n\n    Returns:\n        Optional[XYData]: Predictions if X_ is provided, else predictions on training data.\n\n    Raises:\n        Exception: If any error occurs during execution.\n    \"\"\"\n    try:\n        self.fit(x, y)\n        if X_ is not None:\n            return self.predict(X_)\n        else:\n            return self.predict(x)\n    except Exception as e:\n        print(f\"Error during pipeline execution: {e}\")\n        raise e\n</code></pre>"},{"location":"api/plugins/storage/local/","title":"Local","text":""},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage","title":"<code>framework3.plugins.storage.local_storage.LocalStorage</code>","text":"<p>               Bases: <code>BaseStorage</code></p> <p>A local file system storage implementation for storing and retrieving files.</p> <p>This class provides methods to interact with the local file system, allowing storage operations such as uploading, downloading, and deleting files.</p> Key Features <ul> <li>Simple interface for file operations</li> <li>Support for creating nested directory structures</li> <li>File existence checking</li> <li>Listing stored files in a given context</li> </ul> Usage <pre><code>from framework3.plugins.storage import LocalStorage\n\n# Initialize local storage\nstorage = LocalStorage(storage_path='my_cache')\n\n# Upload a file\nstorage.upload_file(\"Hello, World!\", \"greeting.txt\", \"/tmp\")\n\n# Download and read a file\ncontent = storage.download_file(\"greeting.txt\", \"/tmp\")\nprint(content)  # Output: Hello, World!\n\n# Check if a file exists\nexists = storage.check_if_exists(\"greeting.txt\", \"/tmp\")\nprint(exists)  # Output: True\n\n# List files in a directory\nfiles = storage.list_stored_files(\"/tmp\")\nprint(files)  # Output: ['greeting.txt']\n\n# Delete a file\nstorage.delete_file(\"greeting.txt\", \"/tmp\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>storage_path</code> <code>str</code> <p>The base path for storage operations.</p> <code>_base_path</code> <code>str</code> <p>The full path to the storage directory.</p> <p>Methods:</p> Name Description <code>get_root_path</code> <p>Get the root path of the storage.</p> <code>upload_file</code> <p>str, context: str, direct_stream: bool = False) -&gt; str | None: Upload a file to the specified context.</p> <code>list_stored_files</code> <p>str) -&gt; List[str]: List all files in the specified context.</p> <code>get_file_by_hashcode</code> <p>str, context: str) -&gt; Any: Get a file by its hashcode.</p> <code>check_if_exists</code> <p>str, context: str) -&gt; bool: Check if a file exists in the specified context.</p> <code>download_file</code> <p>str, context: str) -&gt; Any: Download and load a file from the specified context.</p> <code>delete_file</code> <p>str, context: str) -&gt; None: Delete a file from the specified context.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>class LocalStorage(BaseStorage):\n    \"\"\"\n    A local file system storage implementation for storing and retrieving files.\n\n    This class provides methods to interact with the local file system, allowing\n    storage operations such as uploading, downloading, and deleting files.\n\n    Key Features:\n        - Simple interface for file operations\n        - Support for creating nested directory structures\n        - File existence checking\n        - Listing stored files in a given context\n\n    Usage:\n        ```python\n        from framework3.plugins.storage import LocalStorage\n\n        # Initialize local storage\n        storage = LocalStorage(storage_path='my_cache')\n\n        # Upload a file\n        storage.upload_file(\"Hello, World!\", \"greeting.txt\", \"/tmp\")\n\n        # Download and read a file\n        content = storage.download_file(\"greeting.txt\", \"/tmp\")\n        print(content)  # Output: Hello, World!\n\n        # Check if a file exists\n        exists = storage.check_if_exists(\"greeting.txt\", \"/tmp\")\n        print(exists)  # Output: True\n\n        # List files in a directory\n        files = storage.list_stored_files(\"/tmp\")\n        print(files)  # Output: ['greeting.txt']\n\n        # Delete a file\n        storage.delete_file(\"greeting.txt\", \"/tmp\")\n        ```\n\n    Attributes:\n        storage_path (str): The base path for storage operations.\n        _base_path (str): The full path to the storage directory.\n\n    Methods:\n        get_root_path() -&gt; str: Get the root path of the storage.\n        upload_file(file, file_name: str, context: str, direct_stream: bool = False) -&gt; str | None:\n            Upload a file to the specified context.\n        list_stored_files(context: str) -&gt; List[str]: List all files in the specified context.\n        get_file_by_hashcode(hashcode: str, context: str) -&gt; Any: Get a file by its hashcode.\n        check_if_exists(hashcode: str, context: str) -&gt; bool: Check if a file exists in the specified context.\n        download_file(hashcode: str, context: str) -&gt; Any: Download and load a file from the specified context.\n        delete_file(hashcode: str, context: str) -&gt; None: Delete a file from the specified context.\n    \"\"\"\n\n    def __init__(self, storage_path: str = \"cache\"):\n        \"\"\"\n        Initialize the LocalStorage.\n\n        Args:\n            storage_path (str, optional): The base path for storage. Defaults to 'cache'.\n        \"\"\"\n        super().__init__()\n        self.storage_path = storage_path\n        self._base_path = storage_path\n\n    def get_root_path(self) -&gt; str:\n        \"\"\"\n        Get the root path of the storage.\n\n        Returns:\n            str: The full path to the storage directory.\n        \"\"\"\n        return self._base_path\n\n    def upload_file(\n        self, file, file_name: str, context: str, direct_stream: bool = False\n    ) -&gt; str | None:\n        \"\"\"\n        Upload a file to the specified context.\n\n        Args:\n            file (Any): The file content to be uploaded.\n            file_name (str): The name of the file.\n            context (str): The directory path where the file will be saved.\n            direct_stream (bool, optional): Not used in this implementation. Defaults to False.\n\n        Returns:\n            str | None: The file name if successful, None otherwise.\n        \"\"\"\n        try:\n            Path(context).mkdir(parents=True, exist_ok=True)\n            if self._verbose:\n                print(f\"\\t * Saving in local path: {context}/{file_name}\")\n            pickle.dump(file, open(f\"{context}/{file_name}\", \"wb\"))\n            if self._verbose:\n                print(\"\\t * Saved !\")\n            return file_name\n        except Exception as ex:\n            print(ex)\n        return None\n\n    def list_stored_files(self, context: str) -&gt; List[str]:\n        \"\"\"\n        List all files in the specified context.\n\n        Args:\n            context (str): The directory path to list files from.\n\n        Returns:\n            List[str]: A list of file names in the specified context.\n        \"\"\"\n        return os.listdir(context)\n\n    def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; Any:\n        \"\"\"\n        Get a file by its hashcode (filename in this implementation).\n\n        Args:\n            hashcode (str): The hashcode (filename) of the file.\n            context (str): The directory path where the file is located.\n\n        Returns:\n            Any: A file object if found.\n\n        Raises:\n            FileNotFoundError: If the file is not found in the specified context.\n        \"\"\"\n        if hashcode in os.listdir(context):\n            return open(f\"{context}/{hashcode}\", \"rb\")\n        else:\n            raise FileNotFoundError(f\"Couldn't find file {hashcode} in path {context}\")\n\n    def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the specified context.\n\n        Args:\n            hashcode (str): The hashcode (filename) of the file.\n            context (str): The directory path where to check for the file.\n\n        Returns:\n            bool: True if the file exists, False otherwise.\n        \"\"\"\n        try:\n            for file_n in os.listdir(context):\n                if file_n == hashcode:\n                    return True\n            return False\n        except FileNotFoundError:\n            return False\n\n    def download_file(self, hashcode: str, context: str) -&gt; Any:\n        \"\"\"\n        Download and load a file from the specified context.\n\n        Args:\n            hashcode (str): The hashcode (filename) of the file to download.\n            context (str): The directory path where the file is located.\n\n        Returns:\n            Any: The content of the file, unpickled if it was pickled.\n        \"\"\"\n        stream = self.get_file_by_hashcode(hashcode, context)\n        if self._verbose:\n            print(f\"\\t * Downloading: {stream}\")\n        loaded = pickle.load(stream)\n        return pickle.loads(loaded) if isinstance(loaded, bytes) else loaded\n\n    def delete_file(self, hashcode: str, context: str):\n        \"\"\"\n        Delete a file from the specified context.\n\n        Args:\n            hashcode (str): The hashcode (filename) of the file to delete.\n            context (str): The directory path where the file is located.\n\n        Raises:\n            FileExistsError: If the file does not exist in the specified context.\n        \"\"\"\n        if os.path.exists(f\"{context}/{hashcode}\"):\n            os.remove(f\"{context}/{hashcode}\")\n        else:\n            raise FileExistsError(\"No existe en la carpeta\")\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.storage_path","title":"<code>storage_path = storage_path</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.__init__","title":"<code>__init__(storage_path='cache')</code>","text":"<p>Initialize the LocalStorage.</p> <p>Parameters:</p> Name Type Description Default <code>storage_path</code> <code>str</code> <p>The base path for storage. Defaults to 'cache'.</p> <code>'cache'</code> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def __init__(self, storage_path: str = \"cache\"):\n    \"\"\"\n    Initialize the LocalStorage.\n\n    Args:\n        storage_path (str, optional): The base path for storage. Defaults to 'cache'.\n    \"\"\"\n    super().__init__()\n    self.storage_path = storage_path\n    self._base_path = storage_path\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.check_if_exists","title":"<code>check_if_exists(hashcode, context)</code>","text":"<p>Check if a file exists in the specified context.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The hashcode (filename) of the file.</p> required <code>context</code> <code>str</code> <p>The directory path where to check for the file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file exists, False otherwise.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the specified context.\n\n    Args:\n        hashcode (str): The hashcode (filename) of the file.\n        context (str): The directory path where to check for the file.\n\n    Returns:\n        bool: True if the file exists, False otherwise.\n    \"\"\"\n    try:\n        for file_n in os.listdir(context):\n            if file_n == hashcode:\n                return True\n        return False\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.delete_file","title":"<code>delete_file(hashcode, context)</code>","text":"<p>Delete a file from the specified context.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The hashcode (filename) of the file to delete.</p> required <code>context</code> <code>str</code> <p>The directory path where the file is located.</p> required <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file does not exist in the specified context.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def delete_file(self, hashcode: str, context: str):\n    \"\"\"\n    Delete a file from the specified context.\n\n    Args:\n        hashcode (str): The hashcode (filename) of the file to delete.\n        context (str): The directory path where the file is located.\n\n    Raises:\n        FileExistsError: If the file does not exist in the specified context.\n    \"\"\"\n    if os.path.exists(f\"{context}/{hashcode}\"):\n        os.remove(f\"{context}/{hashcode}\")\n    else:\n        raise FileExistsError(\"No existe en la carpeta\")\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.download_file","title":"<code>download_file(hashcode, context)</code>","text":"<p>Download and load a file from the specified context.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The hashcode (filename) of the file to download.</p> required <code>context</code> <code>str</code> <p>The directory path where the file is located.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The content of the file, unpickled if it was pickled.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def download_file(self, hashcode: str, context: str) -&gt; Any:\n    \"\"\"\n    Download and load a file from the specified context.\n\n    Args:\n        hashcode (str): The hashcode (filename) of the file to download.\n        context (str): The directory path where the file is located.\n\n    Returns:\n        Any: The content of the file, unpickled if it was pickled.\n    \"\"\"\n    stream = self.get_file_by_hashcode(hashcode, context)\n    if self._verbose:\n        print(f\"\\t * Downloading: {stream}\")\n    loaded = pickle.load(stream)\n    return pickle.loads(loaded) if isinstance(loaded, bytes) else loaded\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.get_file_by_hashcode","title":"<code>get_file_by_hashcode(hashcode, context)</code>","text":"<p>Get a file by its hashcode (filename in this implementation).</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The hashcode (filename) of the file.</p> required <code>context</code> <code>str</code> <p>The directory path where the file is located.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A file object if found.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found in the specified context.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; Any:\n    \"\"\"\n    Get a file by its hashcode (filename in this implementation).\n\n    Args:\n        hashcode (str): The hashcode (filename) of the file.\n        context (str): The directory path where the file is located.\n\n    Returns:\n        Any: A file object if found.\n\n    Raises:\n        FileNotFoundError: If the file is not found in the specified context.\n    \"\"\"\n    if hashcode in os.listdir(context):\n        return open(f\"{context}/{hashcode}\", \"rb\")\n    else:\n        raise FileNotFoundError(f\"Couldn't find file {hashcode} in path {context}\")\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.get_root_path","title":"<code>get_root_path()</code>","text":"<p>Get the root path of the storage.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full path to the storage directory.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def get_root_path(self) -&gt; str:\n    \"\"\"\n    Get the root path of the storage.\n\n    Returns:\n        str: The full path to the storage directory.\n    \"\"\"\n    return self._base_path\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.list_stored_files","title":"<code>list_stored_files(context)</code>","text":"<p>List all files in the specified context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>The directory path to list files from.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file names in the specified context.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def list_stored_files(self, context: str) -&gt; List[str]:\n    \"\"\"\n    List all files in the specified context.\n\n    Args:\n        context (str): The directory path to list files from.\n\n    Returns:\n        List[str]: A list of file names in the specified context.\n    \"\"\"\n    return os.listdir(context)\n</code></pre>"},{"location":"api/plugins/storage/local/#framework3.plugins.storage.local_storage.LocalStorage.upload_file","title":"<code>upload_file(file, file_name, context, direct_stream=False)</code>","text":"<p>Upload a file to the specified context.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Any</code> <p>The file content to be uploaded.</p> required <code>file_name</code> <code>str</code> <p>The name of the file.</p> required <code>context</code> <code>str</code> <p>The directory path where the file will be saved.</p> required <code>direct_stream</code> <code>bool</code> <p>Not used in this implementation. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The file name if successful, None otherwise.</p> Source code in <code>framework3/plugins/storage/local_storage.py</code> <pre><code>def upload_file(\n    self, file, file_name: str, context: str, direct_stream: bool = False\n) -&gt; str | None:\n    \"\"\"\n    Upload a file to the specified context.\n\n    Args:\n        file (Any): The file content to be uploaded.\n        file_name (str): The name of the file.\n        context (str): The directory path where the file will be saved.\n        direct_stream (bool, optional): Not used in this implementation. Defaults to False.\n\n    Returns:\n        str | None: The file name if successful, None otherwise.\n    \"\"\"\n    try:\n        Path(context).mkdir(parents=True, exist_ok=True)\n        if self._verbose:\n            print(f\"\\t * Saving in local path: {context}/{file_name}\")\n        pickle.dump(file, open(f\"{context}/{file_name}\", \"wb\"))\n        if self._verbose:\n            print(\"\\t * Saved !\")\n        return file_name\n    except Exception as ex:\n        print(ex)\n    return None\n</code></pre>"},{"location":"api/plugins/storage/s3/","title":"S3","text":""},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage","title":"<code>framework3.plugins.storage.s3_storage.S3Storage</code>","text":"<p>               Bases: <code>BaseStorage</code></p> <p>A storage implementation for Amazon S3 to store and retrieve files.</p> <p>This class provides methods to interact with Amazon S3, allowing storage operations such as uploading, downloading, and deleting files in an S3 bucket.</p> Key Features <ul> <li>Simple interface for S3 file operations</li> <li>Support for file existence checking</li> <li>Listing stored files in the bucket</li> <li>Direct streaming support for large files</li> </ul> Usage <pre><code>from framework3.plugins.storage import S3Storage\n\n# Initialize S3 storage\nstorage = S3Storage(bucket='my-bucket', region_name='us-west-2',\n                    access_key_id='YOUR_ACCESS_KEY', access_key='YOUR_SECRET_KEY')\n\n# Upload a file\nstorage.upload_file(\"Hello, World!\", \"greeting.txt\", \"my-folder\")\n\n# Download and read a file\ncontent = storage.download_file(\"greeting.txt\", \"my-folder\")\nprint(content)  # Output: Hello, World!\n\n# Check if a file exists\nexists = storage.check_if_exists(\"greeting.txt\", \"my-folder\")\nprint(exists)  # Output: True\n\n# List files in the bucket\nfiles = storage.list_stored_files(\"\")\nprint(files)  # Output: ['my-folder/greeting.txt']\n\n# Delete a file\nstorage.delete_file(\"greeting.txt\", \"my-folder\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>_client</code> <code>client</code> <p>The boto3 S3 client.</p> <code>bucket</code> <code>str</code> <p>The name of the S3 bucket.</p> <p>Methods:</p> Name Description <code>get_root_path</code> <p>Get the root path (bucket name) of the storage.</p> <code>upload_file</code> <p>object, file_name: str, context: str, direct_stream: bool = False) -&gt; str: Upload a file to the specified context in S3.</p> <code>list_stored_files</code> <p>str) -&gt; List[str]: List all files in the S3 bucket.</p> <code>get_file_by_hashcode</code> <p>str, context: str) -&gt; bytes: Get a file by its hashcode (key in S3).</p> <code>check_if_exists</code> <p>str, context: str) -&gt; bool: Check if a file exists in S3.</p> <code>download_file</code> <p>str, context: str) -&gt; Any: Download a file from S3.</p> <code>delete_file</code> <p>str, context: str) -&gt; None: Delete a file from S3.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>@Container.bind()\nclass S3Storage(BaseStorage):\n    \"\"\"\n    A storage implementation for Amazon S3 to store and retrieve files.\n\n    This class provides methods to interact with Amazon S3, allowing storage operations\n    such as uploading, downloading, and deleting files in an S3 bucket.\n\n    Key Features:\n        - Simple interface for S3 file operations\n        - Support for file existence checking\n        - Listing stored files in the bucket\n        - Direct streaming support for large files\n\n    Usage:\n        ```python\n        from framework3.plugins.storage import S3Storage\n\n        # Initialize S3 storage\n        storage = S3Storage(bucket='my-bucket', region_name='us-west-2',\n                            access_key_id='YOUR_ACCESS_KEY', access_key='YOUR_SECRET_KEY')\n\n        # Upload a file\n        storage.upload_file(\"Hello, World!\", \"greeting.txt\", \"my-folder\")\n\n        # Download and read a file\n        content = storage.download_file(\"greeting.txt\", \"my-folder\")\n        print(content)  # Output: Hello, World!\n\n        # Check if a file exists\n        exists = storage.check_if_exists(\"greeting.txt\", \"my-folder\")\n        print(exists)  # Output: True\n\n        # List files in the bucket\n        files = storage.list_stored_files(\"\")\n        print(files)  # Output: ['my-folder/greeting.txt']\n\n        # Delete a file\n        storage.delete_file(\"greeting.txt\", \"my-folder\")\n        ```\n\n    Attributes:\n        _client (boto3.client): The boto3 S3 client.\n        bucket (str): The name of the S3 bucket.\n\n    Methods:\n        get_root_path() -&gt; str: Get the root path (bucket name) of the storage.\n        upload_file(file: object, file_name: str, context: str, direct_stream: bool = False) -&gt; str:\n            Upload a file to the specified context in S3.\n        list_stored_files(context: str) -&gt; List[str]: List all files in the S3 bucket.\n        get_file_by_hashcode(hashcode: str, context: str) -&gt; bytes: Get a file by its hashcode (key in S3).\n        check_if_exists(hashcode: str, context: str) -&gt; bool: Check if a file exists in S3.\n        download_file(hashcode: str, context: str) -&gt; Any: Download a file from S3.\n        delete_file(hashcode: str, context: str) -&gt; None: Delete a file from S3.\n    \"\"\"\n\n    def __init__(\n        self,\n        bucket: str,\n        region_name: str,\n        access_key_id: str,\n        access_key: str,\n        endpoint_url: str | None = None,\n    ):\n        \"\"\"\n        Initialize the S3Storage.\n\n        Args:\n            bucket (str): The name of the S3 bucket.\n            region_name (str): The AWS region name.\n            access_key_id (str): The AWS access key ID.\n            access_key (str): The AWS secret access key.\n            endpoint_url (str | None, optional): The endpoint URL for the S3 service. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self._client = boto3.client(\n            service_name=\"s3\",\n            region_name=region_name,\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=access_key,\n            endpoint_url=endpoint_url,\n            use_ssl=True,\n        )\n        self.bucket = bucket\n\n    def get_root_path(self) -&gt; str:\n        \"\"\"\n        Get the root path (bucket name) of the storage.\n\n        Returns:\n            str: The name of the S3 bucket.\n        \"\"\"\n        return self.bucket\n\n    def upload_file(\n        self, file: object, file_name: str, context: str, direct_stream: bool = False\n    ) -&gt; str:\n        \"\"\"\n        Upload a file to the specified context in S3.\n\n        Args:\n            file (object): The file content to be uploaded.\n            file_name (str): The name of the file.\n            context (str): The directory path where the file will be saved.\n            direct_stream (bool, optional): If True, assumes file is already a BytesIO object. Defaults to False.\n\n        Returns:\n            str: The file name if successful.\n        \"\"\"\n        if type(file) is not io.BytesIO:\n            binary = pickle.dumps(file)\n            stream = io.BytesIO(binary)\n        else:\n            stream = file\n        if self._verbose:\n            print(\"- Binary prepared!\")\n            print(\"- Stream ready!\")\n            print(f\" \\t * Object size {sys.getsizeof(stream) * 1e-9} GBs \")\n\n        self._client.put_object(\n            Body=stream, Bucket=self.bucket, Key=f\"{context}/{file_name}\"\n        )\n\n        if self._verbose:\n            print(\"Upload Complete!\")\n\n        return file_name\n\n    def list_stored_files(self, context: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a specific folder (context) in the S3 bucket.\n\n        Args:\n            context (str): The folder path within the bucket to list files from.\n\n        Returns:\n            List[str]: A list of object keys in the specified folder.\n        \"\"\"\n        # Ensure the context ends with a trailing slash if it's not empty\n        prefix = f\"{context}/\" if context and not context.endswith(\"/\") else context\n\n        paginator = self._client.get_paginator(\"list_objects_v2\")\n        pages = paginator.paginate(Bucket=self.bucket, Prefix=prefix)\n\n        file_list = []\n        for page in pages:\n            if \"Contents\" in page:\n                for obj in page[\"Contents\"]:\n                    # Remove the prefix from the key to get the relative path\n                    relative_path = obj[\"Key\"][len(prefix) :]\n                    if relative_path:  # Ignore the folder itself\n                        file_list.append(relative_path)\n\n        return file_list\n\n    def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; bytes:\n        \"\"\"\n        Get a file by its hashcode (key in S3).\n\n        Args:\n            hashcode (str): The hashcode (key) of the file.\n            context (str): Not used in this implementation.\n\n        Returns:\n            bytes: The content of the file.\n        \"\"\"\n        obj = self._client.get_object(Bucket=self.bucket, Key=hashcode)\n        return obj[\"Body\"].read()\n\n    def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in S3.\n\n        Args:\n            hashcode (str): The name of the file.\n            context (str): The directory path where the file is located.\n\n        Returns:\n            bool: True if the file exists, False otherwise.\n        \"\"\"\n        try:\n            self._client.head_object(Bucket=self.bucket, Key=f\"{context}/{hashcode}\")\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                return False\n            else:\n                print(f\"An error ocurred &gt; {e}\")\n                return False\n        return True\n\n    def download_file(self, hashcode: str, context: str) -&gt; Any:\n        \"\"\"\n        Download a file from S3.\n\n        Args:\n            hashcode (str): The name of the file.\n            context (str): The directory path where the file is located.\n\n        Returns:\n            Any: The deserialized content of the file.\n        \"\"\"\n        obj = self._client.get_object(Bucket=self.bucket, Key=f\"{context}/{hashcode}\")\n        return pickle.loads(obj[\"Body\"].read())\n\n    def delete_file(self, hashcode: str, context: str) -&gt; None:\n        \"\"\"\n        Delete a file from S3.\n\n        Args:\n            hashcode (str): The name of the file.\n            context (str): The directory path where the file is located.\n\n        Raises:\n            Exception: If the file couldn't be deleted.\n            FileExistsError: If the file doesn't exist in the bucket.\n        \"\"\"\n        if self.check_if_exists(hashcode, context):\n            self._client.delete_object(Bucket=self.bucket, Key=f\"{context}/{hashcode}\")\n            if self.check_if_exists(hashcode, context):\n                raise Exception(\"Couldn't delete file\")\n            else:\n                print(\"Deleted!\")\n        else:\n            raise FileExistsError(\"No existe en el bucket\")\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.bucket","title":"<code>bucket = bucket</code>  <code>instance-attribute</code>","text":""},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.__init__","title":"<code>__init__(bucket, region_name, access_key_id, access_key, endpoint_url=None)</code>","text":"<p>Initialize the S3Storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>region_name</code> <code>str</code> <p>The AWS region name.</p> required <code>access_key_id</code> <code>str</code> <p>The AWS access key ID.</p> required <code>access_key</code> <code>str</code> <p>The AWS secret access key.</p> required <code>endpoint_url</code> <code>str | None</code> <p>The endpoint URL for the S3 service. Defaults to None.</p> <code>None</code> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def __init__(\n    self,\n    bucket: str,\n    region_name: str,\n    access_key_id: str,\n    access_key: str,\n    endpoint_url: str | None = None,\n):\n    \"\"\"\n    Initialize the S3Storage.\n\n    Args:\n        bucket (str): The name of the S3 bucket.\n        region_name (str): The AWS region name.\n        access_key_id (str): The AWS access key ID.\n        access_key (str): The AWS secret access key.\n        endpoint_url (str | None, optional): The endpoint URL for the S3 service. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self._client = boto3.client(\n        service_name=\"s3\",\n        region_name=region_name,\n        aws_access_key_id=access_key_id,\n        aws_secret_access_key=access_key,\n        endpoint_url=endpoint_url,\n        use_ssl=True,\n    )\n    self.bucket = bucket\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.check_if_exists","title":"<code>check_if_exists(hashcode, context)</code>","text":"<p>Check if a file exists in S3.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The name of the file.</p> required <code>context</code> <code>str</code> <p>The directory path where the file is located.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file exists, False otherwise.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def check_if_exists(self, hashcode: str, context: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in S3.\n\n    Args:\n        hashcode (str): The name of the file.\n        context (str): The directory path where the file is located.\n\n    Returns:\n        bool: True if the file exists, False otherwise.\n    \"\"\"\n    try:\n        self._client.head_object(Bucket=self.bucket, Key=f\"{context}/{hashcode}\")\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            return False\n        else:\n            print(f\"An error ocurred &gt; {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.delete_file","title":"<code>delete_file(hashcode, context)</code>","text":"<p>Delete a file from S3.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The name of the file.</p> required <code>context</code> <code>str</code> <p>The directory path where the file is located.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the file couldn't be deleted.</p> <code>FileExistsError</code> <p>If the file doesn't exist in the bucket.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def delete_file(self, hashcode: str, context: str) -&gt; None:\n    \"\"\"\n    Delete a file from S3.\n\n    Args:\n        hashcode (str): The name of the file.\n        context (str): The directory path where the file is located.\n\n    Raises:\n        Exception: If the file couldn't be deleted.\n        FileExistsError: If the file doesn't exist in the bucket.\n    \"\"\"\n    if self.check_if_exists(hashcode, context):\n        self._client.delete_object(Bucket=self.bucket, Key=f\"{context}/{hashcode}\")\n        if self.check_if_exists(hashcode, context):\n            raise Exception(\"Couldn't delete file\")\n        else:\n            print(\"Deleted!\")\n    else:\n        raise FileExistsError(\"No existe en el bucket\")\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.download_file","title":"<code>download_file(hashcode, context)</code>","text":"<p>Download a file from S3.</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The name of the file.</p> required <code>context</code> <code>str</code> <p>The directory path where the file is located.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized content of the file.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def download_file(self, hashcode: str, context: str) -&gt; Any:\n    \"\"\"\n    Download a file from S3.\n\n    Args:\n        hashcode (str): The name of the file.\n        context (str): The directory path where the file is located.\n\n    Returns:\n        Any: The deserialized content of the file.\n    \"\"\"\n    obj = self._client.get_object(Bucket=self.bucket, Key=f\"{context}/{hashcode}\")\n    return pickle.loads(obj[\"Body\"].read())\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.get_file_by_hashcode","title":"<code>get_file_by_hashcode(hashcode, context)</code>","text":"<p>Get a file by its hashcode (key in S3).</p> <p>Parameters:</p> Name Type Description Default <code>hashcode</code> <code>str</code> <p>The hashcode (key) of the file.</p> required <code>context</code> <code>str</code> <p>Not used in this implementation.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The content of the file.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def get_file_by_hashcode(self, hashcode: str, context: str) -&gt; bytes:\n    \"\"\"\n    Get a file by its hashcode (key in S3).\n\n    Args:\n        hashcode (str): The hashcode (key) of the file.\n        context (str): Not used in this implementation.\n\n    Returns:\n        bytes: The content of the file.\n    \"\"\"\n    obj = self._client.get_object(Bucket=self.bucket, Key=hashcode)\n    return obj[\"Body\"].read()\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.get_root_path","title":"<code>get_root_path()</code>","text":"<p>Get the root path (bucket name) of the storage.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the S3 bucket.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def get_root_path(self) -&gt; str:\n    \"\"\"\n    Get the root path (bucket name) of the storage.\n\n    Returns:\n        str: The name of the S3 bucket.\n    \"\"\"\n    return self.bucket\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.list_stored_files","title":"<code>list_stored_files(context)</code>","text":"<p>List all files in a specific folder (context) in the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>The folder path within the bucket to list files from.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of object keys in the specified folder.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def list_stored_files(self, context: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a specific folder (context) in the S3 bucket.\n\n    Args:\n        context (str): The folder path within the bucket to list files from.\n\n    Returns:\n        List[str]: A list of object keys in the specified folder.\n    \"\"\"\n    # Ensure the context ends with a trailing slash if it's not empty\n    prefix = f\"{context}/\" if context and not context.endswith(\"/\") else context\n\n    paginator = self._client.get_paginator(\"list_objects_v2\")\n    pages = paginator.paginate(Bucket=self.bucket, Prefix=prefix)\n\n    file_list = []\n    for page in pages:\n        if \"Contents\" in page:\n            for obj in page[\"Contents\"]:\n                # Remove the prefix from the key to get the relative path\n                relative_path = obj[\"Key\"][len(prefix) :]\n                if relative_path:  # Ignore the folder itself\n                    file_list.append(relative_path)\n\n    return file_list\n</code></pre>"},{"location":"api/plugins/storage/s3/#framework3.plugins.storage.s3_storage.S3Storage.upload_file","title":"<code>upload_file(file, file_name, context, direct_stream=False)</code>","text":"<p>Upload a file to the specified context in S3.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>object</code> <p>The file content to be uploaded.</p> required <code>file_name</code> <code>str</code> <p>The name of the file.</p> required <code>context</code> <code>str</code> <p>The directory path where the file will be saved.</p> required <code>direct_stream</code> <code>bool</code> <p>If True, assumes file is already a BytesIO object. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file name if successful.</p> Source code in <code>framework3/plugins/storage/s3_storage.py</code> <pre><code>def upload_file(\n    self, file: object, file_name: str, context: str, direct_stream: bool = False\n) -&gt; str:\n    \"\"\"\n    Upload a file to the specified context in S3.\n\n    Args:\n        file (object): The file content to be uploaded.\n        file_name (str): The name of the file.\n        context (str): The directory path where the file will be saved.\n        direct_stream (bool, optional): If True, assumes file is already a BytesIO object. Defaults to False.\n\n    Returns:\n        str: The file name if successful.\n    \"\"\"\n    if type(file) is not io.BytesIO:\n        binary = pickle.dumps(file)\n        stream = io.BytesIO(binary)\n    else:\n        stream = file\n    if self._verbose:\n        print(\"- Binary prepared!\")\n        print(\"- Stream ready!\")\n        print(f\" \\t * Object size {sys.getsizeof(stream) * 1e-9} GBs \")\n\n    self._client.put_object(\n        Body=stream, Bucket=self.bucket, Key=f\"{context}/{file_name}\"\n    )\n\n    if self._verbose:\n        print(\"Upload Complete!\")\n\n    return file_name\n</code></pre>"},{"location":"api/utils/pyspark/","title":"PySpark","text":""},{"location":"api/utils/pyspark/#framework3.utils.pyspark","title":"<code>framework3.utils.pyspark</code>","text":""},{"location":"api/utils/pyspark/#framework3.utils.pyspark.PySparkMapReduce","title":"<code>PySparkMapReduce</code>","text":"<p>               Bases: <code>MapReduceStrategy</code></p> <p>A MapReduce strategy implementation using PySpark for distributed computing.</p> <p>This class provides methods to perform map and reduce operations on large datasets using Apache Spark's distributed computing capabilities.</p> Key Features <ul> <li>Initializes a Spark session with configurable parameters</li> <li>Supports map, flatMap, and reduce operations</li> <li>Allows for parallel processing of data across multiple workers</li> <li>Provides a method to stop the Spark context when processing is complete</li> </ul> Usage <pre><code>from framework3.utils.pyspark import PySparkMapReduce\n\n# Initialize the PySparkMapReduce\nspark_mr = PySparkMapReduce(app_name=\"MySparkApp\", master=\"local[*]\", num_workers=4)\n\n# Perform map operation\ndata = [1, 2, 3, 4, 5]\nmapped_data = spark_mr.map(data, lambda x: x * 2)\n\n# Perform reduce operation\nresult = spark_mr.reduce(lambda x, y: x + y)\nprint(result)  # Output: 30\n\n# Stop the Spark context\nspark_mr.stop()\n</code></pre> <p>Attributes:</p> Name Type Description <code>sc</code> <code>SparkContext</code> <p>The Spark context used for distributed computing.</p> <p>Methods:</p> Name Description <code>map</code> <p>Any, map_function: Callable[..., Any], numSlices: int | None = None) -&gt; Any: Applies a map function to the input data in parallel.</p> <code>flatMap</code> <p>Any, map_function: Callable[..., Any], numSlices: int | None = None) -&gt; Any: Applies a flatMap function to the input data in parallel.</p> <code>reduce</code> <p>Callable[..., Any]) -&gt; Any: Applies a reduce function to the mapped data.</p> <code>stop</code> <p>Stops the Spark context.</p> Source code in <code>framework3/utils/pyspark.py</code> <pre><code>class PySparkMapReduce(MapReduceStrategy):\n    \"\"\"\n    A MapReduce strategy implementation using PySpark for distributed computing.\n\n    This class provides methods to perform map and reduce operations on large datasets\n    using Apache Spark's distributed computing capabilities.\n\n    Key Features:\n        - Initializes a Spark session with configurable parameters\n        - Supports map, flatMap, and reduce operations\n        - Allows for parallel processing of data across multiple workers\n        - Provides a method to stop the Spark context when processing is complete\n\n    Usage:\n        ```python\n        from framework3.utils.pyspark import PySparkMapReduce\n\n        # Initialize the PySparkMapReduce\n        spark_mr = PySparkMapReduce(app_name=\"MySparkApp\", master=\"local[*]\", num_workers=4)\n\n        # Perform map operation\n        data = [1, 2, 3, 4, 5]\n        mapped_data = spark_mr.map(data, lambda x: x * 2)\n\n        # Perform reduce operation\n        result = spark_mr.reduce(lambda x, y: x + y)\n        print(result)  # Output: 30\n\n        # Stop the Spark context\n        spark_mr.stop()\n        ```\n\n    Attributes:\n        sc (pyspark.SparkContext): The Spark context used for distributed computing.\n\n    Methods:\n        map(data: Any, map_function: Callable[..., Any], numSlices: int | None = None) -&gt; Any:\n            Applies a map function to the input data in parallel.\n        flatMap(data: Any, map_function: Callable[..., Any], numSlices: int | None = None) -&gt; Any:\n            Applies a flatMap function to the input data in parallel.\n        reduce(reduce_function: Callable[..., Any]) -&gt; Any:\n            Applies a reduce function to the mapped data.\n        stop() -&gt; None:\n            Stops the Spark context.\n    \"\"\"\n\n    def __init__(self, app_name: str, master: str = \"local\", num_workers: int = 4):\n        \"\"\"\n        Initialize the PySparkMapReduce with a Spark session.\n\n        Args:\n            app_name (str): The name of the Spark application.\n            master (str, optional): The Spark master URL. Defaults to \"local\".\n            num_workers (int, optional): The number of worker instances. Defaults to 4.\n        \"\"\"\n        builder: SparkSession.Builder = cast(SparkSession.Builder, SparkSession.builder)\n        spark: SparkSession = (\n            builder.appName(app_name)\n            .config(\"spark.master\", master)\n            .config(\"spark.executor.instances\", str(num_workers))\n            .config(\"spark.cores.max\", str(num_workers * 2))\n            .getOrCreate()\n        )\n\n        self.sc = spark.sparkContext\n\n    def map(\n        self, data: Any, map_function: Callable[..., Any], numSlices: int | None = None\n    ) -&gt; Any:\n        \"\"\"\n        Apply a map function to the input data in parallel.\n\n        Args:\n            data (Any): The input data to be processed.\n            map_function (Callable[..., Any]): The function to apply to each element of the data.\n            numSlices (int | None, optional): The number of partitions to create. Defaults to None.\n\n        Returns:\n            Any: The result of the map operation as a PySpark RDD.\n        \"\"\"\n        self.rdd = self.sc.parallelize(data, numSlices=numSlices)\n        self.mapped_rdd = self.rdd.map(map_function)\n\n        # Aplicar transformaciones map\n        return self.mapped_rdd\n\n    def flatMap(\n        self, data: Any, map_function: Callable[..., Any], numSlices: int | None = None\n    ) -&gt; Any:\n        \"\"\"\n        Apply a flatMap function to the input data in parallel.\n\n        Args:\n            data (Any): The input data to be processed.\n            map_function (Callable[..., Any]): The function to apply to each element of the data.\n            numSlices (int | None, optional): The number of partitions to create. Defaults to None.\n\n        Returns:\n            Any: The result of the flatMap operation as a PySpark RDD.\n        \"\"\"\n        self.rdd = self.sc.parallelize(data, numSlices=numSlices)\n        self.mapped_rdd = self.rdd.flatMap(map_function)\n\n        # Aplicar transformaciones map\n        return self.mapped_rdd\n\n    def reduce(self, reduce_function: Callable[..., Any]) -&gt; Any:\n        \"\"\"\n        Apply a reduce function to the mapped data.\n\n        Args:\n            reduce_function (Callable[..., Any]): The function to reduce the mapped data.\n\n        Returns:\n            Any: The result of the reduce operation.\n        \"\"\"\n        result = self.mapped_rdd.reduce(reduce_function)\n        return result\n\n    def stop(self) -&gt; None:\n        \"\"\"\n        Stop the Spark context.\n\n        This method should be called when you're done with Spark operations to release resources.\n        \"\"\"\n        self.sc.stop()\n</code></pre>"},{"location":"api/utils/pyspark/#framework3.utils.pyspark.PySparkMapReduce.sc","title":"<code>sc = spark.sparkContext</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/pyspark/#framework3.utils.pyspark.PySparkMapReduce.__init__","title":"<code>__init__(app_name, master='local', num_workers=4)</code>","text":"<p>Initialize the PySparkMapReduce with a Spark session.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>The name of the Spark application.</p> required <code>master</code> <code>str</code> <p>The Spark master URL. Defaults to \"local\".</p> <code>'local'</code> <code>num_workers</code> <code>int</code> <p>The number of worker instances. Defaults to 4.</p> <code>4</code> Source code in <code>framework3/utils/pyspark.py</code> <pre><code>def __init__(self, app_name: str, master: str = \"local\", num_workers: int = 4):\n    \"\"\"\n    Initialize the PySparkMapReduce with a Spark session.\n\n    Args:\n        app_name (str): The name of the Spark application.\n        master (str, optional): The Spark master URL. Defaults to \"local\".\n        num_workers (int, optional): The number of worker instances. Defaults to 4.\n    \"\"\"\n    builder: SparkSession.Builder = cast(SparkSession.Builder, SparkSession.builder)\n    spark: SparkSession = (\n        builder.appName(app_name)\n        .config(\"spark.master\", master)\n        .config(\"spark.executor.instances\", str(num_workers))\n        .config(\"spark.cores.max\", str(num_workers * 2))\n        .getOrCreate()\n    )\n\n    self.sc = spark.sparkContext\n</code></pre>"},{"location":"api/utils/pyspark/#framework3.utils.pyspark.PySparkMapReduce.flatMap","title":"<code>flatMap(data, map_function, numSlices=None)</code>","text":"<p>Apply a flatMap function to the input data in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data to be processed.</p> required <code>map_function</code> <code>Callable[..., Any]</code> <p>The function to apply to each element of the data.</p> required <code>numSlices</code> <code>int | None</code> <p>The number of partitions to create. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the flatMap operation as a PySpark RDD.</p> Source code in <code>framework3/utils/pyspark.py</code> <pre><code>def flatMap(\n    self, data: Any, map_function: Callable[..., Any], numSlices: int | None = None\n) -&gt; Any:\n    \"\"\"\n    Apply a flatMap function to the input data in parallel.\n\n    Args:\n        data (Any): The input data to be processed.\n        map_function (Callable[..., Any]): The function to apply to each element of the data.\n        numSlices (int | None, optional): The number of partitions to create. Defaults to None.\n\n    Returns:\n        Any: The result of the flatMap operation as a PySpark RDD.\n    \"\"\"\n    self.rdd = self.sc.parallelize(data, numSlices=numSlices)\n    self.mapped_rdd = self.rdd.flatMap(map_function)\n\n    # Aplicar transformaciones map\n    return self.mapped_rdd\n</code></pre>"},{"location":"api/utils/pyspark/#framework3.utils.pyspark.PySparkMapReduce.map","title":"<code>map(data, map_function, numSlices=None)</code>","text":"<p>Apply a map function to the input data in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data to be processed.</p> required <code>map_function</code> <code>Callable[..., Any]</code> <p>The function to apply to each element of the data.</p> required <code>numSlices</code> <code>int | None</code> <p>The number of partitions to create. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the map operation as a PySpark RDD.</p> Source code in <code>framework3/utils/pyspark.py</code> <pre><code>def map(\n    self, data: Any, map_function: Callable[..., Any], numSlices: int | None = None\n) -&gt; Any:\n    \"\"\"\n    Apply a map function to the input data in parallel.\n\n    Args:\n        data (Any): The input data to be processed.\n        map_function (Callable[..., Any]): The function to apply to each element of the data.\n        numSlices (int | None, optional): The number of partitions to create. Defaults to None.\n\n    Returns:\n        Any: The result of the map operation as a PySpark RDD.\n    \"\"\"\n    self.rdd = self.sc.parallelize(data, numSlices=numSlices)\n    self.mapped_rdd = self.rdd.map(map_function)\n\n    # Aplicar transformaciones map\n    return self.mapped_rdd\n</code></pre>"},{"location":"api/utils/pyspark/#framework3.utils.pyspark.PySparkMapReduce.reduce","title":"<code>reduce(reduce_function)</code>","text":"<p>Apply a reduce function to the mapped data.</p> <p>Parameters:</p> Name Type Description Default <code>reduce_function</code> <code>Callable[..., Any]</code> <p>The function to reduce the mapped data.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the reduce operation.</p> Source code in <code>framework3/utils/pyspark.py</code> <pre><code>def reduce(self, reduce_function: Callable[..., Any]) -&gt; Any:\n    \"\"\"\n    Apply a reduce function to the mapped data.\n\n    Args:\n        reduce_function (Callable[..., Any]): The function to reduce the mapped data.\n\n    Returns:\n        Any: The result of the reduce operation.\n    \"\"\"\n    result = self.mapped_rdd.reduce(reduce_function)\n    return result\n</code></pre>"},{"location":"api/utils/pyspark/#framework3.utils.pyspark.PySparkMapReduce.stop","title":"<code>stop()</code>","text":"<p>Stop the Spark context.</p> <p>This method should be called when you're done with Spark operations to release resources.</p> Source code in <code>framework3/utils/pyspark.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"\n    Stop the Spark context.\n\n    This method should be called when you're done with Spark operations to release resources.\n    \"\"\"\n    self.sc.stop()\n</code></pre>"},{"location":"api/utils/sklearn/","title":"Sklearn Estimator","text":""},{"location":"api/utils/sklearn/#framework3.utils.skestimator","title":"<code>framework3.utils.skestimator</code>","text":""},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper","title":"<code>SkWrapper</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>A wrapper class for BaseFilter that implements scikit-learn's BaseEstimator interface.</p> <p>This class allows BaseFilter objects to be used with scikit-learn's GridSearchCV and other scikit-learn compatible tools.</p> Key Features <ul> <li>Wraps any BaseFilter subclass to make it compatible with scikit-learn</li> <li>Implements fit, predict, and transform methods</li> <li>Supports getting and setting parameters</li> <li>Handles NotTrainableFilterError for filters that don't require training</li> </ul> Usage <pre><code>from framework3.plugins.filters.classification.svm import ClassifierSVMPlugin\nfrom framework3.utils.skestimator import SkWrapper\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a sample BaseFilter\nclass SampleFilter(ClassifierSVMPlugin):\n    pass\n\n# Create an instance of SkWrapper\nwrapper = SkWrapper(SampleFilter, C=1.0, kernel='rbf')\n\n# Use the wrapper with sklearn's GridSearchCV\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nparam_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}\ngrid_search = GridSearchCV(wrapper, param_grid, cv=3)\ngrid_search.fit(X, y)\n\n# Make predictions\nprint(grid_search.predict([[2.5, 3.5]]))\n</code></pre> <p>Attributes:</p> Name Type Description <code>_z_clazz</code> <code>Type[BaseFilter]</code> <p>The BaseFilter class to be wrapped.</p> <code>_model</code> <code>BaseFilter</code> <p>An instance of the wrapped BaseFilter class.</p> <code>kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments passed to the wrapped BaseFilter class.</p> <p>Methods:</p> Name Description <code>get_zclazz</code> <p>Get the name of the wrapped BaseFilter class.</p> <code>fit</code> <p>Any, y: Any, args, *kwargs) -&gt; 'SkWrapper': Fit the wrapped model to the given data.</p> <code>predict</code> <p>Any) -&gt; Any: Make predictions using the wrapped model.</p> <code>transform</code> <p>Any) -&gt; Any: Transform the input data using the wrapped model.</p> <code>get_params</code> <p>bool = True) -&gt; Dict[str, Any]: Get the parameters of the estimator.</p> <code>set_params</code> <p>Set the parameters of the estimator.</p> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>class SkWrapper(BaseEstimator):\n    \"\"\"\n    A wrapper class for BaseFilter that implements scikit-learn's BaseEstimator interface.\n\n    This class allows BaseFilter objects to be used with scikit-learn's GridSearchCV and other\n    scikit-learn compatible tools.\n\n    Key Features:\n        - Wraps any BaseFilter subclass to make it compatible with scikit-learn\n        - Implements fit, predict, and transform methods\n        - Supports getting and setting parameters\n        - Handles NotTrainableFilterError for filters that don't require training\n\n    Usage:\n        ```python\n        from framework3.plugins.filters.classification.svm import ClassifierSVMPlugin\n        from framework3.utils.skestimator import SkWrapper\n        import numpy as np\n        from sklearn.model_selection import GridSearchCV\n\n        # Create a sample BaseFilter\n        class SampleFilter(ClassifierSVMPlugin):\n            pass\n\n        # Create an instance of SkWrapper\n        wrapper = SkWrapper(SampleFilter, C=1.0, kernel='rbf')\n\n        # Use the wrapper with sklearn's GridSearchCV\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        y = np.array([0, 0, 1, 1])\n        param_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}\n        grid_search = GridSearchCV(wrapper, param_grid, cv=3)\n        grid_search.fit(X, y)\n\n        # Make predictions\n        print(grid_search.predict([[2.5, 3.5]]))\n        ```\n\n    Attributes:\n        _z_clazz (Type[BaseFilter]): The BaseFilter class to be wrapped.\n        _model (BaseFilter): An instance of the wrapped BaseFilter class.\n        kwargs (Dict[str, Any]): Keyword arguments passed to the wrapped BaseFilter class.\n\n    Methods:\n        get_zclazz() -&gt; str: Get the name of the wrapped BaseFilter class.\n        fit(x: Any, y: Any, *args, **kwargs) -&gt; 'SkWrapper': Fit the wrapped model to the given data.\n        predict(x: Any) -&gt; Any: Make predictions using the wrapped model.\n        transform(x: Any) -&gt; Any: Transform the input data using the wrapped model.\n        get_params(deep: bool = True) -&gt; Dict[str, Any]: Get the parameters of the estimator.\n        set_params(**parameters) -&gt; 'SkWrapper': Set the parameters of the estimator.\n    \"\"\"\n\n    def __init__(self, z_clazz: type[BaseFilter], **kwargs: Any):\n        \"\"\"\n        Initialize the SkWrapper.\n\n        Args:\n            z_clazz (Type[BaseFilter]): The BaseFilter class to be wrapped.\n            **kwargs: Keyword arguments to be passed to the wrapped BaseFilter class.\n        \"\"\"\n        self._z_clazz: type[BaseFilter] = z_clazz\n        self._model: BaseFilter = self._z_clazz(**kwargs)  # type: ignore\n        self.kwargs = kwargs\n\n    def get_zclazz(self) -&gt; str:\n        \"\"\"\n        Get the name of the wrapped BaseFilter class.\n\n        Returns:\n            str: The name of the wrapped BaseFilter class.\n        \"\"\"\n        return self._z_clazz.__name__\n\n    def fit(self, x, y, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; \"SkWrapper\":\n        \"\"\"\n        Fit the wrapped model to the given data.\n\n        Args:\n            x (Any): The input features.\n            y (Any): The target values.\n            *args (List[Any]): Additional positional arguments.\n            **kwargs (Dict[str, Any]): Additional keyword arguments.\n\n        Returns:\n            SkWrapper: The fitted estimator.\n        \"\"\"\n        try:\n            self._model.fit(XYData.mock(x), XYData.mock(y))\n        except NotTrainableFilterError:\n            self._model.init()\n\n        return self\n\n    def predict(self, x) -&gt; Any:\n        \"\"\"\n        Make predictions using the wrapped model.\n\n        Args:\n            x (Any): The input features.\n\n        Returns:\n            Any: The predicted values.\n        \"\"\"\n        return self._model.predict(XYData.mock(x)).value\n\n    def transform(self, x) -&gt; Any:\n        \"\"\"\n        Transform the input data using the wrapped model.\n\n        Args:\n            x (Any): The input features.\n\n        Returns:\n            Any: The transformed data.\n        \"\"\"\n\n        return self._model.predict(XYData.mock(x)).value\n\n    def get_params(self, deep=True) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the parameters of the estimator.\n\n        Args:\n            deep (bool): If True, will return the parameters for this estimator and\n                         contained subobjects that are estimators.\n\n        Returns:\n            Dict[str, Any]: Parameter names mapped to their values.\n        \"\"\"\n        return self.kwargs | {\"z_clazz\": self._z_clazz}\n\n    def set_params(self, **parameters: Any) -&gt; \"SkWrapper\":\n        \"\"\"\n        Set the parameters of the estimator.\n\n        Args:\n            **parameters (dict): Estimator parameters.\n\n        Returns:\n            (SkWrapper): Estimator instance.\n        \"\"\"\n        for param, value in parameters.items():\n            if param == \"z_clazz\":\n                if type(value) is Type[BaseFilter]:\n                    self._z_clazz = value\n                else:\n                    raise ValueError(\"z_clazz must be a subclass of BaseFilter\")\n            else:\n                self.kwargs[param] = value\n        self._model = cast(BaseFilter, self._z_clazz(**self.kwargs))\n        return self\n</code></pre>"},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.kwargs","title":"<code>kwargs = kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.__init__","title":"<code>__init__(z_clazz, **kwargs)</code>","text":"<p>Initialize the SkWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>z_clazz</code> <code>Type[BaseFilter]</code> <p>The BaseFilter class to be wrapped.</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the wrapped BaseFilter class.</p> <code>{}</code> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>def __init__(self, z_clazz: type[BaseFilter], **kwargs: Any):\n    \"\"\"\n    Initialize the SkWrapper.\n\n    Args:\n        z_clazz (Type[BaseFilter]): The BaseFilter class to be wrapped.\n        **kwargs: Keyword arguments to be passed to the wrapped BaseFilter class.\n    \"\"\"\n    self._z_clazz: type[BaseFilter] = z_clazz\n    self._model: BaseFilter = self._z_clazz(**kwargs)  # type: ignore\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.fit","title":"<code>fit(x, y, *args, **kwargs)</code>","text":"<p>Fit the wrapped model to the given data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The input features.</p> required <code>y</code> <code>Any</code> <p>The target values.</p> required <code>*args</code> <code>List[Any]</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SkWrapper</code> <code>SkWrapper</code> <p>The fitted estimator.</p> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>def fit(self, x, y, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; \"SkWrapper\":\n    \"\"\"\n    Fit the wrapped model to the given data.\n\n    Args:\n        x (Any): The input features.\n        y (Any): The target values.\n        *args (List[Any]): Additional positional arguments.\n        **kwargs (Dict[str, Any]): Additional keyword arguments.\n\n    Returns:\n        SkWrapper: The fitted estimator.\n    \"\"\"\n    try:\n        self._model.fit(XYData.mock(x), XYData.mock(y))\n    except NotTrainableFilterError:\n        self._model.init()\n\n    return self\n</code></pre>"},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get the parameters of the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, will return the parameters for this estimator and          contained subobjects that are estimators.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Parameter names mapped to their values.</p> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>def get_params(self, deep=True) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the parameters of the estimator.\n\n    Args:\n        deep (bool): If True, will return the parameters for this estimator and\n                     contained subobjects that are estimators.\n\n    Returns:\n        Dict[str, Any]: Parameter names mapped to their values.\n    \"\"\"\n    return self.kwargs | {\"z_clazz\": self._z_clazz}\n</code></pre>"},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.get_zclazz","title":"<code>get_zclazz()</code>","text":"<p>Get the name of the wrapped BaseFilter class.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the wrapped BaseFilter class.</p> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>def get_zclazz(self) -&gt; str:\n    \"\"\"\n    Get the name of the wrapped BaseFilter class.\n\n    Returns:\n        str: The name of the wrapped BaseFilter class.\n    \"\"\"\n    return self._z_clazz.__name__\n</code></pre>"},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The input features.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The predicted values.</p> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>def predict(self, x) -&gt; Any:\n    \"\"\"\n    Make predictions using the wrapped model.\n\n    Args:\n        x (Any): The input features.\n\n    Returns:\n        Any: The predicted values.\n    \"\"\"\n    return self._model.predict(XYData.mock(x)).value\n</code></pre>"},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.set_params","title":"<code>set_params(**parameters)</code>","text":"<p>Set the parameters of the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>**parameters</code> <code>dict</code> <p>Estimator parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>SkWrapper</code> <p>Estimator instance.</p> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>def set_params(self, **parameters: Any) -&gt; \"SkWrapper\":\n    \"\"\"\n    Set the parameters of the estimator.\n\n    Args:\n        **parameters (dict): Estimator parameters.\n\n    Returns:\n        (SkWrapper): Estimator instance.\n    \"\"\"\n    for param, value in parameters.items():\n        if param == \"z_clazz\":\n            if type(value) is Type[BaseFilter]:\n                self._z_clazz = value\n            else:\n                raise ValueError(\"z_clazz must be a subclass of BaseFilter\")\n        else:\n            self.kwargs[param] = value\n    self._model = cast(BaseFilter, self._z_clazz(**self.kwargs))\n    return self\n</code></pre>"},{"location":"api/utils/sklearn/#framework3.utils.skestimator.SkWrapper.transform","title":"<code>transform(x)</code>","text":"<p>Transform the input data using the wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The input features.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The transformed data.</p> Source code in <code>framework3/utils/skestimator.py</code> <pre><code>def transform(self, x) -&gt; Any:\n    \"\"\"\n    Transform the input data using the wrapped model.\n\n    Args:\n        x (Any): The input features.\n\n    Returns:\n        Any: The transformed data.\n    \"\"\"\n\n    return self._model.predict(XYData.mock(x)).value\n</code></pre>"},{"location":"api/utils/typeguard/","title":"Typeguar Notebooks","text":""},{"location":"api/utils/typeguard/#framework3.utils.patch_type_guard","title":"<code>framework3.utils.patch_type_guard</code>","text":""},{"location":"api/utils/typeguard/#framework3.utils.patch_type_guard.patch_inspect_for_notebooks","title":"<code>patch_inspect_for_notebooks()</code>","text":"<p>Patch inspect.getsource to use dill for obtaining source code in notebooks.</p> Source code in <code>framework3/utils/patch_type_guard.py</code> <pre><code>def patch_inspect_for_notebooks():\n    \"\"\"\n    Patch inspect.getsource to use dill for obtaining source code in notebooks.\n    \"\"\"\n    try:\n        import dill\n        import inspect\n\n        inspect.getsource = dill.source.getsource\n        print(\"\u2705 Patched inspect.getsource using dill.\")\n    except ImportError:\n        print(\"\u26a0\ufe0f dill is not installed, skipping inspect patch.\")\n</code></pre>"},{"location":"api/utils/utils/","title":"Utils","text":""},{"location":"api/utils/utils/#framework3.utils.utils","title":"<code>framework3.utils.utils</code>","text":""},{"location":"api/utils/utils/#framework3.utils.utils.method_is_overridden","title":"<code>method_is_overridden(cls, method_name)</code>","text":"Source code in <code>framework3/utils/utils.py</code> <pre><code>def method_is_overridden(cls, method_name):\n    method = getattr(cls, method_name)\n    for base in inspect.getmro(cls)[1:]:  # Skip the class itself\n        if hasattr(base, method_name):\n            return method is not getattr(base, method_name)\n    return False\n</code></pre>"},{"location":"api/utils/wandb/","title":"Weights & Biases","text":""},{"location":"api/utils/wandb/#framework3.utils.wandb","title":"<code>framework3.utils.wandb</code>","text":""},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbAgent","title":"<code>WandbAgent</code>","text":"<p>A class to create and run Weights &amp; Biases agents for sweeps.</p> <p>This class provides a callable interface to create and run Weights &amp; Biases agents, which are used to execute sweep runs with specified configurations.</p> Key Features <ul> <li>Create and run Weights &amp; Biases agents for sweeps</li> <li>Execute custom functions with sweep configurations</li> <li>Automatically handle initialization and teardown of Weights &amp; Biases runs</li> </ul> Usage <pre><code>from framework3.utils.wandb import WandbAgent\n\ndef custom_function(config):\n    # Your sweep run logic here\n    result = ...\n    return result\n\nagent = WandbAgent()\nagent(\"your_sweep_id\", \"your_project_name\", custom_function)\n</code></pre> <p>Methods:</p> Name Description <code>__call__</code> <p>str, project: str, function: Callable) -&gt; None: Create and run a Weights &amp; Biases agent for a specified sweep.</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>class WandbAgent:\n    \"\"\"\n    A class to create and run Weights &amp; Biases agents for sweeps.\n\n    This class provides a callable interface to create and run Weights &amp; Biases agents,\n    which are used to execute sweep runs with specified configurations.\n\n    Key Features:\n        - Create and run Weights &amp; Biases agents for sweeps\n        - Execute custom functions with sweep configurations\n        - Automatically handle initialization and teardown of Weights &amp; Biases runs\n\n    Usage:\n        ```python\n        from framework3.utils.wandb import WandbAgent\n\n        def custom_function(config):\n            # Your sweep run logic here\n            result = ...\n            return result\n\n        agent = WandbAgent()\n        agent(\"your_sweep_id\", \"your_project_name\", custom_function)\n        ```\n\n    Methods:\n        __call__(sweep_id: str, project: str, function: Callable) -&gt; None:\n            Create and run a Weights &amp; Biases agent for a specified sweep.\n    \"\"\"\n\n    @staticmethod\n    def __call__(sweep_id: str, project: str, function: Callable) -&gt; None:\n        \"\"\"\n        Create and run a Weights &amp; Biases agent for a specified sweep.\n\n        This method initializes a Weights &amp; Biases agent, executes the provided function\n        with the sweep configuration, and handles the teardown of the Weights &amp; Biases run.\n\n        Args:\n            sweep_id (str): The ID of the sweep to run.\n            project (str): The name of the Weights &amp; Biases project.\n            function (Callable): A function that takes a configuration dictionary and returns a result to be logged.\n\n        Returns:\n            (None)\n        \"\"\"\n        wandb.agent(  # type: ignore\n            sweep_id,\n            function=lambda: {\n                wandb.init(reinit=\"finish_previous\"),  # type: ignore\n                wandb.log(function(dict(wandb.config))),  # type: ignore\n            },\n            project=project,\n        )  # type: ignore\n        wandb.teardown()  # type: ignore\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbAgent.__call__","title":"<code>__call__(sweep_id, project, function)</code>  <code>staticmethod</code>","text":"<p>Create and run a Weights &amp; Biases agent for a specified sweep.</p> <p>This method initializes a Weights &amp; Biases agent, executes the provided function with the sweep configuration, and handles the teardown of the Weights &amp; Biases run.</p> <p>Parameters:</p> Name Type Description Default <code>sweep_id</code> <code>str</code> <p>The ID of the sweep to run.</p> required <code>project</code> <code>str</code> <p>The name of the Weights &amp; Biases project.</p> required <code>function</code> <code>Callable</code> <p>A function that takes a configuration dictionary and returns a result to be logged.</p> required <p>Returns:</p> Type Description <code>None</code> <p>(None)</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>@staticmethod\ndef __call__(sweep_id: str, project: str, function: Callable) -&gt; None:\n    \"\"\"\n    Create and run a Weights &amp; Biases agent for a specified sweep.\n\n    This method initializes a Weights &amp; Biases agent, executes the provided function\n    with the sweep configuration, and handles the teardown of the Weights &amp; Biases run.\n\n    Args:\n        sweep_id (str): The ID of the sweep to run.\n        project (str): The name of the Weights &amp; Biases project.\n        function (Callable): A function that takes a configuration dictionary and returns a result to be logged.\n\n    Returns:\n        (None)\n    \"\"\"\n    wandb.agent(  # type: ignore\n        sweep_id,\n        function=lambda: {\n            wandb.init(reinit=\"finish_previous\"),  # type: ignore\n            wandb.log(function(dict(wandb.config))),  # type: ignore\n        },\n        project=project,\n    )  # type: ignore\n    wandb.teardown()  # type: ignore\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbRunLogger","title":"<code>WandbRunLogger</code>","text":"Source code in <code>framework3/utils/wandb.py</code> <pre><code>class WandbRunLogger: ...\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager","title":"<code>WandbSweepManager</code>","text":"<p>A manager class for creating and handling Weights &amp; Biases sweeps.</p> <p>This class provides methods to generate sweep configurations, create sweeps, and manage sweep runs for hyperparameter optimization using Weights &amp; Biases.</p> Key Features <ul> <li>Generate sweep configurations from BaseFilter pipelines</li> <li>Create sweeps with customizable parameters and metrics</li> <li>Retrieve sweep information and best configurations</li> <li>Restart sweeps by deleting specific runs</li> </ul> Usage <pre><code>from framework3.utils.wandb import WandbSweepManager\nfrom framework3.base import BaseFilter, BaseMetric, XYData\n\n# Create a WandbSweepManager instance\nsweep_manager = WandbSweepManager()\n\n# Create a sweep\npipeline = YourPipeline()\nscorer = YourScorer()\nx_data = XYData(...)\ny_data = XYData(...)\nsweep_id = sweep_manager.create_sweep(pipeline, \"your_project\", scorer, x_data, y_data)\n\n# Get the best configuration from a sweep\nbest_config = sweep_manager.get_best_config(\"your_project\", sweep_id, order=\"ascending\")\n\n# Restart a sweep\nsweep = sweep_manager.get_sweep(\"your_project\", sweep_id)\nsweep_manager.restart_sweep(sweep, states=[\"failed\", \"crashed\"])\n</code></pre> <p>Methods:</p> Name Description <code>get_grid</code> <p>Dict[str, Any], config: Dict[str, Any]) -&gt; None: Recursively extract grid search parameters from a pipeline configuration.</p> <code>generate_config_for_pipeline</code> <p>BaseFilter) -&gt; Dict[str, Any]: Generate a Weights &amp; Biases sweep configuration from a BaseFilter pipeline.</p> <code>create_sweep</code> <p>BaseFilter, project_name: str, scorer: BaseMetric, x: XYData, y: XYData | None = None) -&gt; str: Create a new sweep in Weights &amp; Biases.</p> <code>get_sweep</code> <p>str, sweep_id: str) -&gt; Any: Retrieve a sweep object from Weights &amp; Biases.</p> <code>get_best_config</code> <p>str, sweep_id: str, order: str) -&gt; Dict[str, Any]: Get the best configuration from a completed sweep.</p> <code>restart_sweep</code> <p>Any, states: List[str] | Literal[\"all\"] = \"all\") -&gt; None: Restart a sweep by deleting runs with specified states.</p> <code>init</code> <p>str, name: str, reinit: bool = True) -&gt; Any: Initialize a new Weights &amp; Biases run.</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>class WandbSweepManager:\n    \"\"\"\n    A manager class for creating and handling Weights &amp; Biases sweeps.\n\n    This class provides methods to generate sweep configurations, create sweeps,\n    and manage sweep runs for hyperparameter optimization using Weights &amp; Biases.\n\n    Key Features:\n        - Generate sweep configurations from BaseFilter pipelines\n        - Create sweeps with customizable parameters and metrics\n        - Retrieve sweep information and best configurations\n        - Restart sweeps by deleting specific runs\n\n    Usage:\n        ```python\n        from framework3.utils.wandb import WandbSweepManager\n        from framework3.base import BaseFilter, BaseMetric, XYData\n\n        # Create a WandbSweepManager instance\n        sweep_manager = WandbSweepManager()\n\n        # Create a sweep\n        pipeline = YourPipeline()\n        scorer = YourScorer()\n        x_data = XYData(...)\n        y_data = XYData(...)\n        sweep_id = sweep_manager.create_sweep(pipeline, \"your_project\", scorer, x_data, y_data)\n\n        # Get the best configuration from a sweep\n        best_config = sweep_manager.get_best_config(\"your_project\", sweep_id, order=\"ascending\")\n\n        # Restart a sweep\n        sweep = sweep_manager.get_sweep(\"your_project\", sweep_id)\n        sweep_manager.restart_sweep(sweep, states=[\"failed\", \"crashed\"])\n        ```\n\n    Methods:\n        get_grid(aux: Dict[str, Any], config: Dict[str, Any]) -&gt; None:\n            Recursively extract grid search parameters from a pipeline configuration.\n        generate_config_for_pipeline(pipeline: BaseFilter) -&gt; Dict[str, Any]:\n            Generate a Weights &amp; Biases sweep configuration from a BaseFilter pipeline.\n        create_sweep(pipeline: BaseFilter, project_name: str, scorer: BaseMetric, x: XYData, y: XYData | None = None) -&gt; str:\n            Create a new sweep in Weights &amp; Biases.\n        get_sweep(project_name: str, sweep_id: str) -&gt; Any:\n            Retrieve a sweep object from Weights &amp; Biases.\n        get_best_config(project_name: str, sweep_id: str, order: str) -&gt; Dict[str, Any]:\n            Get the best configuration from a completed sweep.\n        restart_sweep(sweep: Any, states: List[str] | Literal[\"all\"] = \"all\") -&gt; None:\n            Restart a sweep by deleting runs with specified states.\n        init(group: str, name: str, reinit: bool = True) -&gt; Any:\n            Initialize a new Weights &amp; Biases run.\n    \"\"\"\n\n    @staticmethod\n    def get_grid(aux: Dict[str, Any], config: Dict[str, Any]):\n        \"\"\"\n        Recursively extract grid search parameters from a pipeline configuration.\n\n        Args:\n            aux (Dict[str, Any]): The input configuration dictionary.\n            config (Dict[str, Any]): The output configuration dictionary to be updated.\n        \"\"\"\n        match aux[\"params\"]:\n            case {\"filters\": filters, **r}:\n                for filter_config in filters:\n                    WandbSweepManager.get_grid(filter_config, config)\n            case {\"pipeline\": pipeline, **r}:  # noqa: F841\n                WandbSweepManager.get_grid(pipeline, config)\n            case p_params:\n                if \"_grid\" in aux:\n                    f_config = {}\n                    for param, value in aux[\"_grid\"].items():\n                        print(f\"categorical param: {param}: {value}\")\n                        p_params.update({param: value})\n                        match type(value):\n                            case list():\n                                f_config[param] = {\"values\": value}\n                            case dict():\n                                f_config[param] = value\n                            case _:\n                                f_config[param] = {\"values\": value}\n                    if len(f_config) &gt; 0:\n                        config[\"parameters\"][\"filters\"][\"parameters\"][\n                            str(aux[\"clazz\"])\n                        ] = {\"parameters\": f_config}\n\n    @staticmethod\n    def generate_config_for_pipeline(pipeline: BaseFilter) -&gt; Dict[str, Any]:\n        \"\"\"\n        Generate a Weights &amp; Biases sweep configuration from a BaseFilter pipeline.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to generate the configuration for.\n\n        Returns:\n            Dict[str, Any]: A Weights &amp; Biases sweep configuration.\n        \"\"\"\n        sweep_config: Dict[str, Dict[str, Dict[str, Any]]] = {\n            \"parameters\": {\"filters\": {\"parameters\": {}}, \"pipeline\": {\"value\": {}}}\n        }\n\n        dumped_pipeline = pipeline.item_dump(include=[\"_grid\"])\n\n        WandbSweepManager.get_grid(dumped_pipeline, sweep_config)\n\n        sweep_config[\"parameters\"][\"pipeline\"][\"value\"] = dumped_pipeline\n\n        return sweep_config\n\n    def create_sweep(\n        self,\n        pipeline: BaseFilter,\n        project_name: str,\n        scorer: BaseMetric,\n        x: XYData,\n        y: XYData | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Create a new sweep in Weights &amp; Biases.\n\n        Args:\n            pipeline (BaseFilter): The pipeline to be optimized.\n            project_name (str): The name of the Weights &amp; Biases project.\n            scorer (BaseMetric): The metric used to evaluate the pipeline.\n            x (XYData): The input data.\n            y (XYData | None): The target data (optional).\n\n        Returns:\n            str: The ID of the created sweep.\n        \"\"\"\n        sweep_config = WandbSweepManager.generate_config_for_pipeline(pipeline)\n        sweep_config[\"method\"] = \"grid\"\n        sweep_config[\"parameters\"][\"x_dataset\"] = {\"value\": x._hash}\n        sweep_config[\"parameters\"][\"y_dataset\"] = (\n            {\"value\": y._hash} if y is not None else {\"value\": \"None\"}\n        )\n        sweep_config[\"metric\"] = {\n            \"name\": scorer.__class__.__name__,\n            \"goal\": \"maximize\" if scorer.higher_better else \"minimize\",\n        }\n        print(\"______________________SWEE CONFIG_____________________\")\n        print(sweep_config)\n        print(\"_____________________________________________________\")\n        return wandb.sweep(sweep_config, project=project_name)  # type: ignore\n\n    def get_sweep(self, project_name, sweep_id) -&gt; Any:\n        \"\"\"\n        Retrieve a sweep object from Weights &amp; Biases.\n\n        Args:\n            project_name (str): The name of the Weights &amp; Biases project.\n            sweep_id (str): The ID of the sweep to retrieve.\n\n        Returns:\n            (Any): The sweep object.\n        \"\"\"\n        sweep = wandb.Api().sweep(f\"citius-irlab/{project_name}/sweeps/{sweep_id}\")  # type: ignore\n        return sweep\n\n    def get_best_config(self, project_name, sweep_id, order) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the best configuration from a completed sweep.\n\n        Args:\n            project_name (str): The name of the Weights &amp; Biases project.\n            sweep_id (str): The ID of the sweep.\n            order (str): The order to use when selecting the best run (\"ascending\" or \"descending\").\n\n        Returns:\n            (Dict[str, Any]): The configuration of the best run.\n        \"\"\"\n        sweep = self.get_sweep(project_name, sweep_id)\n        winner_run = sweep.best_run(order=order)\n        return dict(winner_run.config)\n\n    def restart_sweep(self, sweep, states: List[str] | Literal[\"all\"] = \"all\"):\n        \"\"\"\n        Restart a sweep by deleting runs with specified states.\n\n        Args:\n            sweep (Any): The sweep object to restart.\n            states (List[str] | Literal[\"all\"]): The states of runs to delete, or \"all\" to delete all runs.\n        \"\"\"\n        # Eliminar todas las ejecuciones fallidas\n        for run in sweep.runs:\n            if run.state in states or states == \"all\":\n                run.delete()\n                print(\"Deleting run:\", run.id)\n\n    def init(self, group: str, name: str, reinit=True) -&gt; Any:\n        \"\"\"\n        Initialize a new Weights &amp; Biases run.\n\n        Args:\n            group (str): The group name for the run.\n            name (str): The name of the run.\n            reinit (bool): Whether to reinitialize if a run is already in progress.\n\n        Returns:\n            (Any): The initialized run object.\n        \"\"\"\n        run = wandb.init(group=group, name=name, reinit=reinit)  # type: ignore\n        return run\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager.create_sweep","title":"<code>create_sweep(pipeline, project_name, scorer, x, y=None)</code>","text":"<p>Create a new sweep in Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to be optimized.</p> required <code>project_name</code> <code>str</code> <p>The name of the Weights &amp; Biases project.</p> required <code>scorer</code> <code>BaseMetric</code> <p>The metric used to evaluate the pipeline.</p> required <code>x</code> <code>XYData</code> <p>The input data.</p> required <code>y</code> <code>XYData | None</code> <p>The target data (optional).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The ID of the created sweep.</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>def create_sweep(\n    self,\n    pipeline: BaseFilter,\n    project_name: str,\n    scorer: BaseMetric,\n    x: XYData,\n    y: XYData | None = None,\n) -&gt; str:\n    \"\"\"\n    Create a new sweep in Weights &amp; Biases.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to be optimized.\n        project_name (str): The name of the Weights &amp; Biases project.\n        scorer (BaseMetric): The metric used to evaluate the pipeline.\n        x (XYData): The input data.\n        y (XYData | None): The target data (optional).\n\n    Returns:\n        str: The ID of the created sweep.\n    \"\"\"\n    sweep_config = WandbSweepManager.generate_config_for_pipeline(pipeline)\n    sweep_config[\"method\"] = \"grid\"\n    sweep_config[\"parameters\"][\"x_dataset\"] = {\"value\": x._hash}\n    sweep_config[\"parameters\"][\"y_dataset\"] = (\n        {\"value\": y._hash} if y is not None else {\"value\": \"None\"}\n    )\n    sweep_config[\"metric\"] = {\n        \"name\": scorer.__class__.__name__,\n        \"goal\": \"maximize\" if scorer.higher_better else \"minimize\",\n    }\n    print(\"______________________SWEE CONFIG_____________________\")\n    print(sweep_config)\n    print(\"_____________________________________________________\")\n    return wandb.sweep(sweep_config, project=project_name)  # type: ignore\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager.generate_config_for_pipeline","title":"<code>generate_config_for_pipeline(pipeline)</code>  <code>staticmethod</code>","text":"<p>Generate a Weights &amp; Biases sweep configuration from a BaseFilter pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>BaseFilter</code> <p>The pipeline to generate the configuration for.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A Weights &amp; Biases sweep configuration.</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>@staticmethod\ndef generate_config_for_pipeline(pipeline: BaseFilter) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a Weights &amp; Biases sweep configuration from a BaseFilter pipeline.\n\n    Args:\n        pipeline (BaseFilter): The pipeline to generate the configuration for.\n\n    Returns:\n        Dict[str, Any]: A Weights &amp; Biases sweep configuration.\n    \"\"\"\n    sweep_config: Dict[str, Dict[str, Dict[str, Any]]] = {\n        \"parameters\": {\"filters\": {\"parameters\": {}}, \"pipeline\": {\"value\": {}}}\n    }\n\n    dumped_pipeline = pipeline.item_dump(include=[\"_grid\"])\n\n    WandbSweepManager.get_grid(dumped_pipeline, sweep_config)\n\n    sweep_config[\"parameters\"][\"pipeline\"][\"value\"] = dumped_pipeline\n\n    return sweep_config\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager.get_best_config","title":"<code>get_best_config(project_name, sweep_id, order)</code>","text":"<p>Get the best configuration from a completed sweep.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>The name of the Weights &amp; Biases project.</p> required <code>sweep_id</code> <code>str</code> <p>The ID of the sweep.</p> required <code>order</code> <code>str</code> <p>The order to use when selecting the best run (\"ascending\" or \"descending\").</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The configuration of the best run.</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>def get_best_config(self, project_name, sweep_id, order) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the best configuration from a completed sweep.\n\n    Args:\n        project_name (str): The name of the Weights &amp; Biases project.\n        sweep_id (str): The ID of the sweep.\n        order (str): The order to use when selecting the best run (\"ascending\" or \"descending\").\n\n    Returns:\n        (Dict[str, Any]): The configuration of the best run.\n    \"\"\"\n    sweep = self.get_sweep(project_name, sweep_id)\n    winner_run = sweep.best_run(order=order)\n    return dict(winner_run.config)\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager.get_grid","title":"<code>get_grid(aux, config)</code>  <code>staticmethod</code>","text":"<p>Recursively extract grid search parameters from a pipeline configuration.</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>Dict[str, Any]</code> <p>The input configuration dictionary.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>The output configuration dictionary to be updated.</p> required Source code in <code>framework3/utils/wandb.py</code> <pre><code>@staticmethod\ndef get_grid(aux: Dict[str, Any], config: Dict[str, Any]):\n    \"\"\"\n    Recursively extract grid search parameters from a pipeline configuration.\n\n    Args:\n        aux (Dict[str, Any]): The input configuration dictionary.\n        config (Dict[str, Any]): The output configuration dictionary to be updated.\n    \"\"\"\n    match aux[\"params\"]:\n        case {\"filters\": filters, **r}:\n            for filter_config in filters:\n                WandbSweepManager.get_grid(filter_config, config)\n        case {\"pipeline\": pipeline, **r}:  # noqa: F841\n            WandbSweepManager.get_grid(pipeline, config)\n        case p_params:\n            if \"_grid\" in aux:\n                f_config = {}\n                for param, value in aux[\"_grid\"].items():\n                    print(f\"categorical param: {param}: {value}\")\n                    p_params.update({param: value})\n                    match type(value):\n                        case list():\n                            f_config[param] = {\"values\": value}\n                        case dict():\n                            f_config[param] = value\n                        case _:\n                            f_config[param] = {\"values\": value}\n                if len(f_config) &gt; 0:\n                    config[\"parameters\"][\"filters\"][\"parameters\"][\n                        str(aux[\"clazz\"])\n                    ] = {\"parameters\": f_config}\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager.get_sweep","title":"<code>get_sweep(project_name, sweep_id)</code>","text":"<p>Retrieve a sweep object from Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>The name of the Weights &amp; Biases project.</p> required <code>sweep_id</code> <code>str</code> <p>The ID of the sweep to retrieve.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sweep object.</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>def get_sweep(self, project_name, sweep_id) -&gt; Any:\n    \"\"\"\n    Retrieve a sweep object from Weights &amp; Biases.\n\n    Args:\n        project_name (str): The name of the Weights &amp; Biases project.\n        sweep_id (str): The ID of the sweep to retrieve.\n\n    Returns:\n        (Any): The sweep object.\n    \"\"\"\n    sweep = wandb.Api().sweep(f\"citius-irlab/{project_name}/sweeps/{sweep_id}\")  # type: ignore\n    return sweep\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager.init","title":"<code>init(group, name, reinit=True)</code>","text":"<p>Initialize a new Weights &amp; Biases run.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>The group name for the run.</p> required <code>name</code> <code>str</code> <p>The name of the run.</p> required <code>reinit</code> <code>bool</code> <p>Whether to reinitialize if a run is already in progress.</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>The initialized run object.</p> Source code in <code>framework3/utils/wandb.py</code> <pre><code>def init(self, group: str, name: str, reinit=True) -&gt; Any:\n    \"\"\"\n    Initialize a new Weights &amp; Biases run.\n\n    Args:\n        group (str): The group name for the run.\n        name (str): The name of the run.\n        reinit (bool): Whether to reinitialize if a run is already in progress.\n\n    Returns:\n        (Any): The initialized run object.\n    \"\"\"\n    run = wandb.init(group=group, name=name, reinit=reinit)  # type: ignore\n    return run\n</code></pre>"},{"location":"api/utils/wandb/#framework3.utils.wandb.WandbSweepManager.restart_sweep","title":"<code>restart_sweep(sweep, states='all')</code>","text":"<p>Restart a sweep by deleting runs with specified states.</p> <p>Parameters:</p> Name Type Description Default <code>sweep</code> <code>Any</code> <p>The sweep object to restart.</p> required <code>states</code> <code>List[str] | Literal['all']</code> <p>The states of runs to delete, or \"all\" to delete all runs.</p> <code>'all'</code> Source code in <code>framework3/utils/wandb.py</code> <pre><code>def restart_sweep(self, sweep, states: List[str] | Literal[\"all\"] = \"all\"):\n    \"\"\"\n    Restart a sweep by deleting runs with specified states.\n\n    Args:\n        sweep (Any): The sweep object to restart.\n        states (List[str] | Literal[\"all\"]): The states of runs to delete, or \"all\" to delete all runs.\n    \"\"\"\n    # Eliminar todas las ejecuciones fallidas\n    for run in sweep.runs:\n        if run.state in states or states == \"all\":\n            run.delete()\n            print(\"Deleting run:\", run.id)\n</code></pre>"},{"location":"architecture/","title":"\ud83c\udfd7\ufe0f LabChain Architecture","text":"<p>LabChain is designed with a modular, decoupled, and extensible architecture, built to support experimentation, evaluation, and comparison of models or processes in AI environments in a fully reproducible way.</p>"},{"location":"architecture/#core-components","title":"\ud83d\udd0c Core Components","text":"Component Description <code>BaseFilter</code> Functional component that transforms, trains, or predicts <code>BasePipeline</code> Chains multiple filters into a sequential or parallel strategy <code>BaseMetric</code> Evaluates the performance of models or pipelines <code>BaseSplitter</code> Splits data into folds for validation <code>BaseOptimizer</code> Optimizes hyperparameters over pipelines or filters <code>BaseStorer</code> Manages storage and retrieval of objects"},{"location":"architecture/#typical-flow","title":"\ud83d\udd04 Typical Flow","text":"<pre><code>graph TD\n    A[Data Input] --&gt; B[XYData]\n    U[BaseSplitter] --&gt; |Split Data| B\n    B --&gt; C[F3Pipeline]\n    C --&gt; D{Filters}\n    D --&gt; |Filter 1| E[PCA]\n    D --&gt; |Filter 2| F[Custom Filter]\n    D --&gt; |Filter N| G[...]\n    E &amp; F &amp; G --&gt; H[Processed Data]\n    H --&gt; I{Metrics}\n    I --&gt; |Metric 1| J[F1 Score]\n    I --&gt; |Metric 2| K[Precision]\n    I --&gt; |Metric 3| L[Recall]\n    J &amp; K &amp; L --&gt; M[Evaluation Results]\n    C --&gt; |Fit| N[Model Training]\n    C --&gt; |Predict| O[Prediction]\n    N --&gt; P[Trained Model]\n    P --&gt; O\n    O --&gt; Q[Output Predictions]\n    R[Container] --&gt; |Dependency Injection| C\n    R --&gt; |Data Storage| S[BaseStorer]\n    S --&gt; |Cache| T[Data Caching]\n    T --&gt; B\n    V[BaseOptimizer] --&gt; |Tune| C\n\n    style C stroke:#1976D2,stroke-width:2px,color:#FFFFFF\n</code></pre>"},{"location":"architecture/#design-principles","title":"\ud83e\uddec Design Principles","text":"<ul> <li>Modularity: Each class has a single responsibility and does it well.</li> <li>Dependency Injection: Plugins are registered and configured via a <code>Container</code>.</li> <li>Extensibility: Easily create new components without modifying the core.</li> <li>Reproducibility: All pipelines are configurable and fully traceable.</li> </ul>"},{"location":"architecture/#container-and-dependency-injection","title":"\ud83d\udce6 Container and Dependency Injection","text":"<p>The <code>Container</code> is a central component in LabChain, managing the registration and retrieval of various components. It allows for easy plugin management and dependency injection throughout the framework.</p>"},{"location":"architecture/#data-handling","title":"\ud83d\udd22 Data Handling","text":"<p>LabChain uses the <code>XYData</code> class for handling input and output data. This class provides a consistent interface for data manipulation across different components of the framework.</p>"},{"location":"architecture/#plugin-system","title":"\ud83d\udd0c Plugin System","text":"<p>LabChain's plugin system allows for easy extension of the framework's capabilities. You can create custom filters, metrics, optimizers, and more by inheriting from the base classes and registering them with the Container.</p>"},{"location":"architecture/#example-workflow","title":"\ud83d\udcca Example Workflow","text":"<p>Here's a basic example of how these components might interact in a typical workflow:</p> <pre><code>from framework3 import (\n    F1,\n    F3Pipeline,\n    KnnFilter,\n    Precission,\n    Recall,\n    StandardScalerPlugin,\n    WandbOptimizer,\n    XYData,\n    KFoldSplitter\n)\n\nfrom sklearn import datasets\n\n# Prepare data\nX = XYData(\n    _hash=\"Iris X data\",\n    _path=\"/datasets\",\n    _value=iris.data,  # type: ignore\n)\ny = XYData(\n    _hash=\"Iris y data\",\n    _path=\"/datasets\",\n    _value=iris.target,  # type: ignore\n)\n\n# Create pipeline\npipeline = F3Pipeline(\n    filters=[\n        StandardScalerPlugin(),\n        KnnFilter().grid({\n             'n_neighbors': [3, 5]\n        })\n    ],\n    metrics=[F1(), Precission(), Recall()]\n).splitter(\n    KFoldSplitter(\n        n_splits=2,\n        shuffle=True,\n        random_state=42,\n    )\n).optimizer(\n    WandbOptimizer(\n        project=\"test_project\",\n        sweep_id=None,\n        scorer=F1(),\n    )\n)\n\n# Fit and evaluate\npipeline.fit(x_data, y_data)\ny_pred = XYData.mock(prediction.value)\n\nresults = pipeline.evaluate(x_data, y_data, y_pred)\n</code></pre> <p>For more detailed examples and use cases, check out the Examples section.</p> <p>To explore all available components, methods, and configuration options, refer to the API Reference.</p>"},{"location":"examples/","title":"Framework3 Examples","text":"<p>Welcome to the Examples section of the Framework3 documentation. Here you'll find a collection of hands-on tutorials and real-world use cases to help you make the most of the framework's capabilities.</p> <p>These examples are designed to walk you through common workflows and showcase best practices when working with Framework3. Whether you're just getting started or aiming to deepen your understanding, this section will guide you through using the platform effectively.</p>"},{"location":"examples/#available-tutorials","title":"Available Tutorials","text":""},{"location":"examples/#simple-tutorials","title":"Simple Tutorials","text":"<p>An index of basic tutorials to get you started:</p> <ol> <li> <p>Datasets Storage    Learn how to store, retrieve, update, and manage datasets using the <code>Container.ds</code> storage backend.</p> </li> <li> <p>Creating a Simple Pipeline    Build a basic machine learning pipeline using Framework3\u2019s modular components.</p> </li> </ol>"},{"location":"examples/#live-tutorials","title":"Live Tutorials","text":"<p>Actively maintained notebooks showcasing more advanced use cases:</p> <ol> <li> <p>Data Ingestion    A working notebook exploring dataset storage and retrieval.</p> </li> <li> <p>Basic Framework Usage    Learn how to create custom filters and integrate them into a pipeline.</p> </li> <li> <p>Caching Data    Techniques for caching data from computationally expensive processes.</p> </li> <li> <p>Optuna and K-Fold    Combine optimizers and data splitters to perform grid search, cross-validation, and hyperparameter tuning.</p> </li> <li> <p>WandB and K-Fold    Leverage Weights &amp; Biases along with K-Fold strategies for model optimization.</p> </li> </ol>"},{"location":"examples/#real-world-applications","title":"Real-World Applications","text":"<p>Explore how Framework3 has been applied in real scientific research:</p> <p>Each tutorial includes step-by-step instructions, code examples, and explanations to help you apply these concepts in your own projects. While we recommend going through them in order, feel free to jump to whichever topic suits your current needs.</p> <p>This section will be regularly updated as Framework3 evolves \u2014 stay tuned for new tutorials and enhancements.</p> <p>Happy experimenting!</p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/","title":"Tutorial: Investigating Topic Coherence with Framework3 using DWR","text":"<p>In this tutorial, we'll use Framework3 to explore different word embeddings and evaluate topic coherence using Distributed Word Representations (DWR) and the average distance between topic words. We'll create a custom pipeline that incorporates word embeddings, topic modeling, and coherence evaluation.</p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#introduction","title":"Introduction","text":"<p>In the field of natural language processing and topic modeling, evaluating the coherence of topics is a crucial aspect. Distributed Word Representations (DWR) have proven to be a valuable tool for calculating the coherence of representative words in a topic <sup>1</sup>. While Framework3 itself does not incorporate these DWR models, our examples demonstrate how various embedding techniques can be utilized within the framework for topic coherence measurement and coherence metrics evaluation.</p> <p>It's important to note that there are several approaches to evaluating topic coherence metrics, each with its own strengths and methodologies:</p> <ol> <li> <p>Word Intrusion: This method involves inserting a word that doesn't belong into a set of topic words and asking human evaluators to identify the intruder. It provides a direct measure of how well humans can interpret the topic.</p> </li> <li> <p>Large Language Models (LLMs) with Human Oversight: Recent advancements have explored using LLMs to evaluate topic coherence, with human experts validating or refining the LLM's assessments. This approach combines the scalability of automated methods with human judgment.</p> </li> <li> <p>Human Annotations: Direct human evaluation of topic coherence, where experts or crowd workers rate the coherence of topics based on various criteria.</p> </li> </ol> <p>For this investigation, we will be using the strategy employed by <sup>2</sup>. We have been generously provided with the human annotations from their work, which offers a valuable ground truth for our coherence metrics evaluations. This approach allows us to directly compare our automated metrics against human judgments, providing a robust benchmark for our coherence measures.</p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#word-embeddings","title":"Word Embeddings","text":"<p>In our examples, we showcase the use of both classic and modern embedding techniques:</p> <ul> <li> <p>Classic embeddings:</p> <ul> <li>Word2Vec <sup>3</sup></li> <li>GloVe <sup>4</sup></li> <li>FastText <sup>5</sup></li> </ul> </li> <li> <p>Modern transformer-based embeddings:</p> <ul> <li>BERT <sup>6</sup></li> <li>RoBERTa <sup>7</sup></li> <li>ALBERT <sup>8</sup></li> </ul> </li> </ul>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#coherence-metrics","title":"Coherence Metrics","text":"<p>Framework3 facilitates the implementation and comparison of various coherence metrics, including both classical and experimental approaches:</p> <ul> <li>Classical metrics:<ul> <li>UMass <sup>9</sup></li> <li>CV (Vector Coherence) <sup>10</sup></li> <li>NPMI (Normalized Pointwise Mutual Information) <sup>11</sup></li> <li>UCI <sup>12</sup></li> </ul> </li> </ul> <p>These metrics offer different perspectives on the quality and coherence of generated topics, potentially enabling a more comprehensive evaluation of topic models. However, it's important to note that the effectiveness of these traditional coherence metrics has been questioned. Somo works criticize the assumption that these metrics are reliable indicators of topic quality or effective in determining the optimal number of topics, especially in domain-specific collections <sup>13</sup>. This critique underscores the need for careful consideration and potentially new approaches when evaluating topic coherence, particularly in specialized domains.</p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#methodology","title":"Methodology","text":"<p>We will follow a methodology similar to <sup>2</sup><sup>14</sup>, with some key differences. Our approach will use various word representations for topic coherence evaluation, and we'll incorporate human evaluations from <sup>2</sup> as a gold standard. Here's a detailed breakdown of our methodology:</p> <ol> <li>Word Representations: We'll use a range of word embedding techniques, including:</li> <li>Classic embeddings: Word2Vec, GloVe, and FastText</li> <li> <p>Modern transformer-based embeddings: BERT, RoBERTa, and ALBERT</p> </li> <li> <p>Topic Modeling: We'll use the provided topics from <sup>2</sup>. These topics were generated using Latent Dirichlet Allocation (LDA).</p> </li> <li> <p>Coherence Metrics: We'll implement and compare various coherence metrics:</p> </li> <li>Classical metrics: UMass, CV, NPMI, and UCI</li> <li> <p>Embedding-based metrics:</p> <ul> <li>Average pairwise cosine similarity: This metric calculates the average cosine similarity between all pairs of word embeddings within a topic. It provides a measure of how closely related the words in a topic are in the embedding space.</li> <li>Other potential similarity metrics that could be added in the future include:</li> <li>Euclidean distance: Measures the straight-line distance between word vectors in the embedding space.</li> <li>Manhattan distance: Calculates the sum of the absolute differences of the vector coordinates.</li> <li>Word Mover's Distance (WMD): Measures the minimum distance that the embedded words of one topic need to \"travel\" to reach the embedded words of another topic.</li> <li>Soft Cosine Similarity: An extension of cosine similarity that takes into account the similarity between different words.</li> <li>Maximum Mean Discrepancy (MMD): A kernel-based method to compare distributions of word embeddings between topics.</li> </ul> </li> <li> <p>Human Evaluations: We'll use the human evaluations provided by <sup>2</sup> as our ground truth for topic coherence.</p> </li> <li> <p>Correlation Analysis: To assess the effectiveness of each coherence metric and word representation combination, we'll use Spearman's rank correlation coefficient. This will measure the correlation between the automated coherence scores and the human evaluations.</p> </li> <li> <p>Comparative Analysis: We'll compare the performance of different word representations and coherence metrics based on their correlation with human judgments.</p> </li> </ol> <p>By following this methodology, we aim to provide a comprehensive evaluation of topic coherence metrics using various word representations, grounded in human judgments. This approach will allow us to identify which combinations of word embeddings and coherence metrics align most closely with human perceptions of topic quality.</p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#implementation-example","title":"Implementation Example","text":"<p>In this example, we'll create three classes that extend <code>BaseFilter</code> to implement our coherence evaluation pipeline:</p> <ol> <li><code>GensimEmbedder</code>: A filter to extract embeddings using Gensim models.</li> <li><code>TransformersEmbedder</code>: A filter to extract embeddings using Transformer models.</li> <li><code>PerTopicMeanSimilarity</code>: A filter to calculate topic coherence based on the average similarity of topic words.</li> </ol> <p>Let's implement these classes:</p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#1-gensimembedder","title":"1. GensimEmbedder","text":"<pre><code>from typing import Callable\nfrom framework3 import Container, XYData\nfrom framework3.base import BaseFilter\nfrom tqdm import tqdm\n\nimport gensim.downloader as api\nimport torch\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=\"Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer.\")\n\n\n@Container.bind()\nclass GensimEmbedder(BaseFilter):\n    def __init__(self, model_path: str):\n        super().__init__(model_path=model_path)\n        self._embeddings:Callable = lambda: api.load(model_path)\n\n    def predict(self, x: XYData) -&gt; XYData:\n        self._embeddings = self._embeddings()\n        all_m = []\n        for topic in tqdm(x.value):\n            topic_m = []\n            for word in topic:\n                try:\n                    topic_m.append(self._embeddings[str(word)]) # type: ignore\n                except KeyError:\n                    topic_m.append([0] * self._embeddings.vector_size) # type: ignore\n            all_m.append(torch.tensor(topic_m))\n\n        all_stack = torch.stack(all_m)\n        return XYData.mock(all_stack.squeeze(2))\n</code></pre>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#2-transformersembedder","title":"2. TransformersEmbedder","text":"<p>It is crucial to keep the _model variable private because Transformer classes are not hashable when generating filter hashes. This is essential for the proper functioning of the caching and serialization system.</p> <pre><code>from transformers import AutoModel, AutoTokenizer\nfrom tqdm import tqdm\n\nfrom framework3.base import BaseFilter\nfrom framework3 import Container\nfrom framework3.base.base_types import XYData\n\nimport torch\n\n\n@Container.bind()\nclass TransformersEmbedder(BaseFilter):\n    def __init__(self, model_path: str):\n        super().__init__(model_path = model_path)\n        self.model_path = model_path\n        self._tokenizer = lambda: AutoTokenizer.from_pretrained(model_path)\n        self._model = lambda: AutoModel.from_pretrained(model_path)\n\n\n    def predict(self, x: XYData) -&gt; XYData:\n        self._tokenizer = self._tokenizer()\n        self._model = self._model()\n\n        all_m = []\n        for topic in tqdm(x.value):\n            topic_m = []\n            for word in topic:\n                encoded_input = self._tokenizer.encode(\n                    str(word),\n                    return_tensors='pt'\n                )\n                out = self._model(input_ids=encoded_input)\n                mean_embeddings = torch.mean(\n                    out.last_hidden_state[0][1:-1].detach().cpu(),\n                    axis=0\n                )\n                topic_m.append(mean_embeddings)\n\n            all_m.append(torch.stack(topic_m))\n        all_stack = torch.stack(all_m)\n        return XYData.mock(all_stack.squeeze(2))\n</code></pre>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#3pertopicmeansimilarity","title":"3.PerTopicMeanSimilarity","text":"<pre><code>from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n\nfrom framework3.base import BaseFilter\nfrom framework3 import Container\nfrom framework3.base.base_types import XYData\n\nimport torch\n\n@Container.bind()\nclass PerTopicMeanSimilarity(BaseFilter):\n\n    def predict(self, x: XYData) -&gt; XYData:\n        topic_tensors = x.value\n\n        topic_results = []\n        for i in range(topic_tensors.size(0)):\n            top_sim = pairwise_cosine_similarity(topic_tensors[i], topic_tensors[i])\n            eye = torch.eye(top_sim.shape[0])\n            top_sim = top_sim * (1-eye)\n            res = torch.sum(top_sim, dim=1) / torch.sum(top_sim&gt;0, dim=1)\n            topic_results.append(res)\n\n        return XYData.mock(torch.mean(torch.stack(topic_results), dim=1))\n</code></pre>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#pipelines-definition","title":"Pipelines Definition","text":"<p>In this section, we'll define our pipelines for evaluating topic coherence using different language models. Before we dive into the pipeline definitions, it's important to note a few key points about our data and approach:</p> <ol> <li> <p>Data Source: The data we're using for this tutorial has been kindly provided by the authors of Aletras and Stevenson (2013). Due to privacy and licensing considerations, we are unable to make this data publicly available.</p> </li> <li> <p>Pipeline Structure: We will create a separate pipeline for each language model we're using. In total, we'll have six pipelines: three for classic word embeddings (Word2Vec, GloVe, and FastText) and three for transformer-based models (BERT, RoBERTa, and ALBERT).</p> </li> <li> <p>Combining Pipelines: To efficiently process and compare results from all our models, we'll combine these individual pipelines into a single MonoPipeline.</p> </li> <li> <p>Visualization: After running our combined pipeline, we'll visualize the results to compare the coherence scores across different models.</p> </li> </ol> <p>Let's start by defining our individual pipelines:</p> <pre><code>transformers_models = [\n        'bert-base-uncased',\n        'roberta-base',\n        'albert-base-v2'\n    ]\n\ngensim_models = [\n    'word2vec-google-news-300',\n    'glove-wiki-gigaword-300',\n    'fasttext-wiki-news-subwords-300'\n]\n\nt_pipelines = [F3Pipeline(\n    filters=[\n        Cached(\n            filter = TransformersEmbedder(model),\n            cache_data=True,\n            cache_filter=False\n        ),\n        PerTopicMeanSimilarity()\n    ]\n) for model in transformers_models]\n\n\ng_pielines = [F3Pipeline(\n    filters=[\n        Cached(\n            filter = GensimEmbedder(model),\n            cache_data=True,\n            cache_filter=False\n        ),\n        PerTopicMeanSimilarity()\n    ]\n) for model in gensim_models]\n\npipeline_l = lambda: MonoPipeline(\n        filters=[*t_pipelines, *g_pipelines]\n    )\n</code></pre>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#executing-pipelines","title":"Executing Pipelines","text":"<p>Now that we have defined our pipelines, it's time to put them into action. In this section, we'll walk through the process of executing our MonoPipeline across multiple datasets to evaluate topic coherence using various language models.</p> <p>The execution process involves several key steps:</p> <ol> <li> <p>Data Preparation: We'll use three datasets, each containing topics that we want to evaluate for coherence. These datasets are represented as XYData objects in our framework.</p> </li> <li> <p>Pipeline Execution: Our MonoPipeline, which combines multiple individual pipelines for different language models, will be executed on each dataset.</p> </li> <li> <p>Result Collection: For each dataset and model combination, we'll collect the coherence scores produced by our pipeline.</p> </li> <li> <p>Data Organization: We'll organize the results into a structured format (a pandas DataFrame) for easy analysis and comparison.</p> </li> </ol> <p>This process will give us a comprehensive view of how different language models perform in assessing topic coherence across various datasets. By executing the pipeline in this systematic way, we can ensure consistency in our evaluation process and facilitate direct comparisons between models and datasets.</p> <p>In the following code snippet, we'll implement this execution process, demonstrating how to leverage Framework3's capabilities to efficiently process multiple datasets and models in parallel.</p> <p><pre><code>final_df = pd.DataFrame()\n\nfor name, config in datasets.items():\n\n    #Data preparation\n    df = pd.read_csv(config['topics'], header=None, index_col=0)\n\n    X = XYData(\n        _hash=name,\n        _path='datasets',\n        _value=df.loc[:,df.columns[1:]].values\n    ) # type: ignore\n\n    # Pipeline instanciation and execution\n    pipeline = pipeline_l()\n\n    pipeline.init()\n\n    result = pipeline.predict(X)\n\n    # Results Collection and Organization\n\n    coh_results = pd.read_csv(config['results'], index_col=0)\n\n    groun_truth = XYData.mock(\n        torch.tensor(coh_results['humans'].values)\n    )\n\n    df = pd.DataFrame(\n        preduction2.value,\n        columns=transformers_models+gensim_models\n    )\n\n    df_combined = pd.concat([coh_results.iloc[:,1:], df], axis=1)\n\n    final_df[name] = df_combined.corr(method='spearman')['humans']\n\n\nprint(final_df.drop(index='humans'))\n</code></pre> Results are ploted in the command line. <pre><code>* Cached({'filter': GensimEmbedder({'model_path': 'word2vec-google-news-300'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, 'storage': None})\n         - El dato XYData(_hash='fb0ce99107cab2c2886814fc2361ed8f95795239', _path='GensimEmbedder/496cbf49bcf06f2e111b0472a88637fc113c157a') Existe, se crea lambda.\n\n* PerTopicMeanSimilarity({})\n         * Downloading: &lt;_io.BufferedReader name='/home/manuel.couto.pintos/Documents/code/research_coherence_metrics/cache/GensimEmbedder/496cbf49bcf06f2e111b0472a88637fc113c157a/fb0ce99107cab2c2886814fc2361ed8f95795239'&gt;\n____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n...\n\n* Cached({'filter': GensimEmbedder({'model_path': 'fasttext-wiki-news-subwords-300'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, 'storage': None})\n         - El dato XYData(_hash='218e787b8bbdb20a93d03053ded7a097fb4bfe2e', _path='GensimEmbedder/1e50810d01b16fee88e1f5bef9066480b4db6f24') Existe, se crea lambda.\n\n* PerTopicMeanSimilarity({})\n         * Downloading: &lt;_io.BufferedReader name='/home/manuel.couto.pintos/Documents/code/research_coherence_metrics/cache/GensimEmbedder/1e50810d01b16fee88e1f5bef9066480b4db6f24/218e787b8bbdb20a93d03053ded7a097fb4bfe2e'&gt;\n                                     20ng       nyt  genomics\nc_npmi                           0.475670  0.741326  0.470800\nu_mass                           0.545781  0.683995  0.396676\nc_v                              0.503730  0.651311  0.472650\nc_uci                            0.487970  0.730463  0.449871\ncpmi                             0.589414  0.563997  0.061916\nbert-base-uncased                0.436957  0.311909  0.205890\nroberta-base                     0.309897  0.397525 -0.077470\nalbert-base-v2                   0.155378  0.304366  0.154627\nword2vec-google-news-300         0.581495  0.460657  0.540738\nglove-wiki-gigaword-300          0.857683  0.752312  0.796962\nfasttext-wiki-news-subwords-300  0.853550  0.521773  0.720706\n</code></pre></p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#analysis-of-topic-coherence-metrics","title":"Analysis of Topic Coherence Metrics","text":"<p>Based on the provided results, we can draw the following conclusions about the correlation between various coherence metrics and human evaluation across different datasets:</p> <ol> <li>Traditional Coherence Metrics</li> <li><code>c_npmi</code>, <code>c_uci</code> and  <code>c_v</code> show consistently high correlation across all datasets, suggesting they may be reliable proxies for human judgment of topic coherence.</li> <li><code>u_mass</code> also perform well, particularly for the 'nyt' dataset, indicating their potential effectiveness in news-related contexts.</li> <li> <p><code>cpmi</code> shows variable performance, with a notably low correlation for the 'genomics' dataset, suggesting it may be less reliable for specialized domains.</p> </li> <li> <p>Word Embedding Models</p> </li> <li> <p>Word embedding models generally demonstrate stronger correlation with human evaluation compared to transformer-based models:</p> <ul> <li><code>glove-wiki-gigaword-300</code> shows the highest overall correlation across all datasets, suggesting it might be the most reliable metric for approximating human judgment.</li> <li><code>fasttext-wiki-news-subwords-300</code> performs exceptionally well, particularly for '20ng' and 'genomics', indicating its strength in handling diverse or specialized vocabularies.</li> <li><code>word2vec-google-news-300</code> shows consistent performance across datasets, though generally lower than GloVe and FastText.</li> </ul> </li> <li> <p>Transformer-Based Models</p> </li> <li> <p>Transformer models show lower correlation with human evaluation compared to traditional metrics and word embeddings:</p> <ul> <li><code>bert-base-uncased</code> performs the best among transformers, but still falls short of word embedding models.</li> <li><code>roberta-base</code> and <code>albert-base-v2</code> show particularly low correlation, with <code>roberta-base</code> even showing a negative correlation for the 'genomics' dataset.</li> </ul> </li> <li> <p>Dataset-Specific Observations</p> </li> <li>The 'nyt' dataset shows the highest correlations across most metrics, suggesting that news articles may produce more coherent topics that align well with human judgment.</li> <li>The 'genomics' dataset presents challenges for some metrics, particularly <code>cpmi</code> and transformer models, highlighting the importance of choosing appropriate metrics for specialized domains.</li> </ol>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#implications-for-topic-modeling-evaluation","title":"Implications for Topic Modeling Evaluation","text":"<ol> <li>Traditional coherence metrics, particularly <code>c_npmi</code>, <code>c_uci</code> and <code>c_v</code>, remain robust choices for evaluating topic coherence across different domains.</li> <li>Word embedding models, especially GloVe and FastText, show the strongest correlation with human evaluation and should be considered as primary metrics for assessing topic coherence.</li> <li>Transformer-based models, despite their success in other NLP tasks, may not be the best choice for evaluating topic coherence, at least in their current implementation.</li> <li>When evaluating topic models, it's advisable to use a combination of traditional metrics (e.g., <code>c_npmi</code>) and word embedding-based metrics (e.g., GloVe or FastText) for a more comprehensive assessment that likely aligns better with human judgment.</li> </ol>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#framework3-for-topic-modeling-and-coherence-metrics-evaluation","title":"Framework3 for Topic Modeling and Coherence Metrics Evaluation","text":"<p>Framework3 offers a comprehensive solution for researchers and practitioners working with topic models and coherence metrics. Its unique features make it particularly well-suited for experimenting with and evaluating different approaches to topic modeling and coherence assessment.</p>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#key-features-for-topic-modeling-research","title":"Key Features for Topic Modeling Research","text":"<ol> <li> <p>Unified Metric Integration: Seamlessly incorporate diverse coherence metrics, from traditional measures like NPMI and UCI to advanced embedding-based techniques, allowing for comprehensive comparisons.</p> </li> <li> <p>Flexible Topic Model Integration: Easily experiment with various topic modeling algorithms, including LDA, NMF, and more recent neural approaches, within a single framework.</p> </li> <li> <p>Customizable Evaluation Pipelines: Design and execute complex evaluation workflows that combine multiple metrics, models, and datasets, facilitating thorough comparative analyses.</p> </li> <li> <p>Extensible Plugin Architecture: Rapidly implement and test new coherence metrics or topic modeling techniques through a plugin system, promoting innovation and experimentation.</p> </li> <li> <p>Scalable Processing: Leverage distributed computing capabilities to handle large-scale topic modeling tasks and coherence evaluations across extensive corpora.</p> </li> </ol>"},{"location":"examples/218e787b8bbdb20a93d03053ded7a097fb4bfe2e/#references","title":"References","text":"<ol> <li> <p>Sergey I Nikolenko. Topic quality metrics based on distributed word representations. In Proceedings of the 39<sup>th</sup> International ACM SIGIR conference on Research and Development in Information Retrieval, 1029\u20131032. 2016.\u00a0\u21a9</p> </li> <li> <p>Nikolaos Aletras and Mark Stevenson. Evaluating Topic Coherence Using Distributional Semantics. In Alexander Koller and Katrin Erk, editors, Proceedings of the 10<sup>th</sup> International Conference on Computational Semantics (IWCS 2013) \u2013 Long Papers, 13\u201322. Potsdam, Germany, March 2013. Association for Computational Linguistics. URL: https://aclanthology.org/W13-0102 (visited on 2024-10-20).\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Tomas Mikolov. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\u00a0\u21a9</p> </li> <li> <p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 1532\u20131543. 2014.\u00a0\u21a9</p> </li> <li> <p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the association for computational linguistics, 5:135\u2013146, 2017.\u00a0\u21a9</p> </li> <li> <p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: bidirectional encoder representations from transformers. arXiv preprint arXiv:1810.04805, pages 15, 2018.\u00a0\u21a9</p> </li> <li> <p>Yinhan Liu. Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\u00a0\u21a9</p> </li> <li> <p>Z Lan. Albert: a lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\u00a0\u21a9</p> </li> <li> <p>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. Optimizing semantic coherence in topic models. In Regina Barzilay and Mark Johnson, editors, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 262\u2013272. Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL: https://aclanthology.org/D11-1024.\u00a0\u21a9</p> </li> <li> <p>Michael R\u00f6der, Andreas Both, and Alexander Hinneburg. Exploring the space of topic coherence measures. In Proceedings of the eighth ACM international conference on Web search and data mining, 399\u2013408. 2015.\u00a0\u21a9</p> </li> <li> <p>Gerlof Bouma. Normalized (pointwise) mutual information in collocation extraction. Proceedings of the Biennial GSCL Conference 2009, pages, 01 2009.\u00a0\u21a9</p> </li> <li> <p>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. Automatic evaluation of topic coherence. In Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics, 100\u2013108. 2010.\u00a0\u21a9</p> </li> <li> <p>Caitlin Doogan and Wray Buntine. Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 3824\u20133848. Online, 2021. Association for Computational Linguistics. URL: https://aclanthology.org/2021.naacl-main.300 (visited on 2024-10-05), doi:10.18653/v1/2021.naacl-main.300.\u00a0\u21a9</p> </li> <li> <p>Sergey I. Nikolenko. Topic Quality Metrics Based on Distributed Word Representations. In Proceedings of the 39<sup>th</sup> International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR '16, 1029\u20131032. New York, NY, USA, July 2016. Association for Computing Machinery. URL: https://dl.acm.org/doi/10.1145/2911451.2914720 (visited on 2024-10-22), doi:10.1145/2911451.2914720.\u00a0\u21a9</p> </li> </ol>"},{"location":"examples/data_ingestion/","title":"Data Ingestion and Storage in Framework3","text":"<p>This tutorial demonstrates how to use different storage backends in Framework3 for storing and retrieving data, including local storage and Amazon S3.</p>"},{"location":"examples/data_ingestion/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, make sure you have:</p> <ol> <li>Framework3 installed</li> <li>Necessary libraries imported:    <pre><code>from framework3.container import Container\nfrom framework3.plugins.storage import S3Storage\nimport pandas as pd\nimport numpy as np\nimport os\n</code></pre></li> <li>For S3 storage: You need an S3-compatible service provider. Options include:<ul> <li>Cloud providers: Amazon AWS, IDrive e2</li> <li>Self-hosted solutions: MinIO, Ceph</li> <li>Local development: LocalStack (for simulating S3 locally)</li> </ul> </li> </ol> <p>Ensure you have the necessary credentials and endpoint information for your chosen S3 service.</p>"},{"location":"examples/data_ingestion/#1-creating-sample-data","title":"1. Creating Sample Data","text":"<p>Let's start by creating some sample data that we'll use throughout this tutorial:</p> <pre><code># Create sample data\ndf = pd.DataFrame({\n    'A': np.random.rand(100),\n    'B': np.random.randint(0, 100, 100),\n    'C': ['cat', 'dog', 'bird'] * 33 + ['cat']\n})\n</code></pre>"},{"location":"examples/data_ingestion/#2-local-storage","title":"2. Local Storage","text":"<p>By default, Framework3 uses local storage. Here's how to use it:</p>"},{"location":"examples/data_ingestion/#storing-data-locally","title":"Storing Data Locally","text":"<p><pre><code># Store the DataFrame locally\nContainer.ds.save('sample_data_local', df)\nprint(\"Data stored successfully locally\")\n</code></pre> <pre><code>     * Saving in local path: cache/datasets/sample_data_local\n     * Saved !\nData stored successfully locally\n</code></pre></p>"},{"location":"examples/data_ingestion/#listing-local-data","title":"Listing Local Data","text":"<p><pre><code>local_files = Container.ds.list()\nprint(\"Files in local storage:\", local_files)\n</code></pre> <pre><code>Files in local storage: ['sample_data', 'sample_data_local']\n</code></pre></p>"},{"location":"examples/data_ingestion/#retrieving-local-data","title":"Retrieving Local Data","text":"<p><pre><code># Retrieve the stored DataFrame from local storage\nretrieved_df = Container.ds.load('sample_data_local')\nprint(\"Data retrieved successfully from local storage\")\nprint(retrieved_df.value.head())\n</code></pre> <pre><code>Data retrieved successfully from local storage\n     * Downloading: &lt;_io.BufferedReader name='cache/datasets/sample_data_local'&gt;\n          A   B     C\n0  0.858184  40   cat\n1  0.467917  62   dog\n2  0.810327  86  bird\n3  0.194756  89   cat\n4  0.313579  61   dog\n</code></pre></p>"},{"location":"examples/data_ingestion/#updating-local-data","title":"Updating Local Data","text":"<p><pre><code># Update the DataFrame\ndf['D'] = np.random.choice(['X', 'Y', 'Z'], 100)\n\n# Store the updated DataFrame locally\nContainer.ds.update('sample_data_local', df)\nprint(\"Updated data stored successfully locally\")\n\n# Retrieve and display the updated DataFrame\nupdated_df = Container.ds.load('sample_data_local')\nprint(updated_df.value.head())\n</code></pre> <pre><code>     * Saving in local path: cache/datasets/sample_data_local\n     * Saved !\nUpdated data stored successfully locally\n     * Downloading: &lt;_io.BufferedReader name='cache/datasets/sample_data_local'&gt;\n          A   B     C  D\n0  0.858184  40   cat  Z\n1  0.467917  62   dog  Y\n2  0.810327  86  bird  X\n3  0.194756  89   cat  X\n4  0.313579  61   dog  X\n</code></pre></p>"},{"location":"examples/data_ingestion/#deleting-local-data","title":"Deleting Local Data","text":"<p><pre><code># Delete the stored data from local storage\nContainer.ds.delete('sample_data_local')\nprint(\"Data deleted successfully from local storage\")\n</code></pre> <pre><code>Data deleted successfully from local storage\n</code></pre></p>"},{"location":"examples/data_ingestion/#3-s3-storage","title":"3. S3 Storage","text":"<p>Now, let's see how to use S3 storage for the same operations:</p>"},{"location":"examples/data_ingestion/#configuring-s3-storage","title":"Configuring S3 Storage","text":"<p>First, we need to configure the S3 storage backend:</p> <pre><code># Configure S3 storage\ns3_storage = S3Storage(\n    bucket=os.environ.get('TEST_BUCKET_NAME'), # type: ignore\n    region_name=os.environ.get('REGION_NAME'), # type: ignore\n    access_key=os.environ.get('ACCESS_KEY'), # type: ignore\n    access_key_id=os.environ.get('ACCESS_KEY_ID'), # type: ignore\n    endpoint_url=os.environ.get('ENDPOINT_URL'),\n)\n\n# Set S3 storage as the default storage backend\nContainer.storage = s3_storage\n</code></pre>"},{"location":"examples/data_ingestion/#storing-data-in-s3","title":"Storing Data in S3","text":"<p><pre><code># Store the DataFrame in S3\nContainer.ds.save('sample_data_s3', df)\nprint(\"Data stored successfully in S3\")\n</code></pre> <pre><code>- Binary prepared!\n- Stream ready!\n     * Object size 8e-08 GBs\nUpload Complete!\nData stored successfully in S3\n</code></pre></p>"},{"location":"examples/data_ingestion/#listing-data-in-s3","title":"Listing Data in S3","text":"<p><pre><code>s3_files = Container.ds.list()\nprint(\"Files in S3 bucket:\", s3_files)\n</code></pre> <pre><code>Files in S3 bucket: ['test-bucket/datasets/sample_data_s3']\n</code></pre></p>"},{"location":"examples/data_ingestion/#retrieving-data-from-s3","title":"Retrieving Data from S3","text":"<p><pre><code># Retrieve the stored DataFrame from S3\nretrieved_df = Container.ds.load('sample_data_s3')\nprint(\"Data retrieved successfully from S3\")\nprint(retrieved_df.value.head())\n</code></pre> <pre><code>Data retrieved successfully from S3\n          A   B     C  D\n0  0.301524  95   cat  Y\n1  0.101139  20   dog  X\n2  0.852597  49  bird  X\n3  0.049054  59   cat  Z\n4  0.463926  59   dog  X\n</code></pre></p>"},{"location":"examples/data_ingestion/#updating-data-in-s3","title":"Updating Data in S3","text":"<p><pre><code># Update the DataFrame\ndf['E'] = np.random.choice(['P', 'Q', 'R'], 100)\n\n# Store the updated DataFrame in S3\nContainer.ds.update('sample_data_s3', df)\nprint(\"Updated data stored successfully in S3\")\n\n# Retrieve and display the updated DataFrame\nupdated_df = Container.ds.load('sample_data_s3')\nprint(updated_df.value.head())\n</code></pre> <pre><code>- Binary prepared!\n- Stream ready!\n     * Object size 8e-08 GBs\nUpload Complete!\nUpdated data stored successfully in S3\n          A   B     C  D  E\n0  0.735935  60   cat  Y  P\n1  0.772428  23   dog  Y  Q\n2  0.509925   6  bird  X  Q\n3  0.775553   7   cat  Z  R\n4  0.395329  81   dog  X  P\n</code></pre></p>"},{"location":"examples/data_ingestion/#deleting-data-from-s3","title":"Deleting Data from S3","text":"<p><pre><code># Delete the stored data from S3\nContainer.ds.delete('sample_data_s3')\nprint(\"Data deleted successfully from S3\")\n</code></pre> <pre><code>Deleted!\nData deleted successfully from S3\n</code></pre></p>"},{"location":"examples/data_ingestion/#conclusion","title":"Conclusion","text":"<p>This tutorial demonstrated how to use both local storage and S3 storage in Framework3 to:</p> <ol> <li>Store data</li> <li>List stored data</li> <li>Retrieve data</li> <li>Update stored data</li> <li>Delete data</li> </ol> <p>The <code>Container.ds</code> interface provides a consistent way to interact with different storage backends, making it easy to switch between local and S3 storage as needed.</p>"},{"location":"examples/simple_f3pipeline/","title":"Tutorial: F3Pipeline with Cache","text":"<p>This tutorial will guide you through a simple example of using F3Pipeline, including the Cache functionality.</p>"},{"location":"examples/simple_f3pipeline/#1-imports","title":"1. Imports","text":"<p>First, we import the required classes and functions:</p> <pre><code>from framework3 import F3Pipeline\nfrom framework3.plugins.filters import StandardScalerPlugin\nfrom framework3.plugins.filters import ClassifierSVMPlugin\nfrom framework3.plugins.metrics import F1\nfrom framework3.base import XYData\nfrom framework3.plugins.filters import Cached\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nfrom rich import print\n</code></pre>"},{"location":"examples/simple_f3pipeline/#2-data-preparation","title":"2. Data Preparation","text":"<p>We load the Iris dataset and split it into training and test sets:</p> <pre><code># Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Crear objetos XYData\nx_train = XYData('Iris dataset X train', 'dataset', X_train)\ny_train = XYData('Iris dataset Y train', 'dataset', y_train)\nx_test = XYData('Iris dataset X test', 'dataset', X_test)\ny_test = XYData('Iris dataset Y test', 'dataset', y_test)\n</code></pre>"},{"location":"examples/simple_f3pipeline/#3-creating-the-pipeline-with-cache","title":"3. Creating the Pipeline with Cache","text":"<p>We create a pipeline that includes a cached <code>StandardScalerPlugin</code> and a <code>ClassifierSVMPlugin</code>:</p> <p><pre><code>pipeline = F3Pipeline(\n    filters=[\n        Cached(\n            filter=StandardScalerPlugin(),\n            cache_data=True,\n            cache_filter=True,\n            overwrite=False\n        ),\n        ClassifierSVMPlugin()\n    ],\n    metrics=[F1()]\n)\nprint(pipeline)\n</code></pre> <pre><code>F3Pipeline(\n    filters=[\n        Cached(filter=StandardScalerPlugin(), cache_data=True, cache_filter=True, overwrite=False, storage=None),\n        ClassifierSVMPlugin(C=1.0, kernel='linear', gamma='scale')\n    ],\n    metrics=[F1(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n</code></pre></p>"},{"location":"examples/simple_f3pipeline/#4-training-the-model","title":"4. Training the Model","text":"<p>We train the model with the training data:</p> <p><pre><code>pipeline.fit(x_train, y_train)\n</code></pre> <pre><code>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False, 'storage': None}):\n         - El filtro StandardScalerPlugin({}) con hash 4f0150e0e11419085ce0f08ab077b7e5891f817b No existe, se va a entrenar.\n         - El filtro StandardScalerPlugin({}) Se cachea.\n     * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/model\n     * Saved !\n         - El dato XYData(_hash='f77f9d95466939988cdd6a13f0cb91260b94c99d', _path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') No existe, se va a crear.\n         - El dato XYData(_hash='f77f9d95466939988cdd6a13f0cb91260b94c99d', _path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Se cachea.\n     * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/f77f9d95466939988cdd6a13f0cb91260b94c99d\n     * Saved !\n* ClassifierSVMPlugin({'C': 1.0, 'kernel': 'linear', 'gamma': 'scale'}):\n</code></pre></p>"},{"location":"examples/simple_f3pipeline/#5-prediction-and-evaluation","title":"5. Prediction and Evaluation","text":"<p>We make predictions and evaluate the model:</p> <p><pre><code># Make predictions\npredictions = pipeline.predict(x_test)\n\n# Evaluate the model\nevaluation = pipeline.evaluate(x_test, y_test, predictions)\nprint(\"Evaluation results:\", evaluation)\n</code></pre> <pre><code>Predicting pipeline...\n****************************************************************************************************\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False, 'storage': None})\n         - El dato XYData(_hash='0403ab1f29eb3d1d59f857aa03f7af153d7ff357', _path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') No existe, se va a crear.\n         - El dato XYData(_hash='0403ab1f29eb3d1d59f857aa03f7af153d7ff357', _path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Se cachea.\n     * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/0403ab1f29eb3d1d59f857aa03f7af153d7ff357\n     * Saved !\n* ClassifierSVMPlugin({'C': 1.0, 'kernel': 'linear', 'gamma': 'scale'})\n____________________________________________________________________________________________________\nEvaluating pipeline...\n____________________________________________________________________________________________________\nResultados de la evaluaci\u00f3n:\n{'F1': 0.9664109121909632}\n</code></pre></p>"},{"location":"examples/simple_f3pipeline/#6-demonstrating-cache-usage","title":"6. Demonstrating Cache Usage","text":"<p>We run the pipeline again to demonstrate the use of cache:</p> <p><pre><code>print(\"Segunda ejecuci\u00f3n (deber\u00eda usar datos en cach\u00e9):\")\n# Crear el pipeline con Cache\npipeline = F3Pipeline(\n    filters=[\n        Cached(\n            filter=StandardScalerPlugin(),\n            cache_data=True,\n            cache_filter=True,\n            overwrite=False,\n        ),\n        KnnFilter(),\n    ],\n    metrics=[F1()],\n)\nprint(pipeline)\n\npipeline.fit(x_train, y_train)\npredictions = pipeline.predict(x_test)\nevaluation = pipeline.evaluate(x_test, y_test, predictions)\n\nprint(\"Resultados de la evaluaci\u00f3n:\", evaluation)\n</code></pre> <pre><code>Segunda ejecuci\u00f3n (deber\u00eda usar datos en cach\u00e9):\nF3Pipeline(\n    filters=[\n        Cached(filter=StandardScalerPlugin(), cache_data=True, cache_filter=True, overwrite=False, storage=None),\n        KnnFilter(\n            n_neighbors=5,\n            weights='uniform',\n            algorithm='auto',\n            leaf_size=30,\n            p=2,\n            metric='minkowski',\n            metric_params=None,\n            n_jobs=None\n        )\n    ],\n    metrics=[F1(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False,\n'storage': None}):\n         - El filtro StandardScalerPlugin({}) Existe, se crea lambda.\n         - El dato XYData(_hash='f77f9d95466939988cdd6a13f0cb91260b94c99d', _path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Existe, se crea lambda.\n* KnnFilter({'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None}):\n     * Downloading: &lt;_io.BufferedReader name='cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/f77f9d95466939988cdd6a13f0cb91260b94c99d'&gt;\n____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False, 'storage': None})\n         - El dato XYData(_hash='a2131ee7caeb76fd704b11b77d0456223b7e0437', _path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') No existe, se va a crear.\n         - Existe un Lambda por lo que se recupera el filtro del storage.\n     * Downloading: &lt;_io.BufferedReader name='cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/model'&gt;\n         - El dato XYData(_hash='a2131ee7caeb76fd704b11b77d0456223b7e0437', _path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Se cachea.\n     * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/a2131ee7caeb76fd704b11b77d0456223b7e0437\n     * Saved !\n* KnnFilter({'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None})\n____________________________________________________________________________________________________\nEvaluating pipeline...\n____________________________________________________________________________________________________\nResultados de la evaluaci\u00f3n:\n{'F1': 1.0}\n</code></pre></p> <p>In this second execution, you should notice that the <code>StandardScalerPlugin</code> uses the cached data, which may result in faster execution time.</p>"},{"location":"examples/simple_f3pipeline/#conclusion","title":"Conclusion","text":"<p>This tutorial has demonstrated how to use F3Pipeline with the Cache functionality in Framework3. We've seen how to create a pipeline that includes a cached filter, how to train the model, make predictions, and evaluate performance. We've also shown how caching can improve performance in subsequent runs.</p>"},{"location":"examples/notebooks/caching_heavy_data/","title":"Caching Data","text":"<p>Sometimes we might want to avoid running the same pipeline multiple times with the same parameters. For this porpouse F3 have a caching mechanism.</p> In\u00a0[1]: Copied! <pre>from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n\npatch_inspect_for_notebooks()\n</pre> from framework3.utils.patch_type_guard import patch_inspect_for_notebooks  patch_inspect_for_notebooks() <pre>\u2705 Patched inspect.getsource using dill.\n</pre> In\u00a0[2]: Copied! <pre>from sklearn.datasets import fetch_20newsgroups\n\n# Cargar el conjunto de datos 20 Newsgroups\ntrain = fetch_20newsgroups(subset=\"train\")\ntest = fetch_20newsgroups(subset=\"test\")\n</pre> from sklearn.datasets import fetch_20newsgroups  # Cargar el conjunto de datos 20 Newsgroups train = fetch_20newsgroups(subset=\"train\") test = fetch_20newsgroups(subset=\"test\") In\u00a0[3]: Copied! <pre>from framework3.base import XYData\n\n\nX_train = XYData(\n    _hash=\"20NG train X\",\n    _path=\"/datasets\",\n    _value=train.data,  # type: ignore\n)\nX_test = XYData(\n    _hash=\"20NG test X\",\n    _path=\"/datasets\",\n    _value=test.data,  # type: ignore\n)\ny_train = XYData(\n    _hash=\"20NG train y\",\n    _path=\"/datasets\",\n    _value=train.target,  # type: ignore\n)\ny_test = XYData(\n    _hash=\"20NG test y\",\n    _path=\"/datasets\",\n    _value=test.target,  # type: ignore\n)\n</pre> from framework3.base import XYData   X_train = XYData(     _hash=\"20NG train X\",     _path=\"/datasets\",     _value=train.data,  # type: ignore ) X_test = XYData(     _hash=\"20NG test X\",     _path=\"/datasets\",     _value=test.data,  # type: ignore ) y_train = XYData(     _hash=\"20NG train y\",     _path=\"/datasets\",     _value=train.target,  # type: ignore ) y_test = XYData(     _hash=\"20NG test y\",     _path=\"/datasets\",     _value=test.target,  # type: ignore ) <p>Let's download a pre-trained language model for sentence embeddings.</p> In\u00a0[4]: Copied! <pre>from framework3.plugins.filters.llm import HuggingFaceSentenceTransformerPlugin\n\nllm = HuggingFaceSentenceTransformerPlugin(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n)\n</pre> from framework3.plugins.filters.llm import HuggingFaceSentenceTransformerPlugin  llm = HuggingFaceSentenceTransformerPlugin(     model_name=\"sentence-transformers/all-mpnet-base-v2\" ) <p>We can perform PCA to reduce dimensionality and then apply SVM for classification. But we don't know wich hiperparamters are the best for this particular data. For that porpose we can use a simple grid search.</p> In\u00a0[5]: Copied! <pre>from framework3 import F1, ClassifierSVMPlugin, SklearnOptimizer\nfrom framework3.plugins.filters import PCAPlugin\nfrom framework3.plugins.pipelines.sequential import F3Pipeline\n\n\ngrid_pipelin = F3Pipeline(\n    filters=[\n        PCAPlugin().grid({\"n_components\": [10, 50, 100]}),\n        ClassifierSVMPlugin(kernel=\"rbf\").grid(\n            {\"C\": [0.1, 1.0, 10.0], \"gamma\": [1e-3, 1e-4]}\n        ),\n    ],\n    metrics=[F1()],\n).optimizer(SklearnOptimizer(scoring=\"f1_weighted\", cv=2))\n</pre> from framework3 import F1, ClassifierSVMPlugin, SklearnOptimizer from framework3.plugins.filters import PCAPlugin from framework3.plugins.pipelines.sequential import F3Pipeline   grid_pipelin = F3Pipeline(     filters=[         PCAPlugin().grid({\"n_components\": [10, 50, 100]}),         ClassifierSVMPlugin(kernel=\"rbf\").grid(             {\"C\": [0.1, 1.0, 10.0], \"gamma\": [1e-3, 1e-4]}         ),     ],     metrics=[F1()], ).optimizer(SklearnOptimizer(scoring=\"f1_weighted\", cv=2)) <pre>{\n    'clazz': 'F3Pipeline',\n    'params': {\n        'filters': [\n            {'clazz': 'PCAPlugin', 'params': {'n_components': 2}, '_grid': {'n_components': [10, 50, 100]}},\n            {\n                'clazz': 'ClassifierSVMPlugin',\n                'params': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'},\n                '_grid': {'C': [0.1, 1.0, 10.0], 'gamma': [0.001, 0.0001]}\n            }\n        ],\n        'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}],\n        'overwrite': False,\n        'store': False,\n        'log': False\n    }\n}\n</pre> <p>Now it comes the moment to put all together. The llm will create a heavy data item wich is the output embeddings of the model and we don't what to compute several times the same embeddings. We will cache this item using the cache_data=True parameter.</p> In\u00a0[6]: Copied! <pre>from framework3 import Cached, Precission, Recall\n\n\nfinal_pipeline = F3Pipeline(\n    filters=[Cached(llm, cache_data=True, cache_filter=False), grid_pipelin],\n    metrics=[F1(), Precission(), Recall()],\n)\n</pre> from framework3 import Cached, Precission, Recall   final_pipeline = F3Pipeline(     filters=[Cached(llm, cache_data=True, cache_filter=False), grid_pipelin],     metrics=[F1(), Precission(), Recall()], ) In\u00a0[7]: Copied! <pre>final_pipeline.fit(X_train, y_train)\n_y = final_pipeline.predict(X_test)\n</pre> final_pipeline.fit(X_train, y_train) _y = final_pipeline.predict(X_test) <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>        *Cached({'filter': HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El filtro HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}) con hash cd52a2089d77df27ae1a888d97422cd38e3bb01a No existe, se va a \nentrenar.\n</pre> <pre>         - El dato XYData(_hash='0bb5f0568e0be233f4dfc5bbe7e893a6369f353e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') No existe, se va a crear.\n</pre> <pre>         - El dato XYData(_hash='0bb5f0568e0be233f4dfc5bbe7e893a6369f353e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') Se cachea.\n</pre> <pre>\t * Saving in local path: cache/HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a/0bb5f0568e0be233f4dfc5bbe7e893a6369f353e\n\t * Saved !\n</pre> <pre>        *SklearnOptimizer({'scoring': 'f1_weighted', 'cv': 2, 'pipeline': F3Pipeline({'filters': \n[PCAPlugin({'n_components': 2}), ClassifierSVMPlugin({'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'})], 'metrics': \n[F1({'average': 'weighted'})], 'overwrite': False, 'store': False, 'log': False})})\n</pre> <pre>Fitting 2 folds for each of 18 candidates, totalling 36 fits\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>        *Cached({'filter': HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El dato XYData(_hash='68fc2995310bba822e143578b3f4ee9ddd9f212e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') No existe, se va a crear.\n</pre> <pre>         - El dato XYData(_hash='68fc2995310bba822e143578b3f4ee9ddd9f212e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') Se cachea.\n</pre> <pre>\t * Saving in local path: cache/HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a/68fc2995310bba822e143578b3f4ee9ddd9f212e\n\t * Saved !\n</pre> <pre>        *SklearnOptimizer({'scoring': 'f1_weighted', 'cv': 2, 'pipeline': F3Pipeline({'filters': \n[PCAPlugin({'n_components': 2}), ClassifierSVMPlugin({'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'})], 'metrics': \n[F1({'average': 'weighted'})], 'overwrite': False, 'store': False, 'log': False})})\n</pre> In\u00a0[8]: Copied! <pre>final_pipeline.evaluate(X_test, y_test, _y)\n</pre> final_pipeline.evaluate(X_test, y_test, _y) <pre>____________________________________________________________________________________________________\nEvaluating pipeline......\n****************************************************************************************************\n</pre> Out[8]: <pre>{'F1': 0.7983619633942866,\n 'Precission': 0.8005437637532571,\n 'Recall': 0.798194370685077}</pre> <p>Petty cool right, but this didn't show the full potential of the caching mechanism. The benefits of caching comes when we want to change things without repeating unnecesary computations. For instance, if we whant to try to optimize other clasifiers or pipelines, the same embeddings won't be computed again. The F3 will use the cached data.</p> In\u00a0[9]: Copied! <pre>from framework3 import F1, KnnFilter, SklearnOptimizer\nfrom framework3.plugins.filters import PCAPlugin\nfrom framework3.plugins.pipelines.sequential import F3Pipeline\n\n\ngrid_pipelin_v2 = F3Pipeline(\n    filters=[\n        PCAPlugin().grid({\"n_components\": [10, 50, 100]}),\n        KnnFilter().grid({\"n_neighbors\": [2, 5, 10]}),\n    ],\n    metrics=[F1()],\n).optimizer(SklearnOptimizer(scoring=\"f1_weighted\", cv=2))\n</pre> from framework3 import F1, KnnFilter, SklearnOptimizer from framework3.plugins.filters import PCAPlugin from framework3.plugins.pipelines.sequential import F3Pipeline   grid_pipelin_v2 = F3Pipeline(     filters=[         PCAPlugin().grid({\"n_components\": [10, 50, 100]}),         KnnFilter().grid({\"n_neighbors\": [2, 5, 10]}),     ],     metrics=[F1()], ).optimizer(SklearnOptimizer(scoring=\"f1_weighted\", cv=2)) <pre>{\n    'clazz': 'F3Pipeline',\n    'params': {\n        'filters': [\n            {'clazz': 'PCAPlugin', 'params': {'n_components': 2}, '_grid': {'n_components': [10, 50, 100]}},\n            {\n                'clazz': 'KnnFilter',\n                'params': {\n                    'n_neighbors': 5,\n                    'weights': 'uniform',\n                    'algorithm': 'auto',\n                    'leaf_size': 30,\n                    'p': 2,\n                    'metric': 'minkowski',\n                    'metric_params': None,\n                    'n_jobs': None\n                },\n                '_grid': {'n_neighbors': [2, 5, 10]}\n            }\n        ],\n        'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}],\n        'overwrite': False,\n        'store': False,\n        'log': False\n    }\n}\n</pre> <p>Now lets update our main pipeline with this new pipeline.</p> In\u00a0[10]: Copied! <pre>final_pipeline = F3Pipeline(\n    filters=[Cached(llm, cache_data=True, cache_filter=False), grid_pipelin_v2],\n    metrics=[F1(), Precission(), Recall()],\n)\n</pre> final_pipeline = F3Pipeline(     filters=[Cached(llm, cache_data=True, cache_filter=False), grid_pipelin_v2],     metrics=[F1(), Precission(), Recall()], ) <p>Let's train and evaluate this modified pipeline and see what happend.</p> In\u00a0[11]: Copied! <pre>final_pipeline.fit(X_train, y_train)\n_y = final_pipeline.predict(X_test)\n</pre> final_pipeline.fit(X_train, y_train) _y = final_pipeline.predict(X_test) <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>        *Cached({'filter': HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El filtro HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}) con hash cd52a2089d77df27ae1a888d97422cd38e3bb01a No existe, se va a \nentrenar.\n</pre> <pre>         - El dato XYData(_hash='0bb5f0568e0be233f4dfc5bbe7e893a6369f353e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') Existe, se crea lambda.\n</pre> <pre>        *SklearnOptimizer({'scoring': 'f1_weighted', 'cv': 2, 'pipeline': F3Pipeline({'filters': \n[PCAPlugin({'n_components': 2}), KnnFilter({'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', \n'leaf_size': 30, 'p': 2, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None})], 'metrics': \n[F1({'average': 'weighted'})], 'overwrite': False, 'store': False, 'log': False})})\n</pre> <pre>\t * Downloading: &lt;_io.BufferedReader name='cache/HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a/0bb5f0568e0be233f4dfc5bbe7e893a6369f353e'&gt;\nFitting 2 folds for each of 9 candidates, totalling 18 fits\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>        *Cached({'filter': HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El dato XYData(_hash='68fc2995310bba822e143578b3f4ee9ddd9f212e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') Existe, se crea lambda.\n</pre> <pre>        *SklearnOptimizer({'scoring': 'f1_weighted', 'cv': 2, 'pipeline': F3Pipeline({'filters': \n[PCAPlugin({'n_components': 2}), KnnFilter({'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', \n'leaf_size': 30, 'p': 2, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None})], 'metrics': \n[F1({'average': 'weighted'})], 'overwrite': False, 'store': False, 'log': False})})\n</pre> <pre>\t * Downloading: &lt;_io.BufferedReader name='cache/HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a/68fc2995310bba822e143578b3f4ee9ddd9f212e'&gt;\n</pre> In\u00a0[12]: Copied! <pre>final_pipeline.evaluate(X_test, y_test, _y)\n</pre> final_pipeline.evaluate(X_test, y_test, _y) <pre>____________________________________________________________________________________________________\nEvaluating pipeline......\n****************************************************************************************************\n</pre> Out[12]: <pre>{'F1': 0.8090253572616752,\n 'Precission': 0.8127742649585784,\n 'Recall': 0.8090812533191716}</pre> <p>As you can see, the F3 has used the cached data from the first pipeline, so it's faster and less resource consuming.</p>"},{"location":"examples/notebooks/caching_heavy_data/#caching-data","title":"Caching Data\u00b6","text":""},{"location":"examples/notebooks/caching_heavy_data/#how-to-cache-heavy-computations-while-looking-for-different-pipeline-combinations","title":"How to cache heavy computations while looking for different pipeline combinations.\u00b6","text":""},{"location":"examples/notebooks/caching_heavy_data/#data-preparation","title":"Data preparation\u00b6","text":""},{"location":"examples/notebooks/caching_heavy_data/#first-we-need-to-transform-our-text-data-into-numerical-vectors-using-sentence-transformers","title":"First we need to transform our text data into numerical vectors using Sentence Transformers.\u00b6","text":""},{"location":"examples/notebooks/caching_heavy_data/#pipeline-preparation","title":"Pipeline preparation\u00b6","text":""},{"location":"examples/notebooks/caching_heavy_data/#caching-the-funny-part","title":"Caching: The funny part\u00b6","text":""},{"location":"examples/notebooks/caching_heavy_data/#lets-define-another-pipeline","title":"Let's define another pipeline\u00b6","text":""},{"location":"examples/notebooks/data_ingestion/","title":"Data Ingestion","text":"<p>In this tutorial, we'll learn how to use the Container.ds storage backend to store and retrieve data in Framework3.</p> In\u00a0[1]: Copied! <pre>from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n\npatch_inspect_for_notebooks()\n</pre> from framework3.utils.patch_type_guard import patch_inspect_for_notebooks  patch_inspect_for_notebooks() <pre>\u2705 Patched inspect.getsource using dill.\n</pre> In\u00a0[2]: Copied! <pre># Import necessary libraries\nfrom framework3.container import Container\n\nimport pandas as pd\nimport numpy as np\n</pre> # Import necessary libraries from framework3.container import Container  import pandas as pd import numpy as np In\u00a0[3]: Copied! <pre>from dotenv import load_dotenv\nfrom pathlib import Path\nimport os\n\nenv_path = Path(\"../../../.env\")\nload_dotenv(dotenv_path=env_path)\n</pre> from dotenv import load_dotenv from pathlib import Path import os  env_path = Path(\"../../../.env\") load_dotenv(dotenv_path=env_path) Out[3]: <pre>True</pre> In\u00a0[4]: Copied! <pre># Create sample data\ndf = pd.DataFrame(\n    {\n        \"A\": np.random.rand(100),\n        \"B\": np.random.randint(0, 100, 100),\n        \"C\": [\"cat\", \"dog\", \"bird\"] * 33 + [\"cat\"],\n    }\n)\n\n# Store the DataFrame\nContainer.ds.save(\"sample_data_local\", df)\nprint(\"Data stored successfully locally\")\n</pre> # Create sample data df = pd.DataFrame(     {         \"A\": np.random.rand(100),         \"B\": np.random.randint(0, 100, 100),         \"C\": [\"cat\", \"dog\", \"bird\"] * 33 + [\"cat\"],     } )  # Store the DataFrame Container.ds.save(\"sample_data_local\", df) print(\"Data stored successfully locally\") <pre>\t * Saving in local path: cache/datasets/sample_data_local\n\t * Saved !\nData stored successfully locally\n</pre> In\u00a0[5]: Copied! <pre>local_files = Container.ds.list()\nprint(\"Files in local storage:\", local_files)\n</pre> local_files = Container.ds.list() print(\"Files in local storage:\", local_files) <pre>Files in local storage: ['sample_data_s3', 'sample_data', 'sample_data_local']\n</pre> In\u00a0[6]: Copied! <pre>retrieved_df = Container.ds.load(\"sample_data_local\")\nprint(\"Data retrieved successfully from local storage\")\nprint(retrieved_df.value.head())\n</pre> retrieved_df = Container.ds.load(\"sample_data_local\") print(\"Data retrieved successfully from local storage\") print(retrieved_df.value.head()) <pre>Data retrieved successfully from local storage\n\t * Downloading: &lt;_io.BufferedReader name='cache/datasets/sample_data_local'&gt;\n          A   B     C\n0  0.273884  59   cat\n1  0.049904  84   dog\n2  0.872462  70  bird\n3  0.795624  34   cat\n4  0.763532  42   dog\n</pre> In\u00a0[7]: Copied! <pre># Update the DataFrame\ndf[\"D\"] = np.random.choice([\"X\", \"Y\", \"Z\"], 100)\n\n# Store the updated DataFrame locally\nContainer.ds.update(\"sample_data_local\", df)\nprint(\"Updated data stored successfully locally\")\n\n# Retrieve and display the updated DataFrame\nupdated_df = Container.ds.load(\"sample_data_local\")\nprint(updated_df.value.head())\n</pre> # Update the DataFrame df[\"D\"] = np.random.choice([\"X\", \"Y\", \"Z\"], 100)  # Store the updated DataFrame locally Container.ds.update(\"sample_data_local\", df) print(\"Updated data stored successfully locally\")  # Retrieve and display the updated DataFrame updated_df = Container.ds.load(\"sample_data_local\") print(updated_df.value.head()) <pre>\t * Saving in local path: cache/datasets/sample_data_local\n\t * Saved !\nUpdated data stored successfully locally\n\t * Downloading: &lt;_io.BufferedReader name='cache/datasets/sample_data_local'&gt;\n          A   B     C  D\n0  0.273884  59   cat  Y\n1  0.049904  84   dog  Y\n2  0.872462  70  bird  Z\n3  0.795624  34   cat  X\n4  0.763532  42   dog  Z\n</pre> In\u00a0[8]: Copied! <pre># Delete the stored data from local storage\nContainer.ds.delete(\"sample_data_local\")\nprint(\"Data deleted successfully from local storage\")\n</pre> # Delete the stored data from local storage Container.ds.delete(\"sample_data_local\") print(\"Data deleted successfully from local storage\") <pre>Data deleted successfully from local storage\n</pre> In\u00a0[9]: Copied! <pre>from framework3.plugins.storage import S3Storage\n\ns3_storage = S3Storage(\n    bucket=os.environ.get(\"TEST_BUCKET_NAME\"),  # type: ignore\n    region_name=os.environ.get(\"REGION_NAME\"),  # type: ignore\n    access_key=os.environ.get(\"TEST_ACCESS_KEY\"),  # type: ignore\n    access_key_id=os.environ.get(\"TEST_ACCESS_KEY_ID\"),  # type: ignore\n    endpoint_url=os.environ.get(\"ENDPOINT_URL\"),\n)\n</pre> from framework3.plugins.storage import S3Storage  s3_storage = S3Storage(     bucket=os.environ.get(\"TEST_BUCKET_NAME\"),  # type: ignore     region_name=os.environ.get(\"REGION_NAME\"),  # type: ignore     access_key=os.environ.get(\"TEST_ACCESS_KEY\"),  # type: ignore     access_key_id=os.environ.get(\"TEST_ACCESS_KEY_ID\"),  # type: ignore     endpoint_url=os.environ.get(\"ENDPOINT_URL\"), ) In\u00a0[11]: Copied! <pre>Container.storage = s3_storage\n</pre> Container.storage = s3_storage In\u00a0[12]: Copied! <pre>Container.ds.save(\"sample_data_s3\", df)\nprint(\"Data stored successfully in S3\")\n</pre> Container.ds.save(\"sample_data_s3\", df) print(\"Data stored successfully in S3\") <pre>- Binary prepared!\n- Stream ready!\n \t * Object size 8e-08 GBs \nUpload Complete!\nData stored successfully in S3\n</pre> In\u00a0[13]: Copied! <pre>s3_files = Container.ds.list()\nprint(\"Files in S3 bucket:\", s3_files)\n</pre> s3_files = Container.ds.list() print(\"Files in S3 bucket:\", s3_files) <pre>Files in S3 bucket: ['datasets/Iris X data.pkl', 'test-bucket/datasets/sample_data_s3']\n</pre> In\u00a0[14]: Copied! <pre>retrieved_df = Container.ds.load(\"sample_data_s3\")\nprint(\"Data retrieved successfully from S3\")\nprint(retrieved_df.value.head())\n</pre> retrieved_df = Container.ds.load(\"sample_data_s3\") print(\"Data retrieved successfully from S3\") print(retrieved_df.value.head()) <pre>Data retrieved successfully from S3\n          A   B     C  D\n0  0.273884  59   cat  Y\n1  0.049904  84   dog  Y\n2  0.872462  70  bird  Z\n3  0.795624  34   cat  X\n4  0.763532  42   dog  Z\n</pre> In\u00a0[15]: Copied! <pre># Update the DataFrame\ndf[\"E\"] = np.random.choice([\"P\", \"Q\", \"R\"], 100)\n\n# Store the updated DataFrame in S3\nContainer.ds.update(\"sample_data_s3\", df)\nprint(\"Updated data stored successfully in S3\")\n\n# Retrieve and display the updated DataFrame\nupdated_df = Container.ds.load(\"sample_data_s3\")\nprint(updated_df.value.head())\n</pre> # Update the DataFrame df[\"E\"] = np.random.choice([\"P\", \"Q\", \"R\"], 100)  # Store the updated DataFrame in S3 Container.ds.update(\"sample_data_s3\", df) print(\"Updated data stored successfully in S3\")  # Retrieve and display the updated DataFrame updated_df = Container.ds.load(\"sample_data_s3\") print(updated_df.value.head()) <pre>- Binary prepared!\n- Stream ready!\n \t * Object size 8e-08 GBs \nUpload Complete!\nUpdated data stored successfully in S3\n          A   B     C  D  E\n0  0.273884  59   cat  Y  R\n1  0.049904  84   dog  Y  P\n2  0.872462  70  bird  Z  P\n3  0.795624  34   cat  X  P\n4  0.763532  42   dog  Z  P\n</pre> In\u00a0[16]: Copied! <pre># Delete the stored data from S3\nContainer.ds.delete(\"sample_data_s3\")\nprint(\"Data deleted successfully from S3\")\n</pre> # Delete the stored data from S3 Container.ds.delete(\"sample_data_s3\") print(\"Data deleted successfully from S3\") <pre>Deleted!\nData deleted successfully from S3\n</pre>"},{"location":"examples/notebooks/data_ingestion/#data-ingestion","title":"Data Ingestion\u00b6","text":""},{"location":"examples/notebooks/data_ingestion/#store-and-retrieve-data-using-the-containerds-storage-backend","title":"Store and retrieve data using the Container.ds storage backend\u00b6","text":""},{"location":"examples/notebooks/data_ingestion/#1-storing-data","title":"1. Storing Data\u00b6","text":""},{"location":"examples/notebooks/data_ingestion/#2-listing-data","title":"2. Listing Data\u00b6","text":""},{"location":"examples/notebooks/data_ingestion/#3-retrieving-data","title":"3. Retrieving Data\u00b6","text":""},{"location":"examples/notebooks/data_ingestion/#4-updating-stored-data","title":"4. Updating Stored Data\u00b6","text":""},{"location":"examples/notebooks/data_ingestion/#5-deleting-stored-data","title":"5. Deleting Stored Data\u00b6","text":""},{"location":"examples/notebooks/f3pipeline_with_cache/","title":"F3Pipeline con Cache","text":"In\u00a0[1]: Copied! <pre>from framework3 import F3Pipeline\nfrom framework3.plugins.filters import StandardScalerPlugin\nfrom framework3.plugins.filters import ClassifierSVMPlugin, KnnFilter\nfrom framework3.plugins.metrics import F1\nfrom framework3.base import XYData\nfrom framework3.plugins.filters import Cached\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nfrom rich import print\n</pre> from framework3 import F3Pipeline from framework3.plugins.filters import StandardScalerPlugin from framework3.plugins.filters import ClassifierSVMPlugin, KnnFilter from framework3.plugins.metrics import F1 from framework3.base import XYData from framework3.plugins.filters import Cached from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split  from rich import print <pre>/home/manuel.couto.pintos/.cache/pypoetry/virtualenvs/framework3-fUhXhdhp-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Cargar el conjunto de datos Iris\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Dividir los datos en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Crear objetos XYData\nx_train = XYData(\"Iris dataset X train\", \"dataset\", X_train)\ny_train = XYData(\"Iris dataset Y train\", \"dataset\", y_train)\nx_test = XYData(\"Iris dataset X test\", \"dataset\", X_test)\ny_test = XYData(\"Iris dataset Y test\", \"dataset\", y_test)\n</pre> # Cargar el conjunto de datos Iris iris = load_iris() X, y = iris.data, iris.target  # Dividir los datos en conjuntos de entrenamiento y prueba X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=42 )  # Crear objetos XYData x_train = XYData(\"Iris dataset X train\", \"dataset\", X_train) y_train = XYData(\"Iris dataset Y train\", \"dataset\", y_train) x_test = XYData(\"Iris dataset X test\", \"dataset\", X_test) y_test = XYData(\"Iris dataset Y test\", \"dataset\", y_test) In\u00a0[3]: Copied! <pre># Crear el pipeline con Cache\npipeline = F3Pipeline(\n    filters=[\n        Cached(\n            filter=StandardScalerPlugin(),\n            cache_data=True,\n            cache_filter=True,\n            overwrite=False,\n        ),\n        ClassifierSVMPlugin(),\n    ],\n    metrics=[F1()],\n)\nprint(pipeline)\n</pre> # Crear el pipeline con Cache pipeline = F3Pipeline(     filters=[         Cached(             filter=StandardScalerPlugin(),             cache_data=True,             cache_filter=True,             overwrite=False,         ),         ClassifierSVMPlugin(),     ],     metrics=[F1()], ) print(pipeline) <pre>F3Pipeline(\n    filters=[\n        Cached(filter=StandardScalerPlugin(), cache_data=True, cache_filter=True, overwrite=False, storage=None),\n        ClassifierSVMPlugin(C=1.0, kernel='linear', gamma='scale')\n    ],\n    metrics=[F1(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n</pre> In\u00a0[4]: Copied! <pre># Entrenar el modelo\npipeline.fit(x_train, y_train)\n</pre> # Entrenar el modelo pipeline.fit(x_train, y_train) <pre>____________________________________________________________________________________________________\n</pre> <pre>Fitting pipeline...\n</pre> <pre>****************************************************************************************************\n</pre> <pre>\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False, \n'storage': None}):\n</pre> <pre>         - El filtro StandardScalerPlugin({}) con hash 4f0150e0e11419085ce0f08ab077b7e5891f817b No existe, se va a \nentrenar.\n</pre> <pre>         - El filtro StandardScalerPlugin({}) Se cachea.\n</pre> <pre>\t * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/model\n\t * Saved !\n</pre> <pre>         - El dato XYData(_hash='f77f9d95466939988cdd6a13f0cb91260b94c99d', \n_path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') No existe, se va a crear.\n</pre> <pre>         - El dato XYData(_hash='f77f9d95466939988cdd6a13f0cb91260b94c99d', \n_path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Se cachea.\n</pre> <pre>\t * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/f77f9d95466939988cdd6a13f0cb91260b94c99d\n\t * Saved !\n</pre> <pre>\n* ClassifierSVMPlugin({'C': 1.0, 'kernel': 'linear', 'gamma': 'scale'}):\n</pre> In\u00a0[5]: Copied! <pre># Realizar predicciones\npredictions = pipeline.predict(x_test)\n\n# Evaluar el modelo\nevaluation = pipeline.evaluate(x_test, y_test, predictions)\nprint(\"Resultados de la evaluaci\u00f3n:\", evaluation)\n</pre> # Realizar predicciones predictions = pipeline.predict(x_test)  # Evaluar el modelo evaluation = pipeline.evaluate(x_test, y_test, predictions) print(\"Resultados de la evaluaci\u00f3n:\", evaluation) <pre>____________________________________________________________________________________________________\n</pre> <pre>Predicting pipeline...\n</pre> <pre>****************************************************************************************************\n</pre> <pre>\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El dato XYData(_hash='0403ab1f29eb3d1d59f857aa03f7af153d7ff357', \n_path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') No existe, se va a crear.\n</pre> <pre>         - El dato XYData(_hash='0403ab1f29eb3d1d59f857aa03f7af153d7ff357', \n_path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Se cachea.\n</pre> <pre>\t * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/0403ab1f29eb3d1d59f857aa03f7af153d7ff357\n\t * Saved !\n</pre> <pre>\n* ClassifierSVMPlugin({'C': 1.0, 'kernel': 'linear', 'gamma': 'scale'})\n</pre> <pre>____________________________________________________________________________________________________\n</pre> <pre>Evaluating pipeline...\n</pre> <pre>____________________________________________________________________________________________________\n</pre> <pre>Resultados de la evaluaci\u00f3n:\n{'F1': 0.9664109121909632}\n</pre> In\u00a0[6]: Copied! <pre># Ejecutar el pipeline nuevamente\nprint(\"Segunda ejecuci\u00f3n (deber\u00eda usar datos en cach\u00e9):\")\n# Crear el pipeline con Cache\npipeline = F3Pipeline(\n    filters=[\n        Cached(\n            filter=StandardScalerPlugin(),\n            cache_data=True,\n            cache_filter=True,\n            overwrite=False,\n        ),\n        KnnFilter(),\n    ],\n    metrics=[F1()],\n)\nprint(pipeline)\npipeline.fit(x_train, y_train)\npredictions = pipeline.predict(x_test)\nevaluation = pipeline.evaluate(x_test, y_test, predictions)\nprint(\"Resultados de la evaluaci\u00f3n:\", evaluation)\n</pre> # Ejecutar el pipeline nuevamente print(\"Segunda ejecuci\u00f3n (deber\u00eda usar datos en cach\u00e9):\") # Crear el pipeline con Cache pipeline = F3Pipeline(     filters=[         Cached(             filter=StandardScalerPlugin(),             cache_data=True,             cache_filter=True,             overwrite=False,         ),         KnnFilter(),     ],     metrics=[F1()], ) print(pipeline) pipeline.fit(x_train, y_train) predictions = pipeline.predict(x_test) evaluation = pipeline.evaluate(x_test, y_test, predictions) print(\"Resultados de la evaluaci\u00f3n:\", evaluation) <pre>Segunda ejecuci\u00f3n (deber\u00eda usar datos en cach\u00e9):\n</pre> <pre>F3Pipeline(\n    filters=[\n        Cached(filter=StandardScalerPlugin(), cache_data=True, cache_filter=True, overwrite=False, storage=None),\n        KnnFilter(\n            n_neighbors=5,\n            weights='uniform',\n            algorithm='auto',\n            leaf_size=30,\n            p=2,\n            metric='minkowski',\n            metric_params=None,\n            n_jobs=None\n        )\n    ],\n    metrics=[F1(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n</pre> <pre>____________________________________________________________________________________________________\n</pre> <pre>Fitting pipeline...\n</pre> <pre>****************************************************************************************************\n</pre> <pre>\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False, \n'storage': None}):\n</pre> <pre>         - El filtro StandardScalerPlugin({}) Existe, se crea lambda.\n</pre> <pre>         - El dato XYData(_hash='f77f9d95466939988cdd6a13f0cb91260b94c99d', \n_path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Existe, se crea lambda.\n</pre> <pre>\n* KnnFilter({'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'metric': \n'minkowski', 'metric_params': None, 'n_jobs': None}):\n</pre> <pre>\t * Downloading: &lt;_io.BufferedReader name='cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/f77f9d95466939988cdd6a13f0cb91260b94c99d'&gt;\n</pre> <pre>____________________________________________________________________________________________________\n</pre> <pre>Predicting pipeline...\n</pre> <pre>****************************************************************************************************\n</pre> <pre>\n* Cached({'filter': StandardScalerPlugin({}), 'cache_data': True, 'cache_filter': True, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El dato XYData(_hash='a2131ee7caeb76fd704b11b77d0456223b7e0437', \n_path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') No existe, se va a crear.\n</pre> <pre>         - Existe un Lambda por lo que se recupera el filtro del storage.\n</pre> <pre>\t * Downloading: &lt;_io.BufferedReader name='cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/model'&gt;\n</pre> <pre>         - El dato XYData(_hash='a2131ee7caeb76fd704b11b77d0456223b7e0437', \n_path='StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b') Se cachea.\n</pre> <pre>\t * Saving in local path: cache/StandardScalerPlugin/4f0150e0e11419085ce0f08ab077b7e5891f817b/a2131ee7caeb76fd704b11b77d0456223b7e0437\n\t * Saved !\n</pre> <pre>\n* KnnFilter({'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'metric': \n'minkowski', 'metric_params': None, 'n_jobs': None})\n</pre> <pre>____________________________________________________________________________________________________\n</pre> <pre>Evaluating pipeline...\n</pre> <pre>____________________________________________________________________________________________________\n</pre> <pre>Resultados de la evaluaci\u00f3n:\n{'F1': 1.0}\n</pre> <p>En esta segunda ejecuci\u00f3n, deber\u00edas notar que el StandardScalerPlugin usa los datos en cach\u00e9, lo que puede resultar en un tiempo de ejecuci\u00f3n m\u00e1s r\u00e1pido.</p>"},{"location":"examples/notebooks/f3pipeline_with_cache/#f3pipeline-con-cache","title":"F3Pipeline con Cache\u00b6","text":"<p>Este tutorial te guiar\u00e1 a trav\u00e9s de un ejemplo sencillo de uso de F3Pipeline, incluyendo la funcionalidad de Cache.</p>"},{"location":"examples/notebooks/f3pipeline_with_cache/#1-importaciones-necesarias","title":"1. Importaciones necesarias\u00b6","text":""},{"location":"examples/notebooks/f3pipeline_with_cache/#2-preparacion-de-los-datos","title":"2. Preparaci\u00f3n de los datos\u00b6","text":""},{"location":"examples/notebooks/f3pipeline_with_cache/#3-creacion-del-pipeline-con-cache","title":"3. Creaci\u00f3n del pipeline con Cache\u00b6","text":""},{"location":"examples/notebooks/f3pipeline_with_cache/#5-entrenamiento-del-modelo","title":"5. Entrenamiento del modelo\u00b6","text":""},{"location":"examples/notebooks/f3pipeline_with_cache/#6-prediccion-y-evaluacion","title":"6. Predicci\u00f3n y evaluaci\u00f3n\u00b6","text":""},{"location":"examples/notebooks/f3pipeline_with_cache/#7-demostracion-del-uso-de-cache","title":"7. Demostraci\u00f3n del uso de Cache\u00b6","text":""},{"location":"examples/notebooks/grid_search_optimizer_with_data_splitter_kfold/","title":"Grid Optimizer","text":"<p>If you have seen the tutorial Reuse Data you mai noticed that we've use a standard sklean optimizer for hyperparameter tuning. This is fine for many uses cases, but it might not be the best choice for somo others.</p> In\u00a0[1]: Copied! <pre>from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n\npatch_inspect_for_notebooks()\n</pre> from framework3.utils.patch_type_guard import patch_inspect_for_notebooks  patch_inspect_for_notebooks() <pre>\u2705 Patched inspect.getsource using dill.\n</pre> In\u00a0[2]: Copied! <pre>from sklearn import datasets\nfrom framework3.base.base_clases import XYData\n\niris = datasets.load_iris()\n\n\nX_train, X_test, y_train, y_test = XYData(\n    _hash=\"Iris \", _path=\"/dataset\", _value=[]\n).train_test_split(\n    iris.data,\n    iris.target,\n    test_size=0.2,\n    random_state=42,  # type: ignore\n)\n</pre> from sklearn import datasets from framework3.base.base_clases import XYData  iris = datasets.load_iris()   X_train, X_test, y_train, y_test = XYData(     _hash=\"Iris \", _path=\"/dataset\", _value=[] ).train_test_split(     iris.data,     iris.target,     test_size=0.2,     random_state=42,  # type: ignore ) In\u00a0[3]: Copied! <pre>from framework3 import (\n    F1,\n    Cached,\n    F3Pipeline,\n    KnnFilter,\n    Precission,\n    StandardScalerPlugin,\n)\nfrom framework3.plugins.metrics.classification import Recall, XYData\nfrom framework3.plugins.optimizer.grid_optimizer import GridOptimizer\nfrom framework3.plugins.splitter.cross_validation_splitter import KFoldSplitter\n\n\nwandb_pipeline = (\n    F3Pipeline(\n        filters=[\n            Cached(StandardScalerPlugin()),\n            KnnFilter().grid({\"n_neighbors\": [2, 3, 4, 5, 6]}),\n        ],\n        metrics=[F1(), Precission(), Recall()],\n    )\n    .splitter(\n        KFoldSplitter(\n            n_splits=2,\n            shuffle=True,\n            random_state=42,\n        )\n    )\n    .optimizer(GridOptimizer(scorer=F1()))\n)\n</pre> from framework3 import (     F1,     Cached,     F3Pipeline,     KnnFilter,     Precission,     StandardScalerPlugin, ) from framework3.plugins.metrics.classification import Recall, XYData from framework3.plugins.optimizer.grid_optimizer import GridOptimizer from framework3.plugins.splitter.cross_validation_splitter import KFoldSplitter   wandb_pipeline = (     F3Pipeline(         filters=[             Cached(StandardScalerPlugin()),             KnnFilter().grid({\"n_neighbors\": [2, 3, 4, 5, 6]}),         ],         metrics=[F1(), Precission(), Recall()],     )     .splitter(         KFoldSplitter(             n_splits=2,             shuffle=True,             random_state=42,         )     )     .optimizer(GridOptimizer(scorer=F1())) ) In\u00a0[4]: Copied! <pre>wandb_pipeline.fit(X_train, y_train)\n_y = wandb_pipeline.predict(x=X_test)\n</pre> wandb_pipeline.fit(X_train, y_train) _y = wandb_pipeline.predict(x=X_test) <pre>{'KnnFilter': {'n_neighbors': [2, 3, 4, 5, 6]}}\n</pre> <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>Cached(filter=StandardScalerPlugin(), cache_data=True, cache_filter=True, overwrite=False, storage=None)\n</pre> <pre>         - El filtro StandardScalerPlugin({}) Existe, se carga del storage.\n</pre> <pre>         - El dato XYData(_hash='8cf971f3f80c552a61c115451c6de5ed26ac6c9d', \n_path='StandardScalerPlugin/0f98887c2bd6020b824a410979d85cbf1d8ebfd4') Existe, se carga del storage.\n</pre> <pre>KnnFilter(\n    n_neighbors=4,\n    weights='uniform',\n    algorithm='auto',\n    leaf_size=30,\n    p=2,\n    metric='minkowski',\n    metric_params=None,\n    n_jobs=None\n)\n</pre> <pre>____________________________________________________________________________________________________\nPredicting with KFold Splitter......\n****************************************************************************************************\n</pre> <pre>F3Pipeline(\n    filters=[\n        Cached(filter=StandardScalerPlugin(), cache_data=True, cache_filter=True, overwrite=False, storage=None),\n        KnnFilter(\n            n_neighbors=4,\n            weights='uniform',\n            algorithm='auto',\n            leaf_size=30,\n            p=2,\n            metric='minkowski',\n            metric_params=None,\n            n_jobs=None\n        )\n    ],\n    metrics=[F1(average='weighted'), Precission(average='weighted'), Recall(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>Cached(filter=StandardScalerPlugin(), cache_data=True, cache_filter=True, overwrite=False, storage=None)\n</pre> <pre>         - El dato XYData(_hash='b4ff2a642069bfc672713ea400d29c66ecf21d93', \n_path='StandardScalerPlugin/0f98887c2bd6020b824a410979d85cbf1d8ebfd4') Existe, se carga del storage.\n</pre> <pre>KnnFilter(\n    n_neighbors=4,\n    weights='uniform',\n    algorithm='auto',\n    leaf_size=30,\n    p=2,\n    metric='minkowski',\n    metric_params=None,\n    n_jobs=None\n)\n</pre> In\u00a0[5]: Copied! <pre>y_test.value\n</pre> y_test.value Out[5]: <pre>array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n       0, 2, 2, 2, 2, 2, 0, 0])</pre> In\u00a0[6]: Copied! <pre>_y.value\n</pre> _y.value Out[6]: <pre>array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n       0, 2, 2, 2, 2, 2, 0, 0])</pre> In\u00a0[7]: Copied! <pre>wandb_pipeline.evaluate(X_test, y_test, _y)\n</pre> wandb_pipeline.evaluate(X_test, y_test, _y) <pre>____________________________________________________________________________________________________\nEvaluating pipeline......\n****************************************************************************************************\n</pre> Out[7]: <pre>{'F1': 1.0, 'Precission': 1.0, 'Recall': 1.0}</pre> In\u00a0[8]: Copied! <pre>wandb_pipeline._results\n</pre> wandb_pipeline._results Out[8]: KnnFilter score 2 {'n_neighbors': 4} 0.933723 4 {'n_neighbors': 6} 0.932844 1 {'n_neighbors': 3} 0.925411 3 {'n_neighbors': 5} 0.916946 0 {'n_neighbors': 2} 0.908650"},{"location":"examples/notebooks/grid_search_optimizer_with_data_splitter_kfold/#grid-optimizer","title":"Grid Optimizer\u00b6","text":""},{"location":"examples/notebooks/grid_search_optimizer_with_data_splitter_kfold/#how-to-perform-cross-validation-and-hiperparameter-optimization-with-grid-search","title":"How to perform cross validation and hiperparameter optimization with Grid search\u00b6","text":""},{"location":"examples/notebooks/grid_search_optimizer_with_data_splitter_kfold/#we-will-use-a-simple-pipeline-for-the-iris-dataset","title":"We will use a simple pipeline for the iris dataset.\u00b6","text":""},{"location":"examples/notebooks/grid_search_optimizer_with_data_splitter_kfold/#then-we-will-configure-grid-search-for-hyperparameter-tuning-and-a-sklearn-splitter-for-cross-validation","title":"Then we will configure Grid Search for hyperparameter tuning and a Sklearn splitter for cross validation.\u00b6","text":""},{"location":"examples/notebooks/grid_search_optimizer_with_data_splitter_kfold/#grid-results","title":"grid results\u00b6","text":""},{"location":"examples/notebooks/optuna_optimizer_with_data_splitter_kfold/","title":"Optuna Optimizer","text":"<p>If you have seen the tutorial Reuse Data you mai noticed that we've use a standard sklean optimizer for hyperparameter tuning. This is fine for many uses cases, but it might not be the best choice for somo others. For those how need a more advanced optimization strategy, Optuna is a great choice.</p> In\u00a0[1]: Copied! <pre>from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n\npatch_inspect_for_notebooks()\n</pre> from framework3.utils.patch_type_guard import patch_inspect_for_notebooks  patch_inspect_for_notebooks() <pre>\u2705 Patched inspect.getsource using dill.\n</pre> In\u00a0[2]: Copied! <pre>from sklearn.datasets import fetch_20newsgroups\n\n# Cargar el conjunto de datos 20 Newsgroups\ntrain = fetch_20newsgroups(subset=\"train\")\ntest = fetch_20newsgroups(subset=\"test\")\n</pre> from sklearn.datasets import fetch_20newsgroups  # Cargar el conjunto de datos 20 Newsgroups train = fetch_20newsgroups(subset=\"train\") test = fetch_20newsgroups(subset=\"test\") In\u00a0[3]: Copied! <pre>from framework3.base import XYData\n\n\nX_train = XYData(\n    _hash=\"20NG train X\",\n    _path=\"/datasets\",\n    _value=train.data,  # type: ignore\n)\nX_test = XYData(\n    _hash=\"20NG test X\",\n    _path=\"/datasets\",\n    _value=test.data,  # type: ignore\n)\ny_train = XYData(\n    _hash=\"20NG train y\",\n    _path=\"/datasets\",\n    _value=train.target,  # type: ignore\n)\ny_test = XYData(\n    _hash=\"20NG test y\",\n    _path=\"/datasets\",\n    _value=test.target,  # type: ignore\n)\n</pre> from framework3.base import XYData   X_train = XYData(     _hash=\"20NG train X\",     _path=\"/datasets\",     _value=train.data,  # type: ignore ) X_test = XYData(     _hash=\"20NG test X\",     _path=\"/datasets\",     _value=test.data,  # type: ignore ) y_train = XYData(     _hash=\"20NG train y\",     _path=\"/datasets\",     _value=train.target,  # type: ignore ) y_test = XYData(     _hash=\"20NG test y\",     _path=\"/datasets\",     _value=test.target,  # type: ignore ) In\u00a0[4]: Copied! <pre>from framework3.plugins.filters.llm import HuggingFaceSentenceTransformerPlugin\n\nllm = HuggingFaceSentenceTransformerPlugin(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n)\n</pre> from framework3.plugins.filters.llm import HuggingFaceSentenceTransformerPlugin  llm = HuggingFaceSentenceTransformerPlugin(     model_name=\"sentence-transformers/all-mpnet-base-v2\" ) <p>Optuna provides a dashboard to visualize the results of the experiments. For this to work, you need to define an sqlite storage path. Also, each study meed a name and the optimizer need a direction for the optimization.</p> In\u00a0[8]: Copied! <pre>from framework3 import F1, ClassifierSVMPlugin, Precission, Recall\nfrom framework3.plugins.optimizer.optuna_optimizer import OptunaOptimizer\nfrom framework3.plugins.filters import PCAPlugin\nfrom framework3.plugins.pipelines.sequential import F3Pipeline\nfrom framework3.plugins.splitter import KFoldSplitter\n\n\ngrid_pipelin = (\n    F3Pipeline(\n        filters=[\n            PCAPlugin().grid({\"n_components\": [10, 100]}),\n            ClassifierSVMPlugin(kernel=\"rbf\").grid(\n                {\"C\": [0.1, 10.0], \"gamma\": (0.001, 0.0001)}\n            ),\n        ],\n        metrics=[F1(), Precission(), Recall()],\n    )\n    .splitter(KFoldSplitter(n_splits=2, shuffle=True))\n    .optimizer(\n        OptunaOptimizer(\n            n_trials=10,\n            direction=\"maximize\",\n            study_name=\"20 NG\",\n            load_if_exists=True,\n            storage=\"sqlite:///optuna_estudios.db\",\n        )\n    )\n)\n</pre> from framework3 import F1, ClassifierSVMPlugin, Precission, Recall from framework3.plugins.optimizer.optuna_optimizer import OptunaOptimizer from framework3.plugins.filters import PCAPlugin from framework3.plugins.pipelines.sequential import F3Pipeline from framework3.plugins.splitter import KFoldSplitter   grid_pipelin = (     F3Pipeline(         filters=[             PCAPlugin().grid({\"n_components\": [10, 100]}),             ClassifierSVMPlugin(kernel=\"rbf\").grid(                 {\"C\": [0.1, 10.0], \"gamma\": (0.001, 0.0001)}             ),         ],         metrics=[F1(), Precission(), Recall()],     )     .splitter(KFoldSplitter(n_splits=2, shuffle=True))     .optimizer(         OptunaOptimizer(             n_trials=10,             direction=\"maximize\",             study_name=\"20 NG\",             load_if_exists=True,             storage=\"sqlite:///optuna_estudios.db\",         )     ) ) <pre>[I 2025-04-15 18:36:58,503] Using an existing study with name '20 NG' instead of creating a new one.\n</pre> <p>Now we can search for the best hyperparameters.</p> In\u00a0[9]: Copied! <pre>from framework3 import Cached, Precission, Recall\n\nfinal_pipeline = F3Pipeline(\n    filters=[Cached(llm, cache_data=True, cache_filter=False), grid_pipelin],\n    metrics=[F1(), Precission(), Recall()],\n)\n</pre> from framework3 import Cached, Precission, Recall  final_pipeline = F3Pipeline(     filters=[Cached(llm, cache_data=True, cache_filter=False), grid_pipelin],     metrics=[F1(), Precission(), Recall()], ) In\u00a0[10]: Copied! <pre>final_pipeline.fit(X_train, y_train)\n_y = final_pipeline.predict(X_test)\n</pre> final_pipeline.fit(X_train, y_train) _y = final_pipeline.predict(X_test) <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>        *Cached({'filter': HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El filtro HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}) con hash cd52a2089d77df27ae1a888d97422cd38e3bb01a No existe, se va a \nentrenar.\n</pre> <pre>         - El dato XYData(_hash='0bb5f0568e0be233f4dfc5bbe7e893a6369f353e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') No existe, se va a crear.\n</pre> <pre>         - El dato XYData(_hash='0bb5f0568e0be233f4dfc5bbe7e893a6369f353e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') Se cachea.\n</pre> <pre>\t * Saving in local path: cache/HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a/0bb5f0568e0be233f4dfc5bbe7e893a6369f353e\n\t * Saved !\n</pre> <pre>        *OptunaOptimizer({'direction': 'maximize', 'study_name': '20 NG', 'storage': \n'sqlite:///optuna_estudios.db', 'pipeline': KFoldSplitter({'n_splits': 2, 'shuffle': True, 'random_state': 42, \n'pipeline': F3Pipeline({'filters': [PCAPlugin({'n_components': 2}), ClassifierSVMPlugin({'C': 1.0, 'kernel': 'rbf',\n'gamma': 'scale'})], 'metrics': [F1({'average': 'weighted'}), Precission({'average': 'weighted'}), \nRecall({'average': 'weighted'})], 'overwrite': False, 'store': False, 'log': False})}), 'n_trials': 10, \n'load_if_exists': True, 'reset_study': False})\n</pre> <pre>____________________________________________________________________________________________________\nFitting with OptunaOptimizer......\n****************************************************************************************************\n</pre> <pre>KFoldSplitter(\n    n_splits=2,\n    shuffle=True,\n    random_state=42,\n    pipeline=F3Pipeline(\n        filters=[PCAPlugin(n_components=2), ClassifierSVMPlugin(C=1.0, kernel='rbf', gamma='scale')],\n        metrics=[F1(average='weighted'), Precission(average='weighted'), Recall(average='weighted')],\n        overwrite=False,\n        store=False,\n        log=False\n    )\n)\n</pre> <pre>{\n    'clazz': 'KFoldSplitter',\n    'params': {\n        'n_splits': 2,\n        'shuffle': True,\n        'random_state': 42,\n        'pipeline': {\n            'clazz': 'F3Pipeline',\n            'params': {\n                'filters': [\n                    {'clazz': 'PCAPlugin', 'params': {'n_components': 2}, '_grid': {'n_components': [10, 100]}},\n                    {\n                        'clazz': 'ClassifierSVMPlugin',\n                        'params': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'},\n                        '_grid': {'C': [0.1, 10.0], 'gamma': [0.001, 0.0001]}\n                    }\n                ],\n                'metrics': [\n                    {'clazz': 'F1', 'params': {'average': 'weighted'}},\n                    {'clazz': 'Precission', 'params': {'average': 'weighted'}},\n                    {'clazz': 'Recall', 'params': {'average': 'weighted'}}\n                ],\n                'overwrite': False,\n                'store': False,\n                'log': False\n            }\n        }\n    }\n}\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>[I 2025-04-15 18:38:15,478] Trial 0 finished with value: 0.004751068615760974 and parameters: {'n_components': 100, 'C': 10.0, 'gamma': 0.0001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:38:38,307] Trial 1 finished with value: 0.004751068615760974 and parameters: {'n_components': 100, 'C': 0.1, 'gamma': 0.001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:39:01,823] Trial 2 finished with value: 0.004751068615760974 and parameters: {'n_components': 100, 'C': 0.1, 'gamma': 0.0001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:39:23,418] Trial 3 finished with value: 0.004751068615760974 and parameters: {'n_components': 100, 'C': 0.1, 'gamma': 0.0001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:39:36,287] Trial 4 finished with value: 0.004751068615760974 and parameters: {'n_components': 10, 'C': 10.0, 'gamma': 0.0001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:39:48,866] Trial 5 finished with value: 0.004751068615760974 and parameters: {'n_components': 10, 'C': 0.1, 'gamma': 0.0001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:40:01,358] Trial 6 finished with value: 0.004751068615760974 and parameters: {'n_components': 10, 'C': 10.0, 'gamma': 0.0001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:40:24,237] Trial 7 finished with value: 0.004751068615760974 and parameters: {'n_components': 100, 'C': 0.1, 'gamma': 0.0001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:40:47,039] Trial 8 finished with value: 0.004751068615760974 and parameters: {'n_components': 100, 'C': 0.1, 'gamma': 0.001}. Best is trial 0 with value: 0.004751068615760974.\n[I 2025-04-15 18:40:59,514] Trial 9 finished with value: 0.3378964875790811 and parameters: {'n_components': 10, 'C': 10.0, 'gamma': 0.001}. Best is trial 9 with value: 0.3378964875790811.\n</pre> <pre>Best params: {'n_components': 10, 'C': 10.0, 'gamma': 0.001}\n</pre> <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>        *PCAPlugin({'n_components': 10})\n</pre> <pre>        *ClassifierSVMPlugin({'C': 10.0, 'kernel': 'rbf', 'gamma': 0.001})\n</pre> <pre>____________________________________________________________________________________________________\nPredicting with OptunaOptimizer......\n****************************************************************************************************\n</pre> <pre>F3Pipeline(\n    filters=[PCAPlugin(n_components=10), ClassifierSVMPlugin(C=10.0, kernel='rbf', gamma=0.001)],\n    metrics=[F1(average='weighted'), Precission(average='weighted'), Recall(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>        *PCAPlugin({'n_components': 10})\n</pre> <pre>        *ClassifierSVMPlugin({'C': 10.0, 'kernel': 'rbf', 'gamma': 0.001})\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>        *Cached({'filter': HuggingFaceSentenceTransformerPlugin({'model_name': \n'sentence-transformers/all-mpnet-base-v2'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, \n'storage': None})\n</pre> <pre>         - El dato XYData(_hash='68fc2995310bba822e143578b3f4ee9ddd9f212e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') No existe, se va a crear.\n</pre> <pre>         - El dato XYData(_hash='68fc2995310bba822e143578b3f4ee9ddd9f212e', \n_path='HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a') Se cachea.\n</pre> <pre>\t * Saving in local path: cache/HuggingFaceSentenceTransformerPlugin/cd52a2089d77df27ae1a888d97422cd38e3bb01a/68fc2995310bba822e143578b3f4ee9ddd9f212e\n\t * Saved !\n</pre> <pre>        *OptunaOptimizer({'direction': 'maximize', 'study_name': '20 NG', 'storage': \n'sqlite:///optuna_estudios.db', 'pipeline': F3Pipeline({'filters': [PCAPlugin({'n_components': 10}), \nClassifierSVMPlugin({'C': 10.0, 'kernel': 'rbf', 'gamma': 0.001})], 'metrics': [F1({'average': 'weighted'}), \nPrecission({'average': 'weighted'}), Recall({'average': 'weighted'})], 'overwrite': False, 'store': False, 'log': \nFalse}), 'n_trials': 10, 'load_if_exists': True, 'reset_study': False})\n</pre> <pre>____________________________________________________________________________________________________\nPredicting with OptunaOptimizer......\n****************************************************************************************************\n</pre> <pre>F3Pipeline(\n    filters=[PCAPlugin(n_components=10), ClassifierSVMPlugin(C=10.0, kernel='rbf', gamma=0.001)],\n    metrics=[F1(average='weighted'), Precission(average='weighted'), Recall(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>        *PCAPlugin({'n_components': 10})\n</pre> <pre>        *ClassifierSVMPlugin({'C': 10.0, 'kernel': 'rbf', 'gamma': 0.001})\n</pre> <p>Optuna need a significant amount of trials if the param gridd is large.</p> In\u00a0[11]: Copied! <pre>final_pipeline.evaluate(X_test, y_test, _y)\n</pre> final_pipeline.evaluate(X_test, y_test, _y) <pre>____________________________________________________________________________________________________\nEvaluating pipeline......\n****************************************************************************************************\n</pre> Out[11]: <pre>{'F1': 0.4684588862007859,\n 'Precission': 0.5393927363357506,\n 'Recall': 0.5354487519915029}</pre> <p>Another cool feature of Optuna is the dashboard. It allows us to visualize some interesting things about the optimization process.</p> In\u00a0[19]: Copied! <pre>from optuna.visualization import (\n    plot_optimization_history,\n    plot_parallel_coordinate,\n    plot_param_importances,\n)\nimport plotly.io as pio\nfrom IPython.display import Image, display\n\nfig = plot_optimization_history(grid_pipelin._study)\n# Convertir a PNG en memoria\nimg_bytes = pio.to_image(fig, format=\"png\", width=800, height=600)\n# Mostrar en el notebook\ndisplay(Image(data=img_bytes))\n</pre> from optuna.visualization import (     plot_optimization_history,     plot_parallel_coordinate,     plot_param_importances, ) import plotly.io as pio from IPython.display import Image, display  fig = plot_optimization_history(grid_pipelin._study) # Convertir a PNG en memoria img_bytes = pio.to_image(fig, format=\"png\", width=800, height=600) # Mostrar en el notebook display(Image(data=img_bytes)) In\u00a0[21]: Copied! <pre>display(Image(data=pio.to_image(plot_parallel_coordinate(grid_pipelin._study))))\n</pre> display(Image(data=pio.to_image(plot_parallel_coordinate(grid_pipelin._study)))) In\u00a0[22]: Copied! <pre>display(Image(data=pio.to_image(plot_param_importances(grid_pipelin._study))))\n</pre> display(Image(data=pio.to_image(plot_param_importances(grid_pipelin._study)))) <p>This is awesome right, this dashboard can be accessed in your browser at http://localhost:8080/</p>"},{"location":"examples/notebooks/optuna_optimizer_with_data_splitter_kfold/#optuna-optimizer","title":"Optuna Optimizer\u00b6","text":""},{"location":"examples/notebooks/optuna_optimizer_with_data_splitter_kfold/#how-to-perform-cross-validation-and-hiperparameter-optimization-with-optuna","title":"How to perform cross validation and hiperparameter optimization with Optuna.\u00b6","text":""},{"location":"examples/notebooks/optuna_optimizer_with_data_splitter_kfold/#we-will-use-the-same-pipeline-as-before","title":"We will use the same pipeline as before.\u00b6","text":""},{"location":"examples/notebooks/optuna_optimizer_with_data_splitter_kfold/#then-we-will-configure-optuna-for-hyperparameter-tuning-and-a-sklearn-splitter-for-cross-validation","title":"Then we will configure Optuna for hyperparameter tuning and a Sklearn splitter for cross validation.\u00b6","text":""},{"location":"examples/notebooks/optuna_optimizer_with_data_splitter_kfold/#optuna-dashboard","title":"Optuna dashboard\u00b6","text":""},{"location":"examples/notebooks/research_deltas/","title":"Temporal Word Embeddings for Early Detection of Psychological Disorders on Social Media","text":"In\u00a0[17]: Copied! <pre>from models.twec import TWEC\n</pre> from models.twec import TWEC In\u00a0[18]: Copied! <pre>from models.deltas import DISTANCES\n</pre> from models.deltas import DISTANCES In\u00a0[19]: Copied! <pre># !pip install framework3==1.0.15\n</pre> # !pip install framework3==1.0.15 In\u00a0[20]: Copied! <pre>from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n\npatch_inspect_for_notebooks()\n</pre> from framework3.utils.patch_type_guard import patch_inspect_for_notebooks  patch_inspect_for_notebooks() <pre>\u2705 Patched inspect.getsource using dill.\n</pre> In\u00a0[21]: Copied! <pre>from concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import List, Literal\nfrom scipy.sparse import dok_matrix\nfrom tqdm import tqdm\nfrom framework3.base.base_clases import BaseFilter\nfrom framework3.base.base_types import XYData\nfrom framework3.container import Container\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\n\n\n@Container.bind()\nclass TWECFilter(BaseFilter):\n    def __init__(\n        self,\n        context_size: int,\n        _cpus: int = 4,\n        deltas_f: List[\n            Literal[\n                \"cosine\",\n                \"euclidean\",\n                \"chebyshev\",\n                \"jensen_shannon\",\n                \"wasserstein\",\n                \"manhattan\",\n                \"minkowski\",\n            ]\n        ] = [\"cosine\"],\n    ):\n        super().__init__()\n        self._twec = TWEC(size=300, window=context_size)\n        self.deltas_f = deltas_f\n        self.context_size = context_size\n        actual_cpus = os.cpu_count()\n        if actual_cpus is not None:\n            self._cpus = min(actual_cpus, _cpus)\n        else:\n            self._cpus = _cpus\n\n    def fit(self, x: XYData, y: XYData | None) -&gt; float | None:\n        data: pd.DataFrame = x.value\n        self._twec.train_compass(data.text.values.tolist())\n        self._vocab_hash_map = dict(\n            zip(\n                self._twec.compass.wv.index_to_key,  # type: ignore\n                range(len(self._twec.compass.wv.index_to_key)),  # type: ignore\n            )\n        )\n\n    def predict(self, x: XYData) -&gt; XYData:\n        data: pd.DataFrame = x.value\n        n_rows = len(data.index)\n        n_cols = len(self._vocab_hash_map.items())\n        metric_names = self.deltas_f\n\n        all_deltas = {\n            metric: dok_matrix((n_rows, n_cols), dtype=np.float32)\n            for metric in metric_names\n        }\n\n        def process_user_deltas(i, tc):\n            result = {metric: [] for metric in metric_names}\n            for word in tc.wv.index_to_key:  # type: ignore\n                if word in self._vocab_hash_map:\n                    j = self._vocab_hash_map[word]\n                    for metric in metric_names:\n                        dist = (\n                            DISTANCES[metric](\n                                torch.tensor(np.array([[self._twec.compass.wv[word]]])),  # type: ignore\n                                torch.tensor(np.array([[tc.wv[word]]])),  # type: ignore\n                            )\n                            .detach()\n                            .cpu()\n                            .item()\n                        )\n                        result[metric].append((i, j, dist))\n            return result\n\n        with ThreadPoolExecutor(max_workers=self._cpus) as executor:\n            futures = {\n                executor.submit(\n                    process_user_deltas, i, self._twec.train_slice(row.text)\n                ): i\n                for i, row in tqdm(\n                    enumerate(data.itertuples()),\n                    total=n_rows,\n                    desc=\"generating embeddings\",\n                )\n            }\n\n            for future in tqdm(\n                as_completed(futures), total=n_rows, desc=\"parallel prediction\"\n            ):\n                chunk_result = future.result()\n                for metric, values in chunk_result.items():\n                    for i, j, val in values:\n                        all_deltas[metric][i, j] = val\n\n        return XYData.mock(all_deltas)\n</pre> from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Literal from scipy.sparse import dok_matrix from tqdm import tqdm from framework3.base.base_clases import BaseFilter from framework3.base.base_types import XYData from framework3.container import Container  import pandas as pd import numpy as np import torch import os   @Container.bind() class TWECFilter(BaseFilter):     def __init__(         self,         context_size: int,         _cpus: int = 4,         deltas_f: List[             Literal[                 \"cosine\",                 \"euclidean\",                 \"chebyshev\",                 \"jensen_shannon\",                 \"wasserstein\",                 \"manhattan\",                 \"minkowski\",             ]         ] = [\"cosine\"],     ):         super().__init__()         self._twec = TWEC(size=300, window=context_size)         self.deltas_f = deltas_f         self.context_size = context_size         actual_cpus = os.cpu_count()         if actual_cpus is not None:             self._cpus = min(actual_cpus, _cpus)         else:             self._cpus = _cpus      def fit(self, x: XYData, y: XYData | None) -&gt; float | None:         data: pd.DataFrame = x.value         self._twec.train_compass(data.text.values.tolist())         self._vocab_hash_map = dict(             zip(                 self._twec.compass.wv.index_to_key,  # type: ignore                 range(len(self._twec.compass.wv.index_to_key)),  # type: ignore             )         )      def predict(self, x: XYData) -&gt; XYData:         data: pd.DataFrame = x.value         n_rows = len(data.index)         n_cols = len(self._vocab_hash_map.items())         metric_names = self.deltas_f          all_deltas = {             metric: dok_matrix((n_rows, n_cols), dtype=np.float32)             for metric in metric_names         }          def process_user_deltas(i, tc):             result = {metric: [] for metric in metric_names}             for word in tc.wv.index_to_key:  # type: ignore                 if word in self._vocab_hash_map:                     j = self._vocab_hash_map[word]                     for metric in metric_names:                         dist = (                             DISTANCES[metric](                                 torch.tensor(np.array([[self._twec.compass.wv[word]]])),  # type: ignore                                 torch.tensor(np.array([[tc.wv[word]]])),  # type: ignore                             )                             .detach()                             .cpu()                             .item()                         )                         result[metric].append((i, j, dist))             return result          with ThreadPoolExecutor(max_workers=self._cpus) as executor:             futures = {                 executor.submit(                     process_user_deltas, i, self._twec.train_slice(row.text)                 ): i                 for i, row in tqdm(                     enumerate(data.itertuples()),                     total=n_rows,                     desc=\"generating embeddings\",                 )             }              for future in tqdm(                 as_completed(futures), total=n_rows, desc=\"parallel prediction\"             ):                 chunk_result = future.result()                 for metric, values in chunk_result.items():                     for i, j, val in values:                         all_deltas[metric][i, j] = val          return XYData.mock(all_deltas) <p>This work addresses an early prediction task using the eRisk dataset, which requires the use of classification models. We will now define a set of classifiers and integrate them within the framework by wrapping them in the appropriate classes.</p> <p>\u26a0\ufe0f Warning: In order for classes to be parallelizable, they must be defined in a standalone module. For this reason, we have moved the classifiers to separate files. The code shown here is provided for reference purposes only.</p> <p>\u26a0\ufe0f Warning: Also note that some hyperparameters are not primitive types. While this works well with <code>sklearn</code> and <code>Optuna</code> optimizers, it may break when using the <code>wandb</code> optimizer. The code should be adapted accordingly if you plan to use <code>wandb</code> for optimization.</p> <pre>from typing import Callable, Mapping\nfrom sklearn.svm import SVC\n\n\n@Container.bind()\nclass ClassifierSVM(BaseFilter):\n    def __init__(\n        self,\n        C: float = 1,\n        kernel: Callable | Literal['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'] = \"rbf\",\n        gamma: float | Literal['scale', 'auto'] = \"scale\",\n        coef0:float=0.0,\n        tol:float=0.001,\n        decision_function_shape:Literal['ovo', 'ovr'] = \"ovr\",\n        class_weight_1: Mapping[Any, Any] | str | None = None,\n        probability:bool = False,\n    ):\n        super().__init__()\n        self.proba = probability\n        self._model = SVC(\n            C=C,\n            kernel=kernel,\n            gamma=gamma,\n            coef0=coef0,\n            tol=tol,\n            decision_function_shape=decision_function_shape,\n            class_weight={1: class_weight_1},\n            probability=probability,\n            random_state=43,\n        )\n\n    def fit(self, x: XYData, y: XYData | None):\n        if y is None:\n            raise ValueError(\"y must be provided for training\")\n        self._model.fit(x.value, y.value)\n\n    def predict(self, x: XYData) -&gt; XYData:\n        if self.proba:\n            result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))\n            return XYData.mock(result)\n        else:\n            result = self._model.predict(x.value)\n            return XYData.mock(result)\n</pre> In\u00a0[22]: Copied! <pre>from models.svm import ClassifierSVM\n</pre> from models.svm import ClassifierSVM In\u00a0[23]: Copied! <pre>from framework3.plugins.metrics import F1, Precission, Recall\n\nf1 = F1()\nprecision = Precission()\nrecall = Recall()\n</pre> from framework3.plugins.metrics import F1, Precission, Recall  f1 = F1() precision = Precission() recall = Recall() <pre>from typing import Iterable\nfrom sklearn.metrics import confusion_matrix\nfrom framework3 import BaseMetric\n\nfrom numpy import exp\n\n@Container.bind()\nclass ERDE(BaseMetric):\n    def __init__(self, count: Iterable, k: int = 5):\n        self.k = k\n        self.count = count\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; float | np.ndarray:\n        if y_true is None:\n            raise ValueError(\"y_true must be provided for evaluation\")\n\n        all_erde = []\n        _, _, _, tp = confusion_matrix(y_true.value, y_pred.value).ravel()\n        for expected, result, count in list(\n            zip(y_true.value, y_pred.value, self.count)\n        ):\n            if result == 1 and expected == 0:\n                all_erde.append(float(tp) / len(y_true.value))\n            elif result == 0 and expected == 1:\n                all_erde.append(1.0)\n            elif result == 1 and expected == 1:\n                all_erde.append(1.0 - (1.0 / (1.0 + exp(count - self.k))))\n            elif result == 0 and expected == 0:\n                all_erde.append(0.0)\n        return float(np.mean(all_erde) * 100)\n</pre> In\u00a0[24]: Copied! <pre>from metrics.erde import ERDE_5, ERDE_50\n</pre> from metrics.erde import ERDE_5, ERDE_50 In\u00a0[25]: Copied! <pre>gambling_2023_train = pd.read_csv(\"data/standard_gambling_train_2023.csv\", index_col=0)\ngambling_2023_train.head(5)\n\ngambling_2023_test = pd.read_csv(\"data/standard_gambling_2023.csv\", index_col=0)\ngambling_2023_test.head(5)\n</pre> gambling_2023_train = pd.read_csv(\"data/standard_gambling_train_2023.csv\", index_col=0) gambling_2023_train.head(5)  gambling_2023_test = pd.read_csv(\"data/standard_gambling_2023.csv\", index_col=0) gambling_2023_test.head(5) Out[25]: id text date chunk label user 0 subject5539_0 For PC: I don't know what company, but they ne... 2015-12-03 13:31:29 0 0 subject5539 1 subject5539_1 You play as a Pokmon trainer (that you customi... 2015-12-04 19:45:40 0 0 subject5539 2 subject5539_2 A Clash of Clans RPG (or MMORPG) 2015-12-23 23:32:51 0 0 subject5539 3 subject5539_3 You would have to manage your species's needs ... 2015-12-26 20:45:30 0 0 subject5539 4 subject5539_4 The game starts you as a child and you have to... 2016-01-02 08:40:18 0 0 subject5539 In\u00a0[26]: Copied! <pre>gg_2023_train = (\n    gambling_2023_train.groupby([\"user\", \"chunk\"])\n    .agg(\n        {\n            \"id\": \"count\",\n            \"text\": list,\n            \"date\": list,\n            \"label\": \"first\",\n        }\n    )\n    .rename(columns={\"id\": \"n_texts\"})\n    .reset_index()\n)\n\ngg_2023_test = (\n    gambling_2023_test.groupby([\"user\", \"chunk\"])\n    .agg(\n        {\n            \"id\": \"count\",\n            \"text\": list,\n            \"date\": list,\n            \"label\": \"first\",\n        }\n    )\n    .rename(columns={\"id\": \"n_texts\"})\n    .reset_index()\n)\ngg_2023_train\n</pre> gg_2023_train = (     gambling_2023_train.groupby([\"user\", \"chunk\"])     .agg(         {             \"id\": \"count\",             \"text\": list,             \"date\": list,             \"label\": \"first\",         }     )     .rename(columns={\"id\": \"n_texts\"})     .reset_index() )  gg_2023_test = (     gambling_2023_test.groupby([\"user\", \"chunk\"])     .agg(         {             \"id\": \"count\",             \"text\": list,             \"date\": list,             \"label\": \"first\",         }     )     .rename(columns={\"id\": \"n_texts\"})     .reset_index() ) gg_2023_train Out[26]: user chunk n_texts text date label 0 subject1 0 132 [Vulcan's ultimate landing at max range is so ... [2017-08-18 11:34:09, 2017-08-20 15:26:34, 201... 0 1 subject1 1 132 [Awesome! It is always good to hear these news... [2018-05-18 23:46:33, 2018-06-18 17:17:55, 201... 0 2 subject1 2 132 [The syringe is a lie!, I'd say Scylla or Than... [2018-09-20 08:20:44, 2018-09-24 10:12:03, 201... 0 3 subject1 3 131 [Some of the symptoms you may experience are b... [2019-05-06 17:50:52, 2019-05-06 19:05:44, 201... 0 4 subject1 4 132 [So ur saying that huge map is better than Afg... [2019-10-06 23:22:46, 2019-10-12 18:08:06, 201... 0 ... ... ... ... ... ... ... 39265 subject9999 5 8 [10v10, is those a bunch of bots, I didn't eve... [2021-07-04 06:51:04, 2021-07-04 07:01:05, 202... 0 39266 subject9999 6 8 [I'm commenting this based on the fact that Am... [2021-07-15 19:11:42, 2021-07-27 17:44:09, 202... 0 39267 subject9999 7 8 [Aesthetic set, It's a fucking downgrade,, It'... [2021-09-12 14:55:05, 2021-09-23 00:31:11, 202... 0 39268 subject9999 8 8 [u/save, u/savevideo, Snu snu ! Snu snu! Snu s... [2021-09-23 13:38:48, 2021-10-08 13:44:30, 202... 0 39269 subject9999 9 8 [Why every fucking time there's a new weapon o... [2021-11-27 06:45:16, 2021-11-27 07:04:36, 202... 0 <p>39270 rows \u00d7 6 columns</p> <p>\u26a0\ufe0f Warning: There are several restrictions for the plugins to work properly:</p> <ul> <li>Constructor arguments should be public attributes.</li> <li>Other data must be set as private attributes.</li> <li>All public attributes must be serializable using <code>jsonable_encoder</code>.</li> </ul> In\u00a0[27]: Copied! <pre>test_erde_5 = ERDE_5(gg_2023_test.n_texts.values.tolist())\ntest_erde_50 = ERDE_50(gg_2023_test.n_texts.values.tolist())\n</pre> test_erde_5 = ERDE_5(gg_2023_test.n_texts.values.tolist()) test_erde_50 = ERDE_50(gg_2023_test.n_texts.values.tolist()) In\u00a0[28]: Copied! <pre>@Container.bind()\nclass DeltaSelectorFilter(BaseFilter):\n    def __init__(\n        self,\n        deltas_f: Literal[\n            \"cosine\",\n            \"euclidean\",\n            \"chebyshev\",\n            \"jensen_shannon\",\n            \"wasserstein\",\n            \"manhattan\",\n            \"minkowski\",\n        ] = \"cosine\",\n    ):\n        self.deltas_f = deltas_f\n\n    def fit(self, x: XYData, y: XYData | None):\n        pass\n\n    def predict(self, x: XYData) -&gt; XYData:\n        # Crear una nueva dok_matrix con el mismo shape y en float32\n        old_dok = x.value[self.deltas_f]\n        new_dok = dok_matrix(old_dok.shape, dtype=np.float32)\n\n        # Copiar todos los valores existentes y convertir el dtype\n        for (i, j), value in old_dok.items():\n            new_dok[i, j] = float(value)  # conversi\u00f3n a float32 impl\u00edcita\n        return XYData.mock(new_dok.tocsr())\n</pre> @Container.bind() class DeltaSelectorFilter(BaseFilter):     def __init__(         self,         deltas_f: Literal[             \"cosine\",             \"euclidean\",             \"chebyshev\",             \"jensen_shannon\",             \"wasserstein\",             \"manhattan\",             \"minkowski\",         ] = \"cosine\",     ):         self.deltas_f = deltas_f      def fit(self, x: XYData, y: XYData | None):         pass      def predict(self, x: XYData) -&gt; XYData:         # Crear una nueva dok_matrix con el mismo shape y en float32         old_dok = x.value[self.deltas_f]         new_dok = dok_matrix(old_dok.shape, dtype=np.float32)          # Copiar todos los valores existentes y convertir el dtype         for (i, j), value in old_dok.items():             new_dok[i, j] = float(value)  # conversi\u00f3n a float32 impl\u00edcita         return XYData.mock(new_dok.tocsr()) In\u00a0[29]: Copied! <pre>from framework3 import Cached, SklearnOptimizer\nfrom framework3.plugins.pipelines.sequential import F3Pipeline\n\nall_test_metrics = [\n    f1,\n    precision,\n    recall,\n    test_erde_5,\n    test_erde_50,\n]\n\npipeline_svm = F3Pipeline(\n    filters=[\n        Cached(\n            filter=TWECFilter(\n                context_size=25,\n                _cpus=10,\n                deltas_f=[\"cosine\", \"euclidean\", \"manhattan\", \"chebyshev\"],\n            ),\n        ),\n        DeltaSelectorFilter(deltas_f=\"cosine\"),\n        F3Pipeline(\n            filters=[\n                ClassifierSVM(\n                    tol=0.003,\n                    probability=False,\n                    decision_function_shape=\"ovr\",\n                    kernel=\"rbf\",\n                    gamma=\"scale\",\n                ).grid(\n                    {\n                        \"C\": [1, 3, 5],\n                        \"class_weight_1\": [{1: 1.5}, {1: 2.5}, {1: 3.0}],\n                    }\n                )\n            ],\n            metrics=[F1()],\n        ).optimizer(SklearnOptimizer(scoring=\"f1_weighted\", cv=2, n_jobs=-1)),\n    ],\n    metrics=all_test_metrics,\n)\n</pre> from framework3 import Cached, SklearnOptimizer from framework3.plugins.pipelines.sequential import F3Pipeline  all_test_metrics = [     f1,     precision,     recall,     test_erde_5,     test_erde_50, ]  pipeline_svm = F3Pipeline(     filters=[         Cached(             filter=TWECFilter(                 context_size=25,                 _cpus=10,                 deltas_f=[\"cosine\", \"euclidean\", \"manhattan\", \"chebyshev\"],             ),         ),         DeltaSelectorFilter(deltas_f=\"cosine\"),         F3Pipeline(             filters=[                 ClassifierSVM(                     tol=0.003,                     probability=False,                     decision_function_shape=\"ovr\",                     kernel=\"rbf\",                     gamma=\"scale\",                 ).grid(                     {                         \"C\": [1, 3, 5],                         \"class_weight_1\": [{1: 1.5}, {1: 2.5}, {1: 3.0}],                     }                 )             ],             metrics=[F1()],         ).optimizer(SklearnOptimizer(scoring=\"f1_weighted\", cv=2, n_jobs=-1)),     ],     metrics=all_test_metrics, ) <pre>/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:134: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.TWECFilter.__init__\n  cls.__init__ = typechecked(init_method)  # type: ignore[method-assign]\n/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.TWECFilter.fit\n  setattr(cls, attr_name, typechecked(attr_value))\n/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.TWECFilter.predict\n  setattr(cls, attr_name, typechecked(attr_value))\n/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:134: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.DeltaSelectorFilter.__init__\n  cls.__init__ = typechecked(init_method)  # type: ignore[method-assign]\n/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.DeltaSelectorFilter.fit\n  setattr(cls, attr_name, typechecked(attr_value))\n/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.DeltaSelectorFilter.predict\n  setattr(cls, attr_name, typechecked(attr_value))\n</pre> In\u00a0[30]: Copied! <pre>train_x = XYData(_hash=\" Gambling_2023_train_x\", _path=\"/dataset\", _value=gg_2023_train)\ntrain_y = XYData(\n    _hash=\"Gambling_2023_train_y\", _path=\"/dataset\", _value=gg_2023_train.label.tolist()\n)\n\ntest_x = XYData(_hash=\"Gambling_2023_test_x\", _path=\"/dataset\", _value=gg_2023_test)\n\ntest_y = XYData(\n    _hash=\"Gambling_2023_test_y\", _path=\"/dataset\", _value=gg_2023_test.label.tolist()\n)\n</pre> train_x = XYData(_hash=\" Gambling_2023_train_x\", _path=\"/dataset\", _value=gg_2023_train) train_y = XYData(     _hash=\"Gambling_2023_train_y\", _path=\"/dataset\", _value=gg_2023_train.label.tolist() )  test_x = XYData(_hash=\"Gambling_2023_test_x\", _path=\"/dataset\", _value=gg_2023_test)  test_y = XYData(     _hash=\"Gambling_2023_test_y\", _path=\"/dataset\", _value=gg_2023_test.label.tolist() ) <p>\u26a0\ufe0f Warning: Please note that for parallel backend usage, a considerable amount of RAM will be required.</p> In\u00a0[31]: Copied! <pre>from joblib import parallel_backend\nimport sys\n\nwith parallel_backend(\"loky\", n_jobs=-1):\n    print(\"Starting GridSearchCV fitting...\", flush=True)\n    pipeline_svm.fit(train_x, train_y)\n    sys.stdout.flush()\n</pre> from joblib import parallel_backend import sys  with parallel_backend(\"loky\", n_jobs=-1):     print(\"Starting GridSearchCV fitting...\", flush=True)     pipeline_svm.fit(train_x, train_y)     sys.stdout.flush() <pre>Starting GridSearchCV fitting...\n</pre> <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>Cached(\n    filter=TWECFilter(deltas_f=['cosine', 'euclidean', 'manhattan', 'chebyshev'], context_size=25),\n    cache_data=True,\n    cache_filter=True,\n    overwrite=False,\n    storage=None\n)\n</pre> <pre>         - El filtro TWECFilter({'deltas_f': ['cosine', 'euclidean', 'manhattan', 'chebyshev'], 'context_size': \n25}) Existe, se carga del storage.\n</pre> <pre>         - El dato XYData(_hash='f991d8f14f3bbdb0a54b565de7e60e42cfd36dc9', \n_path='TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914') Existe, se carga del storage.\n</pre> <pre>DeltaSelectorFilter(deltas_f='cosine')\n</pre> <pre>\t * Downloading: &lt;_io.BufferedReader name='cache/TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914/f991d8f14f3bbdb0a54b565de7e60e42cfd36dc9'&gt;\n</pre> <pre>SklearnOptimizer(\n    scoring='f1_weighted',\n    cv=2,\n    pipeline=F3Pipeline(\n        filters=[ClassifierSVM(proba=False)],\n        metrics=[F1(average='weighted')],\n        overwrite=False,\n        store=False,\n        log=False\n    ),\n    n_jobs=-1\n)\n</pre> <pre>Fitting 2 folds for each of 9 candidates, totalling 18 fits\n[CV 1/2; 9/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0}..\n[CV 1/2; 8/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5}..\n[CV 1/2; 2/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5}..\n[CV 2/2; 1/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5}..\n[CV 1/2; 4/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5}..\n[CV 2/2; 6/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0}..\n[CV 2/2; 8/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5}..\n[CV 2/2; 7/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5}..\n[CV 2/2; 5/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5}..\n[CV 2/2; 3/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0}..\n[CV 1/2; 7/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5}..\n[CV 2/2; 2/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5}..\n[CV 2/2; 9/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0}..\n[CV 1/2; 6/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0}..\n[CV 1/2; 3/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0}..\n[CV 1/2; 1/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5}..\n[CV 1/2; 5/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5}..\n[CV 2/2; 4/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5}..\n[CV 1/2; 1/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5};, score=0.964 total time=15.5min\n[CV 1/2; 9/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0};, score=0.965 total time=18.8min\n[CV 1/2; 2/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5};, score=0.964 total time=19.0min\n[CV 2/2; 9/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0};, score=0.966 total time=19.2min\n[CV 2/2; 1/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5};, score=0.963 total time=19.3min\n[CV 1/2; 6/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0};, score=0.965 total time=19.4min\n[CV 1/2; 8/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5};, score=0.965 total time=19.5min\n[CV 2/2; 7/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5};, score=0.965 total time=19.6min\n[CV 2/2; 8/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5};, score=0.965 total time=19.7min\n[CV 1/2; 5/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5};, score=0.965 total time=19.8min\n[CV 1/2; 3/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0};, score=0.964 total time=19.9min\n[CV 1/2; 7/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5};, score=0.965 total time=20.2min\n[CV 1/2; 4/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5};, score=0.965 total time=20.4min\n[CV 2/2; 3/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0};, score=0.965 total time=20.4min\n[CV 2/2; 4/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5};, score=0.964 total time=20.5min\n[CV 2/2; 5/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5};, score=0.966 total time=20.7min\n[CV 2/2; 6/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0};, score=0.966 total time=21.1min\n[CV 2/2; 2/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5};, score=0.964 total time=21.1min\n</pre> <pre>   param_ClassifierSVM__C param_ClassifierSVM__class_weight_1  \\\n5                       3                            {1: 3.0}   \n4                       3                            {1: 2.5}   \n8                       5                            {1: 3.0}   \n7                       5                            {1: 2.5}   \n6                       5                            {1: 1.5}   \n2                       1                            {1: 3.0}   \n3                       3                            {1: 1.5}   \n1                       1                            {1: 2.5}   \n0                       1                            {1: 1.5}   \n\n                                              params  split0_test_score  \\\n5  {'ClassifierSVM__C': 3, 'ClassifierSVM__class_...           0.965313   \n4  {'ClassifierSVM__C': 3, 'ClassifierSVM__class_...           0.965364   \n8  {'ClassifierSVM__C': 5, 'ClassifierSVM__class_...           0.965114   \n7  {'ClassifierSVM__C': 5, 'ClassifierSVM__class_...           0.965356   \n6  {'ClassifierSVM__C': 5, 'ClassifierSVM__class_...           0.965201   \n2  {'ClassifierSVM__C': 1, 'ClassifierSVM__class_...           0.964449   \n3  {'ClassifierSVM__C': 3, 'ClassifierSVM__class_...           0.964589   \n1  {'ClassifierSVM__C': 1, 'ClassifierSVM__class_...           0.964195   \n0  {'ClassifierSVM__C': 1, 'ClassifierSVM__class_...           0.963823   \n\n   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n5           0.965726         0.965519        0.000206                1  \n4           0.965614         0.965489        0.000125                2  \n8           0.965626         0.965370        0.000256                3  \n7           0.965307         0.965332        0.000024                4  \n6           0.965129         0.965165        0.000036                5  \n2           0.965158         0.964803        0.000354                6  \n3           0.964434         0.964512        0.000078                7  \n1           0.964178         0.964186        0.000009                8  \n0           0.963234         0.963529        0.000295                9  \n</pre> In\u00a0[32]: Copied! <pre>_y = pipeline_svm.predict(test_x)\n</pre> _y = pipeline_svm.predict(test_x) <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>Cached(\n    filter=TWECFilter(deltas_f=['cosine', 'euclidean', 'manhattan', 'chebyshev'], context_size=25),\n    cache_data=True,\n    cache_filter=True,\n    overwrite=False,\n    storage=None\n)\n</pre> <pre>         - El dato XYData(_hash='ed20b892e7858a253df46cdd3d19ef040844625d', \n_path='TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914') Existe, se carga del storage.\n</pre> <pre>DeltaSelectorFilter(deltas_f='cosine')\n</pre> <pre>\t * Downloading: &lt;_io.BufferedReader name='cache/TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914/ed20b892e7858a253df46cdd3d19ef040844625d'&gt;\n</pre> <pre>SklearnOptimizer(\n    scoring='f1_weighted',\n    cv=2,\n    pipeline=F3Pipeline(\n        filters=[ClassifierSVM(proba=False)],\n        metrics=[F1(average='weighted')],\n        overwrite=False,\n        store=False,\n        log=False\n    ),\n    n_jobs=-1\n)\n</pre> <p>After training the model on the training set using cross-validation, we evaluate its performance on the test set. This comparison is somewhat biased, as it involves predicting the label of individual chunks while evaluating against labels that were propagated from user-level annotations to their corresponding chunks.</p> In\u00a0[33]: Copied! <pre>pipeline_svm.evaluate(test_x, test_y, _y)\n</pre> pipeline_svm.evaluate(test_x, test_y, _y) <pre>____________________________________________________________________________________________________\nEvaluating pipeline......\n****************************************************************************************************\n</pre> Out[33]: <pre>{'F1': 0.9881395187504476,\n 'Precission': 0.9880309912771197,\n 'Recall': 0.9883182081482912,\n 'ERDE_5': 3.0647720557294345,\n 'ERDE_50': 0.959133296979966}</pre> <p>If we perform a fairer evaluation by propagating the predictions to the user level\u2014assigning a user as positive if at least one of their chunks is predicted as positive\u2014we observe that the performance remains similar or even improves, which indicates that the system is working as intended.</p> In\u00a0[34]: Copied! <pre>gg_2023_test[\"_y\"] = _y.value\ngg_2023_test.head(5)\n</pre> gg_2023_test[\"_y\"] = _y.value gg_2023_test.head(5) Out[34]: user chunk n_texts text date label _y 0 subject1 0 64 [Dope, No retcons or changes. The way it was, ... [2020-08-03 21:33:23, 2020-08-12 16:41:30, 202... 0 0 1 subject1 1 64 [Where did you get this?, I have no idea how t... [2020-10-04 22:34:08, 2020-10-04 22:38:55, 202... 0 0 2 subject1 2 64 [A little something im working on, Tried to do... [2021-02-10 21:13:06, 2021-02-17 20:34:45, 202... 0 0 3 subject1 3 64 [Oh the episodes after the characters stories ... [2021-04-08 22:55:11, 2021-04-08 23:48:15, 202... 0 0 4 subject1 4 64 [They need to drop easter eggs or hints like t... [2021-04-25 23:19:00, 2021-04-28 23:10:38, 202... 0 0 In\u00a0[35]: Copied! <pre>aux = gg_2023_test.groupby([\"user\"]).agg(\n    {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}\n)\naux\n</pre> aux = gg_2023_test.groupby([\"user\"]).agg(     {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0} ) aux Out[35]: label _y user subject1 0 0 subject10 0 0 subject10000 0 0 subject1001 0 0 subject1005 0 0 ... ... ... subject9982 0 0 subject9984 0 0 subject999 0 0 subject9990 0 0 subject9999 0 0 <p>2079 rows \u00d7 2 columns</p> In\u00a0[37]: Copied! <pre>aux.groupby([\"label\"]).describe()\n</pre> aux.groupby([\"label\"]).describe() Out[37]: _y count mean std min 25% 50% 75% max label 0 1998.0 0.009009 0.094511 0.0 0.0 0.0 0.0 1.0 1 81.0 0.975309 0.156150 0.0 1.0 1.0 1.0 1.0 In\u00a0[36]: Copied! <pre>{\n    \"F1\": F1().evaluate(\n        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n    ),\n    \"Precission\": Precission().evaluate(\n        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n    ),\n    \"Recall\": Recall().evaluate(\n        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n    ),\n    \"ERDE_5\": test_erde_5.evaluate(\n        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n    ),\n    \"ERDE_50\": test_erde_50.evaluate(\n        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n    ),\n}\n</pre> {     \"F1\": F1().evaluate(         test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())     ),     \"Precission\": Precission().evaluate(         test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())     ),     \"Recall\": Recall().evaluate(         test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())     ),     \"ERDE_5\": test_erde_5.evaluate(         test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())     ),     \"ERDE_50\": test_erde_50.evaluate(         test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())     ), } Out[36]: <pre>{'F1': 0.9907930136601583,\n 'Precission': 0.9918003496186526,\n 'Recall': 0.9903799903799904,\n 'ERDE_5': 3.3773273408026343,\n 'ERDE_50': 1.8609453827254776}</pre>"},{"location":"examples/notebooks/research_deltas/#temporal-word-embeddings-for-early-detection-of-psychological-disorders-on-social-media","title":"Temporal Word Embeddings for Early Detection of Psychological Disorders on Social Media\u00b6","text":""},{"location":"examples/notebooks/research_deltas/#how-to-early-detect-psychological-disorders-on-social-media-using-temporal-word-embeddings","title":"How to early detect psychological disorders on social media using temporal word embeddings\u00b6","text":""},{"location":"examples/notebooks/research_deltas/#abastract","title":"Abastract\u00b6","text":"<p>Mental health disorders represent a public health challenge, where early detection is critical to mitigating adverse outcomes for individuals and society. The study of language and behavior is a pivotal component in mental health research, and the content from social media platforms serves as a valuable tool for identifying signs of mental health risks. This paper presents a novel framework leveraging temporal word embeddings to capture linguistic changes over time. We specifically aim at at identifying emerging psychological concerns on social media. By adapting temporal word representations, our approach quantifies shifts in language use that may signal mental health risks. To that end, we implement two alternative temporal word embedding models to detect linguistic variations and exploit these variations to train early detection classifiers. Our experiments, conducted on 18 datasets from the eRisk initiative (covering signs of conditions such as depression, anorexia, and self-harm), show that simple models focusing exclusively on temporal word usage patterns achieve competitive performance compared to state-of-the-art systems. Additionally, we perform a word-level analysis to understand the evolution of key terms among positive and control users. These findings underscore the potential of time-sensitive word models in this domain, being a promising avenue for future research in mental health surveillance.</p>"},{"location":"examples/notebooks/research_deltas/#models","title":"Models\u00b6","text":""},{"location":"examples/notebooks/research_deltas/#twec","title":"TWEC\u00b6","text":"<p>In this tutorial, we will focus exclusively on TWEC. Let's begin by defining our temporal word embedding models. The first model, TWEC (Temporal Word Embeddings with a Compass), is an extension of Word2Vec that incorporates temporal information. It captures linguistic shifts over time by leveraging the context of surrounding words across different time periods.</p>"},{"location":"examples/notebooks/research_deltas/#deltas","title":"Deltas\u00b6","text":"<p>Deltas is a metric designed to quantify semantic drift in word meaning over time within a diachronic corpus. It is computed by applying similarity measures\u2014such as cosine similarity or Euclidean distance\u2014between temporally contextualized word embeddings and their corresponding static representations.</p>"},{"location":"examples/notebooks/research_deltas/#filters","title":"Filters\u00b6","text":"<p>Now that we have defined our models, we need to encapsulate them inside a class that implements the <code>BaseFilter</code> interface and binds them to the container.</p>"},{"location":"examples/notebooks/research_deltas/#the-classifiers","title":"The classifiers\u00b6","text":""},{"location":"examples/notebooks/research_deltas/#svm","title":"SVM\u00b6","text":""},{"location":"examples/notebooks/research_deltas/#the-metrics","title":"The Metrics\u00b6","text":"<p>Since the early prediction task is essentially a classification problem, we will use standard classification metrics such as F1-score, Precision, and Recall. However, due to the early nature of the task, we also need to include metrics that penalize delayed decisions, as timing is a critical aspect of the evaluation.</p>"},{"location":"examples/notebooks/research_deltas/#for-simplicity-we-will-only-consider-the-2023-gambling-data","title":"For simplicity, we will only consider the 2023 gambling data.\u00b6","text":""},{"location":"examples/notebooks/research_deltas/#selector","title":"Selector\u00b6","text":"<p>We are using <code>sklearn</code> for grid search. This optimizer will check the input dimensions of the <code>X</code> and <code>y</code> values. We have generated a dictionary with the deltas based on different distance measures, but this results in an incompatible dimensions error from <code>sklearn</code>. To work around this issue, we define a class that selects the appropriate deltas based on a hyperparameter.</p>"},{"location":"examples/notebooks/research_deltas/#the-pipeline","title":"The pipeline\u00b6","text":"<p>Now comes the most exciting part: integrating the filters into the pipeline. This step can be done incrementally, which is more convenient when developing a model. However, since we already have a clear understanding of the process, we will combine all the parts into one step.</p>"},{"location":"examples/notebooks/research_deltas/#data-preparation","title":"Data Preparation\u00b6","text":"<p>In F3, all data must be wrapped in the <code>XYData</code> class. This ensures that each data transformation is hashed and the results are cached.</p>"},{"location":"examples/notebooks/research_deltas/#model-training","title":"Model training\u00b6","text":""},{"location":"examples/notebooks/research_deltas/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"examples/notebooks/simple_custom_filter/","title":"Basic Pipeline","text":"In\u00a0[1]: Copied! <pre>from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n\npatch_inspect_for_notebooks()\n</pre> from framework3.utils.patch_type_guard import patch_inspect_for_notebooks  patch_inspect_for_notebooks() <pre>\u2705 Patched inspect.getsource using dill.\n</pre> <p>First we import and prepare the data. We meed to use XYData class to track the data transformations in the pipeline. This let hash each data versi\u00f3n and provide cach\u00e9 captabilities.</p> In\u00a0[2]: Copied! <pre>from framework3.base import XYData\nfrom sklearn import datasets\n\n# Load iris dataset and convert it to XYData format\niris = datasets.load_iris()\n\nX = XYData(\n    _hash=\"Iris X data\",\n    _path=\"/datasets\",\n    _value=iris.data,  # type: ignore\n)\ny = XYData(\n    _hash=\"Iris y data\",\n    _path=\"/datasets\",\n    _value=iris.target,  # type: ignore\n)\n</pre> from framework3.base import XYData from sklearn import datasets  # Load iris dataset and convert it to XYData format iris = datasets.load_iris()  X = XYData(     _hash=\"Iris X data\",     _path=\"/datasets\",     _value=iris.data,  # type: ignore ) y = XYData(     _hash=\"Iris y data\",     _path=\"/datasets\",     _value=iris.target,  # type: ignore ) <p>Now we can define our custom filter class.</p> In\u00a0[3]: Copied! <pre>from typing import Optional\n\nfrom sklearn.linear_model import LogisticRegression\nfrom framework3.base import BaseFilter, XYData\nfrom framework3 import Container\n\n\n@Container.bind()\nclass CustomLogisticRegresion(BaseFilter):\n    def __init__(self, threshold: float = 0.5):\n        super().__init__()\n        self.threshold = threshold\n        # Non configuration attributes should be private\n        self._model = LogisticRegression()\n\n    def fit(self, x: XYData, y: Optional[XYData]) -&gt; None:\n        X = x.value\n\n        if y is not None:\n            self._model.fit(X, y.value)\n        else:\n            raise ValueError(\"y must be provided for training\")\n\n    def predict(self, x: XYData) -&gt; XYData:\n        X = x.value\n\n        probabilities = self._model.predict_proba(X)[:, 1]\n\n        predictions = (probabilities &gt; self.threshold).astype(int)\n\n        # We have to wrap the output class with a Mock XYData object\n        # The framework will update the attributs with the new hash data.\n        return XYData.mock(predictions)\n</pre> from typing import Optional  from sklearn.linear_model import LogisticRegression from framework3.base import BaseFilter, XYData from framework3 import Container   @Container.bind() class CustomLogisticRegresion(BaseFilter):     def __init__(self, threshold: float = 0.5):         super().__init__()         self.threshold = threshold         # Non configuration attributes should be private         self._model = LogisticRegression()      def fit(self, x: XYData, y: Optional[XYData]) -&gt; None:         X = x.value          if y is not None:             self._model.fit(X, y.value)         else:             raise ValueError(\"y must be provided for training\")      def predict(self, x: XYData) -&gt; XYData:         X = x.value          probabilities = self._model.predict_proba(X)[:, 1]          predictions = (probabilities &gt; self.threshold).astype(int)          # We have to wrap the output class with a Mock XYData object         # The framework will update the attributs with the new hash data.         return XYData.mock(predictions) <p>Now we want to use this filter in our pipeline. We also will add a PCA filter and we will set several metrics: F1, Precision, and Recall.</p> In\u00a0[4]: Copied! <pre>from framework3 import F1, F3Pipeline, Precission, Recall\nfrom framework3.plugins.filters import PCAPlugin\n\npipeline = F3Pipeline(\n    filters=[PCAPlugin(n_components=2), CustomLogisticRegresion()],\n    metrics=[F1(), Precission(), Recall()],\n)\n</pre> from framework3 import F1, F3Pipeline, Precission, Recall from framework3.plugins.filters import PCAPlugin  pipeline = F3Pipeline(     filters=[PCAPlugin(n_components=2), CustomLogisticRegresion()],     metrics=[F1(), Precission(), Recall()], ) <pre>/home/manuel.couto.pintos/Documents/code/framework3/framework3/base/base_clases.py:56: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.CustomLogisticRegresion.__init__\n  cls.__init__ = typechecked(init_method)\n/home/manuel.couto.pintos/Documents/code/framework3/framework3/base/base_clases.py:64: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.CustomLogisticRegresion.fit\n  setattr(cls, attr_name, typechecked(attr_value))\n/home/manuel.couto.pintos/Documents/code/framework3/framework3/base/base_clases.py:64: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.CustomLogisticRegresion.predict\n  setattr(cls, attr_name, typechecked(attr_value))\n</pre> <p>Note that we have some warnings due to type hinting. These are due to a limitation of typecheker in jupyter notebooks. It'll will be fixed them in the next versions.</p> In\u00a0[5]: Copied! <pre>pipeline.fit(X, y)\n_y = pipeline.predict(X)\npipeline.evaluate(X, y, _y)\n</pre> pipeline.fit(X, y) _y = pipeline.predict(X) pipeline.evaluate(X, y, _y) <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>        *PCAPlugin({'n_components': 2})\n</pre> <pre>        *CustomLogisticRegresion({'threshold': 0.5})\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>        *PCAPlugin({'n_components': 2})\n</pre> <pre>        *CustomLogisticRegresion({'threshold': 0.5})\n</pre> <pre>____________________________________________________________________________________________________\nEvaluating pipeline......\n****************************************************************************************************\n</pre> Out[5]: <pre>{'F1': 0.5372488683746962,\n 'Precission': 0.4847443928066276,\n 'Recall': 0.6466666666666666}</pre> <p>We can apreciate that results are not the best posible with this basic example, but it gives you a starting point. To get better results, you should tune the hyperparameters of your models, preprocess your data, and add more features.</p>"},{"location":"examples/notebooks/simple_custom_filter/#basic-pipeline","title":"Basic Pipeline\u00b6","text":""},{"location":"examples/notebooks/simple_custom_filter/#how-to-create-custom-filters-for-f3pipeline","title":"How to create custom filters for F3Pipeline\u00b6","text":""},{"location":"examples/notebooks/wandb_optimizer_with_data_splitter_kfold/","title":"Wandb Optimizer","text":"<p>If you have seen the tutorial Reuse Data you mai noticed that we've use a standard sklean optimizer for hyperparameter tuning. This is fine for many uses cases, but it might not be the best choice for somo others. For those how need a more advanced optimization strategy, Wandb is a great choice.</p> In\u00a0[1]: Copied! <pre>import wandb\n\nwandb.login()\n</pre> import wandb  wandb.login() <pre>wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: manu-couto1k (citius-irlab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n</pre> Out[1]: <pre>True</pre> In\u00a0[2]: Copied! <pre>from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n\npatch_inspect_for_notebooks()\n</pre> from framework3.utils.patch_type_guard import patch_inspect_for_notebooks  patch_inspect_for_notebooks() <pre>\u2705 Patched inspect.getsource using dill.\n</pre> In\u00a0[3]: Copied! <pre>from sklearn import datasets\nfrom framework3.base.base_clases import XYData\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data,\n    iris.target,\n    test_size=0.2,\n    random_state=42,  # type: ignore\n)\n\n\nX_train = XYData(\n    _hash=\"Iris X train data\",\n    _path=\"/datasets\",\n    _value=X_train,\n)\ny_train = XYData(\n    _hash=\"Iris y train data\",\n    _path=\"/datasets\",\n    _value=y_train,  # type: ignore\n)\n\nX_test = XYData(\n    _hash=\"Iris X train data\",\n    _path=\"/datasets\",\n    _value=X_test,\n)\ny_test = XYData(\n    _hash=\"Iris y train data\",\n    _path=\"/datasets\",\n    _value=y_test,  # type: ignore\n)\n</pre> from sklearn import datasets from framework3.base.base_clases import XYData from sklearn.model_selection import train_test_split  iris = datasets.load_iris()   X_train, X_test, y_train, y_test = train_test_split(     iris.data,     iris.target,     test_size=0.2,     random_state=42,  # type: ignore )   X_train = XYData(     _hash=\"Iris X train data\",     _path=\"/datasets\",     _value=X_train, ) y_train = XYData(     _hash=\"Iris y train data\",     _path=\"/datasets\",     _value=y_train,  # type: ignore )  X_test = XYData(     _hash=\"Iris X train data\",     _path=\"/datasets\",     _value=X_test, ) y_test = XYData(     _hash=\"Iris y train data\",     _path=\"/datasets\",     _value=y_test,  # type: ignore ) <p>Wandb provides a dashboard to visualize the results of the experiments. For this to work, you need to define project name and login to the wandb services.</p> In\u00a0[4]: Copied! <pre>from framework3 import F1, F3Pipeline, KnnFilter, Precission, StandardScalerPlugin\nfrom framework3.plugins.metrics.classification import Recall, XYData\nfrom framework3.plugins.optimizer.wandb_optimizer import WandbOptimizer\nfrom framework3.plugins.splitter.cross_validation_splitter import KFoldSplitter\n\n\nwandb_pipeline = (\n    F3Pipeline(\n        filters=[\n            StandardScalerPlugin(),\n            KnnFilter().grid({\"n_neighbors\": [2, 3, 4, 5, 6]}),\n        ],\n        metrics=[F1(), Precission(), Recall()],\n    )\n    .splitter(\n        KFoldSplitter(\n            n_splits=2,\n            shuffle=True,\n            random_state=42,\n        )\n    )\n    .optimizer(\n        WandbOptimizer(\n            project=\"test_project\",\n            sweep_id=None,\n            scorer=F1(),\n        )\n    )\n)\n</pre> from framework3 import F1, F3Pipeline, KnnFilter, Precission, StandardScalerPlugin from framework3.plugins.metrics.classification import Recall, XYData from framework3.plugins.optimizer.wandb_optimizer import WandbOptimizer from framework3.plugins.splitter.cross_validation_splitter import KFoldSplitter   wandb_pipeline = (     F3Pipeline(         filters=[             StandardScalerPlugin(),             KnnFilter().grid({\"n_neighbors\": [2, 3, 4, 5, 6]}),         ],         metrics=[F1(), Precission(), Recall()],     )     .splitter(         KFoldSplitter(             n_splits=2,             shuffle=True,             random_state=42,         )     )     .optimizer(         WandbOptimizer(             project=\"test_project\",             sweep_id=None,             scorer=F1(),         )     ) ) In\u00a0[5]: Copied! <pre>wandb_pipeline.fit(X_train, y_train)\n_y = wandb_pipeline.predict(x=X_test)\n</pre> wandb_pipeline.fit(X_train, y_train) _y = wandb_pipeline.predict(x=X_test) <pre>categorical param: n_neighbors: [2, 3, 4, 5, 6]\n</pre> <pre>______________________SWEE CONFIG_____________________\n</pre> <pre>{\n    'parameters': {\n        'filters': {'parameters': {'KnnFilter': {'parameters': {'n_neighbors': {'values': [2, 3, 4, 5, 6]}}}}},\n        'pipeline': {\n            'value': {\n                'clazz': 'KFoldSplitter',\n                'params': {\n                    'n_splits': 2,\n                    'shuffle': True,\n                    'random_state': 42,\n                    'pipeline': {\n                        'clazz': 'F3Pipeline',\n                        'params': {\n                            'filters': [\n                                {'clazz': 'StandardScalerPlugin', 'params': {}},\n                                {\n                                    'clazz': 'KnnFilter',\n                                    'params': {\n                                        'n_neighbors': [2, 3, 4, 5, 6],\n                                        'weights': 'uniform',\n                                        'algorithm': 'auto',\n                                        'leaf_size': 30,\n                                        'p': 2,\n                                        'metric': 'minkowski',\n                                        'metric_params': None,\n                                        'n_jobs': None\n                                    },\n                                    '_grid': {'n_neighbors': [2, 3, 4, 5, 6]}\n                                }\n                            ],\n                            'metrics': [\n                                {'clazz': 'F1', 'params': {'average': 'weighted'}},\n                                {'clazz': 'Precission', 'params': {'average': 'weighted'}},\n                                {'clazz': 'Recall', 'params': {'average': 'weighted'}}\n                            ],\n                            'overwrite': False,\n                            'store': False,\n                            'log': False\n                        }\n                    }\n                }\n            }\n        },\n        'x_dataset': {'value': 'Iris X train data'},\n        'y_dataset': {'value': 'Iris y train data'}\n    },\n    'method': 'grid',\n    'metric': {'name': 'F1', 'goal': 'maximize'}\n}\n</pre> <pre>_____________________________________________________\n</pre> <pre>Create sweep with ID: kr0p2w24\nSweep URL: https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24\n</pre> <pre>wandb: Agent Starting Run: qvhp71sa with config:\nwandb: \tfilters: {'KnnFilter': {'n_neighbors': 2}}\nwandb: \tpipeline: {'clazz': 'KFoldSplitter', 'params': {'n_splits': 2, 'pipeline': {'clazz': 'F3Pipeline', 'params': {'filters': [{'clazz': 'StandardScalerPlugin', 'params': {}}, {'_grid': {'n_neighbors': [2, 3, 4, 5, 6]}, 'clazz': 'KnnFilter', 'params': {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': [2, 3, 4, 5, 6], 'p': 2, 'weights': 'uniform'}}], 'log': False, 'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}, {'clazz': 'Precission', 'params': {'average': 'weighted'}}, {'clazz': 'Recall', 'params': {'average': 'weighted'}}], 'overwrite': False, 'store': False}}, 'random_state': 42, 'shuffle': True}}\nwandb: \tx_dataset: Iris X train data\nwandb: \ty_dataset: Iris y train data\n</pre>  Tracking run with wandb version 0.19.9   Run data is saved locally in <code>/home/manuel.couto.pintos/Documents/code/framework3/docs/examples/notebooks/wandb/run-20250416_170907-qvhp71sa</code>  Syncing run swift-sweep-1 to Weights &amp; Biases (docs)Sweep page: https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View project at https://wandb.ai/citius-irlab/test_project   View sweep at https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View run at https://wandb.ai/citius-irlab/test_project/runs/qvhp71sa Run history:F1\u2581Run summary:F10.90865   View run swift-sweep-1 at: https://wandb.ai/citius-irlab/test_project/runs/qvhp71sa View project at: https://wandb.ai/citius-irlab/test_projectSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)   Find logs at: <code>./wandb/run-20250416_170907-qvhp71sa/logs</code> <pre>wandb: Agent Starting Run: bv8epurg with config:\nwandb: \tfilters: {'KnnFilter': {'n_neighbors': 3}}\nwandb: \tpipeline: {'clazz': 'KFoldSplitter', 'params': {'n_splits': 2, 'pipeline': {'clazz': 'F3Pipeline', 'params': {'filters': [{'clazz': 'StandardScalerPlugin', 'params': {}}, {'_grid': {'n_neighbors': [2, 3, 4, 5, 6]}, 'clazz': 'KnnFilter', 'params': {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': [2, 3, 4, 5, 6], 'p': 2, 'weights': 'uniform'}}], 'log': False, 'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}, {'clazz': 'Precission', 'params': {'average': 'weighted'}}, {'clazz': 'Recall', 'params': {'average': 'weighted'}}], 'overwrite': False, 'store': False}}, 'random_state': 42, 'shuffle': True}}\nwandb: \tx_dataset: Iris X train data\nwandb: \ty_dataset: Iris y train data\n</pre>  Tracking run with wandb version 0.19.9   Run data is saved locally in <code>/home/manuel.couto.pintos/Documents/code/framework3/docs/examples/notebooks/wandb/run-20250416_170913-bv8epurg</code>  Syncing run driven-sweep-2 to Weights &amp; Biases (docs)Sweep page: https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View project at https://wandb.ai/citius-irlab/test_project   View sweep at https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View run at https://wandb.ai/citius-irlab/test_project/runs/bv8epurg Run history:F1\u2581Run summary:F10.92541   View run driven-sweep-2 at: https://wandb.ai/citius-irlab/test_project/runs/bv8epurg View project at: https://wandb.ai/citius-irlab/test_projectSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)   Find logs at: <code>./wandb/run-20250416_170913-bv8epurg/logs</code> <pre>wandb: Agent Starting Run: y6ebmuh1 with config:\nwandb: \tfilters: {'KnnFilter': {'n_neighbors': 4}}\nwandb: \tpipeline: {'clazz': 'KFoldSplitter', 'params': {'n_splits': 2, 'pipeline': {'clazz': 'F3Pipeline', 'params': {'filters': [{'clazz': 'StandardScalerPlugin', 'params': {}}, {'_grid': {'n_neighbors': [2, 3, 4, 5, 6]}, 'clazz': 'KnnFilter', 'params': {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': [2, 3, 4, 5, 6], 'p': 2, 'weights': 'uniform'}}], 'log': False, 'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}, {'clazz': 'Precission', 'params': {'average': 'weighted'}}, {'clazz': 'Recall', 'params': {'average': 'weighted'}}], 'overwrite': False, 'store': False}}, 'random_state': 42, 'shuffle': True}}\nwandb: \tx_dataset: Iris X train data\nwandb: \ty_dataset: Iris y train data\n</pre>  Tracking run with wandb version 0.19.9   Run data is saved locally in <code>/home/manuel.couto.pintos/Documents/code/framework3/docs/examples/notebooks/wandb/run-20250416_170918-y6ebmuh1</code>  Syncing run vague-sweep-3 to Weights &amp; Biases (docs)Sweep page: https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View project at https://wandb.ai/citius-irlab/test_project   View sweep at https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View run at https://wandb.ai/citius-irlab/test_project/runs/y6ebmuh1 Run history:F1\u2581Run summary:F10.93372   View run vague-sweep-3 at: https://wandb.ai/citius-irlab/test_project/runs/y6ebmuh1 View project at: https://wandb.ai/citius-irlab/test_projectSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)   Find logs at: <code>./wandb/run-20250416_170918-y6ebmuh1/logs</code> <pre>wandb: Agent Starting Run: srq9t0tk with config:\nwandb: \tfilters: {'KnnFilter': {'n_neighbors': 5}}\nwandb: \tpipeline: {'clazz': 'KFoldSplitter', 'params': {'n_splits': 2, 'pipeline': {'clazz': 'F3Pipeline', 'params': {'filters': [{'clazz': 'StandardScalerPlugin', 'params': {}}, {'_grid': {'n_neighbors': [2, 3, 4, 5, 6]}, 'clazz': 'KnnFilter', 'params': {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': [2, 3, 4, 5, 6], 'p': 2, 'weights': 'uniform'}}], 'log': False, 'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}, {'clazz': 'Precission', 'params': {'average': 'weighted'}}, {'clazz': 'Recall', 'params': {'average': 'weighted'}}], 'overwrite': False, 'store': False}}, 'random_state': 42, 'shuffle': True}}\nwandb: \tx_dataset: Iris X train data\nwandb: \ty_dataset: Iris y train data\n</pre>  Tracking run with wandb version 0.19.9   Run data is saved locally in <code>/home/manuel.couto.pintos/Documents/code/framework3/docs/examples/notebooks/wandb/run-20250416_170924-srq9t0tk</code>  Syncing run apricot-sweep-4 to Weights &amp; Biases (docs)Sweep page: https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View project at https://wandb.ai/citius-irlab/test_project   View sweep at https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View run at https://wandb.ai/citius-irlab/test_project/runs/srq9t0tk Run history:F1\u2581Run summary:F10.91695   View run apricot-sweep-4 at: https://wandb.ai/citius-irlab/test_project/runs/srq9t0tk View project at: https://wandb.ai/citius-irlab/test_projectSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)   Find logs at: <code>./wandb/run-20250416_170924-srq9t0tk/logs</code> <pre>wandb: Agent Starting Run: za54vh64 with config:\nwandb: \tfilters: {'KnnFilter': {'n_neighbors': 6}}\nwandb: \tpipeline: {'clazz': 'KFoldSplitter', 'params': {'n_splits': 2, 'pipeline': {'clazz': 'F3Pipeline', 'params': {'filters': [{'clazz': 'StandardScalerPlugin', 'params': {}}, {'_grid': {'n_neighbors': [2, 3, 4, 5, 6]}, 'clazz': 'KnnFilter', 'params': {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': [2, 3, 4, 5, 6], 'p': 2, 'weights': 'uniform'}}], 'log': False, 'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}, {'clazz': 'Precission', 'params': {'average': 'weighted'}}, {'clazz': 'Recall', 'params': {'average': 'weighted'}}], 'overwrite': False, 'store': False}}, 'random_state': 42, 'shuffle': True}}\nwandb: \tx_dataset: Iris X train data\nwandb: \ty_dataset: Iris y train data\n</pre>  Tracking run with wandb version 0.19.9   Run data is saved locally in <code>/home/manuel.couto.pintos/Documents/code/framework3/docs/examples/notebooks/wandb/run-20250416_170929-za54vh64</code>  Syncing run silvery-sweep-5 to Weights &amp; Biases (docs)Sweep page: https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View project at https://wandb.ai/citius-irlab/test_project   View sweep at https://wandb.ai/citius-irlab/test_project/sweeps/kr0p2w24   View run at https://wandb.ai/citius-irlab/test_project/runs/za54vh64 Run history:F1\u2581Run summary:F10.93284   View run silvery-sweep-5 at: https://wandb.ai/citius-irlab/test_project/runs/za54vh64 View project at: https://wandb.ai/citius-irlab/test_projectSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)   Find logs at: <code>./wandb/run-20250416_170929-za54vh64/logs</code> <pre>wandb: Sweep Agent: Waiting for job.\nwandb: Sweep Agent: Exiting.\nwandb: Sorting runs by -summary_metrics.F1\n</pre> <pre>{\n    'filters': {'KnnFilter': {'n_neighbors': 4}},\n    'pipeline': {\n        'clazz': 'KFoldSplitter',\n        'params': {\n            'shuffle': True,\n            'n_splits': 2,\n            'pipeline': {\n                'clazz': 'F3Pipeline',\n                'params': {\n                    'log': False,\n                    'store': False,\n                    'filters': [\n                        {'clazz': 'StandardScalerPlugin', 'params': {}},\n                        {\n                            '_grid': {'n_neighbors': [2, 3, 4, 5, 6]},\n                            'clazz': 'KnnFilter',\n                            'params': {\n                                'p': 2,\n                                'metric': 'minkowski',\n                                'n_jobs': None,\n                                'weights': 'uniform',\n                                'algorithm': 'auto',\n                                'leaf_size': 30,\n                                'n_neighbors': [2, 3, 4, 5, 6],\n                                'metric_params': None\n                            }\n                        }\n                    ],\n                    'metrics': [\n                        {'clazz': 'F1', 'params': {'average': 'weighted'}},\n                        {'clazz': 'Precission', 'params': {'average': 'weighted'}},\n                        {'clazz': 'Recall', 'params': {'average': 'weighted'}}\n                    ],\n                    'overwrite': False\n                }\n            },\n            'random_state': 42\n        }\n    },\n    'x_dataset': 'Iris X train data',\n    'y_dataset': 'Iris y train data'\n}\n</pre> <pre>____________________________________________________________________________________________________\nFitting pipeline...\n****************************************************************************************************\n</pre> <pre>        *StandardScalerPlugin({})\n</pre> <pre>        *KnnFilter({'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'metric':\n'minkowski', 'metric_params': None, 'n_jobs': None})\n</pre> <pre>____________________________________________________________________________________________________\nPredicting with KFold Splitter......\n****************************************************************************************************\n</pre> <pre>F3Pipeline(\n    filters=[\n        StandardScalerPlugin(),\n        KnnFilter(\n            n_neighbors=4,\n            weights='uniform',\n            algorithm='auto',\n            leaf_size=30,\n            p=2,\n            metric='minkowski',\n            metric_params=None,\n            n_jobs=None\n        )\n    ],\n    metrics=[F1(average='weighted'), Precission(average='weighted'), Recall(average='weighted')],\n    overwrite=False,\n    store=False,\n    log=False\n)\n</pre> <pre>____________________________________________________________________________________________________\nPredicting pipeline...\n****************************************************************************************************\n</pre> <pre>        *StandardScalerPlugin({})\n</pre> <pre>        *KnnFilter({'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'metric':\n'minkowski', 'metric_params': None, 'n_jobs': None})\n</pre> In\u00a0[6]: Copied! <pre>wandb_pipeline.evaluate(X_test, y_test, _y)\n</pre> wandb_pipeline.evaluate(X_test, y_test, _y) <pre>____________________________________________________________________________________________________\nEvaluating pipeline......\n****************************************************************************************************\n</pre> Out[6]: <pre>{'F1': 1.0, 'Precission': 1.0, 'Recall': 1.0}</pre> <p></p> <p>Similar to Optuna, we can analyze the influence of each parameter on the selected metric. However, unlike Optuna, WandB offers a paid version with additional and more advanced features.</p>"},{"location":"examples/notebooks/wandb_optimizer_with_data_splitter_kfold/#wandb-optimizer","title":"Wandb Optimizer\u00b6","text":""},{"location":"examples/notebooks/wandb_optimizer_with_data_splitter_kfold/#how-to-perform-cross-validation-and-hiperparameter-optimization-with-wandb","title":"How to perform cross validation and hiperparameter optimization with WandB\u00b6","text":""},{"location":"examples/notebooks/wandb_optimizer_with_data_splitter_kfold/#we-will-use-a-simple-pipeline-for-the-iris-dataset","title":"We will use a simple pipeline for the iris dataset.\u00b6","text":""},{"location":"examples/notebooks/wandb_optimizer_with_data_splitter_kfold/#then-we-will-configure-wandb-for-hyperparameter-tuning-and-a-sklearn-splitter-for-cross-validation","title":"Then we will configure wandb for hyperparameter tuning and a Sklearn splitter for cross validation.\u00b6","text":""},{"location":"examples/notebooks/wandb_optimizer_with_data_splitter_kfold/#wandb-dashboard","title":"Wandb dashboard\u00b6","text":""},{"location":"examples/notebooks/metrics/erde/","title":"Erde","text":"In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import confusion_matrix\nfrom framework3 import BaseMetric, Container, XYData\n</pre> from sklearn.metrics import confusion_matrix from framework3 import BaseMetric, Container, XYData In\u00a0[\u00a0]: Copied! <pre>from numpy import exp\nimport numpy as np\n</pre> from numpy import exp import numpy as np In\u00a0[\u00a0]: Copied! <pre>__all__ = [\"ERDE_5\", \"ERDE_50\"]\n</pre> __all__ = [\"ERDE_5\", \"ERDE_50\"] In\u00a0[\u00a0]: Copied! <pre>class ERDE(BaseMetric):\n    def __init__(self, k: int = 5):\n        self.k = k\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; float | np.ndarray:\n        if y_true is None:\n            raise ValueError(\"y_true must be provided for evaluation\")\n\n        all_erde = []\n        _, _, _, tp = confusion_matrix(y_true.value, y_pred.value).ravel()\n        for expected, result, count in list(\n            zip(y_true.value, y_pred.value, x_data.value.n_texts.values.tolist())\n        ):\n            if result == 1 and expected == 0:\n                all_erde.append(float(tp) / len(y_true.value))\n            elif result == 0 and expected == 1:\n                all_erde.append(1.0)\n            elif result == 1 and expected == 1:\n                all_erde.append(1.0 - (1.0 / (1.0 + exp(count - self.k))))\n            elif result == 0 and expected == 0:\n                all_erde.append(0.0)\n        return float(np.mean(all_erde) * 100)\n</pre> class ERDE(BaseMetric):     def __init__(self, k: int = 5):         self.k = k      def evaluate(         self, x_data: XYData, y_true: XYData | None, y_pred: XYData     ) -&gt; float | np.ndarray:         if y_true is None:             raise ValueError(\"y_true must be provided for evaluation\")          all_erde = []         _, _, _, tp = confusion_matrix(y_true.value, y_pred.value).ravel()         for expected, result, count in list(             zip(y_true.value, y_pred.value, x_data.value.n_texts.values.tolist())         ):             if result == 1 and expected == 0:                 all_erde.append(float(tp) / len(y_true.value))             elif result == 0 and expected == 1:                 all_erde.append(1.0)             elif result == 1 and expected == 1:                 all_erde.append(1.0 - (1.0 / (1.0 + exp(count - self.k))))             elif result == 0 and expected == 0:                 all_erde.append(0.0)         return float(np.mean(all_erde) * 100) In\u00a0[\u00a0]: Copied! <pre>@Container.bind()\nclass ERDE_5(BaseMetric):\n    def __init__(self):\n        self._erde = ERDE(k=5)\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; float | np.ndarray:\n        if y_true is None:\n            raise ValueError(\"y_true must be provided for evaluation\")\n\n        return self._erde.evaluate(x_data, y_true, y_pred)\n</pre> @Container.bind() class ERDE_5(BaseMetric):     def __init__(self):         self._erde = ERDE(k=5)      def evaluate(         self, x_data: XYData, y_true: XYData | None, y_pred: XYData     ) -&gt; float | np.ndarray:         if y_true is None:             raise ValueError(\"y_true must be provided for evaluation\")          return self._erde.evaluate(x_data, y_true, y_pred) In\u00a0[\u00a0]: Copied! <pre>@Container.bind()\nclass ERDE_50(BaseMetric):\n    def __init__(self):\n        self._erde = ERDE(k=50)\n\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; float | np.ndarray:\n        if y_true is None:\n            raise ValueError(\"y_true must be provided for evaluation\")\n\n        return self._erde.evaluate(x_data, y_true, y_pred)\n</pre> @Container.bind() class ERDE_50(BaseMetric):     def __init__(self):         self._erde = ERDE(k=50)      def evaluate(         self, x_data: XYData, y_true: XYData | None, y_pred: XYData     ) -&gt; float | np.ndarray:         if y_true is None:             raise ValueError(\"y_true must be provided for evaluation\")          return self._erde.evaluate(x_data, y_true, y_pred)"},{"location":"examples/notebooks/metrics/f1/","title":"F1","text":"In\u00a0[\u00a0]: Copied! <pre>from framework3 import BaseMetric, Container, XYData, F1\n</pre> from framework3 import BaseMetric, Container, XYData, F1 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>__all__ = [\"F1Score\"]\n</pre> __all__ = [\"F1Score\"] In\u00a0[\u00a0]: Copied! <pre>@Container.bind()\nclass F1Score(BaseMetric):\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; float | np.ndarray:\n        if y_true is None:\n            raise ValueError(\"y_true must be provided for evaluation\")\n\n        x_data.value[\"_y\"] = y_pred.value\n\n        aux = x_data.value.groupby([\"user\"]).agg(\n            {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}\n        )\n\n        return float(\n            F1().evaluate(\n                x_data, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n            )\n        )\n</pre> @Container.bind() class F1Score(BaseMetric):     def evaluate(         self, x_data: XYData, y_true: XYData | None, y_pred: XYData     ) -&gt; float | np.ndarray:         if y_true is None:             raise ValueError(\"y_true must be provided for evaluation\")          x_data.value[\"_y\"] = y_pred.value          aux = x_data.value.groupby([\"user\"]).agg(             {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}         )          return float(             F1().evaluate(                 x_data, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())             )         )"},{"location":"examples/notebooks/metrics/precission/","title":"Precission","text":"In\u00a0[\u00a0]: Copied! <pre>from framework3 import BaseMetric, Container, XYData, Precission\n</pre> from framework3 import BaseMetric, Container, XYData, Precission In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>__all__ = [\"PrecissionScore\"]\n</pre> __all__ = [\"PrecissionScore\"] In\u00a0[\u00a0]: Copied! <pre>@Container.bind()\nclass PrecissionScore(BaseMetric):\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; float | np.ndarray:\n        if y_true is None:\n            raise ValueError(\"y_true must be provided for evaluation\")\n\n        x_data.value[\"_y\"] = y_pred.value\n\n        aux = x_data.value.groupby([\"user\"]).agg(\n            {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}\n        )\n\n        return float(\n            Precission().evaluate(\n                x_data, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n            )\n        )\n</pre> @Container.bind() class PrecissionScore(BaseMetric):     def evaluate(         self, x_data: XYData, y_true: XYData | None, y_pred: XYData     ) -&gt; float | np.ndarray:         if y_true is None:             raise ValueError(\"y_true must be provided for evaluation\")          x_data.value[\"_y\"] = y_pred.value          aux = x_data.value.groupby([\"user\"]).agg(             {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}         )          return float(             Precission().evaluate(                 x_data, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())             )         )"},{"location":"examples/notebooks/metrics/recall/","title":"Recall","text":"In\u00a0[\u00a0]: Copied! <pre>from framework3 import BaseMetric, Container, XYData, Recall\n</pre> from framework3 import BaseMetric, Container, XYData, Recall In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>__all__ = [\"RecallScore\"]\n</pre> __all__ = [\"RecallScore\"] In\u00a0[\u00a0]: Copied! <pre>@Container.bind()\nclass RecallScore(BaseMetric):\n    def evaluate(\n        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n    ) -&gt; float | np.ndarray:\n        if y_true is None:\n            raise ValueError(\"y_true must be provided for evaluation\")\n\n        x_data.value[\"_y\"] = y_pred.value\n\n        aux = x_data.value.groupby([\"user\"]).agg(\n            {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}\n        )\n\n        return float(\n            Recall().evaluate(\n                x_data, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n            )\n        )\n</pre> @Container.bind() class RecallScore(BaseMetric):     def evaluate(         self, x_data: XYData, y_true: XYData | None, y_pred: XYData     ) -&gt; float | np.ndarray:         if y_true is None:             raise ValueError(\"y_true must be provided for evaluation\")          x_data.value[\"_y\"] = y_pred.value          aux = x_data.value.groupby([\"user\"]).agg(             {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}         )          return float(             Recall().evaluate(                 x_data, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())             )         )"},{"location":"examples/notebooks/models/dcwe/","title":"Dcwe","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport numpy as np\n</pre> import torch import numpy as np In\u00a0[\u00a0]: Copied! <pre>from torch import nn\nfrom transformers.models.bert import BertForMaskedLM\nfrom torch.nn import functional as F\nfrom scipy.sparse import dok_matrix\n</pre> from torch import nn from transformers.models.bert import BertForMaskedLM from torch.nn import functional as F from scipy.sparse import dok_matrix In\u00a0[\u00a0]: Copied! <pre>def cosine_similarity(V1, V2):\n    dot_prod = torch.einsum(\n        \"abc, cba -&gt; ab\", [V1, V2.permute(*torch.arange(V2.ndim - 1, -1, -1))]\n    )\n    norm_1 = torch.norm(V1, dim=-1)\n    norm_2 = torch.norm(V2, dim=-1)\n    return dot_prod / torch.einsum(\n        \"bc, bc -&gt; bc\", norm_1, norm_2\n    )  # Scores de similitud entre embeddings est\u00e1ticos y contextualizados\n</pre> def cosine_similarity(V1, V2):     dot_prod = torch.einsum(         \"abc, cba -&gt; ab\", [V1, V2.permute(*torch.arange(V2.ndim - 1, -1, -1))]     )     norm_1 = torch.norm(V1, dim=-1)     norm_2 = torch.norm(V2, dim=-1)     return dot_prod / torch.einsum(         \"bc, bc -&gt; bc\", norm_1, norm_2     )  # Scores de similitud entre embeddings est\u00e1ticos y contextualizados In\u00a0[\u00a0]: Copied! <pre>def isin(ar1, ar2):\n    return (ar1[..., None] == ar2).any(-1)\n</pre> def isin(ar1, ar2):     return (ar1[..., None] == ar2).any(-1) In\u00a0[\u00a0]: Copied! <pre>class DCWE(nn.Module):\n    def __init__(\n        self,\n        lambda_a,\n        lambda_w,\n        vocab_filter=torch.tensor([]),\n        n_times=10,\n        *args,\n        **kwargs,\n    ):\n        super(DCWE, self).__init__()\n        self.bert = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n        self.bert_emb_layer = self.bert.get_input_embeddings()\n        print(f\"Model offset_components = {n_times}\")\n        self.offset_components = nn.ModuleList(\n            [OffsetComponent() for _ in range(n_times)]\n        )\n        self.lambda_a = lambda_a\n        self.lambda_w = lambda_w\n        self.vocab_filter = vocab_filter\n\n    # mlm_label, reviews, masks, segs, times, vocab_filter, SA\n    def forward(self, reviews, times, masks, segs):\n        bert_embs = self.bert_emb_layer(reviews)\n\n        offset_last = torch.cat(\n            [\n                self.offset_components[int(j.item())](bert_embs[i])\n                for i, j in enumerate(F.relu(times.detach().cpu() - 1))\n            ],\n            dim=0,\n        )\n        offset_now = torch.cat(\n            [\n                self.offset_components[int(j.item())](bert_embs[i])\n                for i, j in enumerate(times.detach().cpu())\n            ],\n            dim=0,\n        )\n        offset_last = offset_last * (\n            isin(reviews, self.vocab_filter)\n        ).float().unsqueeze(-1).expand(-1, -1, 768)\n        offset_now = offset_now * (isin(reviews, self.vocab_filter)).float().unsqueeze(\n            -1\n        ).expand(-1, -1, 768)\n\n        input_embs = bert_embs + offset_now\n\n        output = self.bert(\n            inputs_embeds=input_embs,\n            attention_mask=masks,\n            token_type_ids=segs,\n            output_hidden_states=True,\n        )\n\n        return offset_last, offset_now, output\n\n    def loss(self, out, labels, function):\n        offset_last, offset_now, output = out\n\n        logits = output.logits\n        loss = function(logits.view(-1, self.bert.config.vocab_size), labels.view(-1))\n        loss += self.lambda_a * torch.norm(offset_now, dim=-1).pow(2).mean()\n        loss += (\n            self.lambda_w * torch.norm(offset_now - offset_last, dim=-1).pow(2).mean()\n        )\n        return loss\n\n    def generate_deltas(self, texts, input_embs, output_embs, vocab_hash_map, deltas_f):\n        sim_matrix = deltas_f(input_embs, output_embs).detach().cpu().numpy()\n\n        chunk_mat = dok_matrix(\n            (sim_matrix.shape[0], len(vocab_hash_map)), dtype=np.float32\n        )\n        aux_t = texts.cpu().numpy()\n\n        for post_idx, post in enumerate(aux_t):\n            for token in set(post):\n                if token in vocab_hash_map:\n                    indices = np.where(post == token)\n                    # TODO estaba sum ahora mean\n                    chunk_mat[post_idx, vocab_hash_map[token]] = np.mean(\n                        sim_matrix[post_idx][indices]\n                    )\n\n        return chunk_mat\n</pre> class DCWE(nn.Module):     def __init__(         self,         lambda_a,         lambda_w,         vocab_filter=torch.tensor([]),         n_times=10,         *args,         **kwargs,     ):         super(DCWE, self).__init__()         self.bert = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")         self.bert_emb_layer = self.bert.get_input_embeddings()         print(f\"Model offset_components = {n_times}\")         self.offset_components = nn.ModuleList(             [OffsetComponent() for _ in range(n_times)]         )         self.lambda_a = lambda_a         self.lambda_w = lambda_w         self.vocab_filter = vocab_filter      # mlm_label, reviews, masks, segs, times, vocab_filter, SA     def forward(self, reviews, times, masks, segs):         bert_embs = self.bert_emb_layer(reviews)          offset_last = torch.cat(             [                 self.offset_components[int(j.item())](bert_embs[i])                 for i, j in enumerate(F.relu(times.detach().cpu() - 1))             ],             dim=0,         )         offset_now = torch.cat(             [                 self.offset_components[int(j.item())](bert_embs[i])                 for i, j in enumerate(times.detach().cpu())             ],             dim=0,         )         offset_last = offset_last * (             isin(reviews, self.vocab_filter)         ).float().unsqueeze(-1).expand(-1, -1, 768)         offset_now = offset_now * (isin(reviews, self.vocab_filter)).float().unsqueeze(             -1         ).expand(-1, -1, 768)          input_embs = bert_embs + offset_now          output = self.bert(             inputs_embeds=input_embs,             attention_mask=masks,             token_type_ids=segs,             output_hidden_states=True,         )          return offset_last, offset_now, output      def loss(self, out, labels, function):         offset_last, offset_now, output = out          logits = output.logits         loss = function(logits.view(-1, self.bert.config.vocab_size), labels.view(-1))         loss += self.lambda_a * torch.norm(offset_now, dim=-1).pow(2).mean()         loss += (             self.lambda_w * torch.norm(offset_now - offset_last, dim=-1).pow(2).mean()         )         return loss      def generate_deltas(self, texts, input_embs, output_embs, vocab_hash_map, deltas_f):         sim_matrix = deltas_f(input_embs, output_embs).detach().cpu().numpy()          chunk_mat = dok_matrix(             (sim_matrix.shape[0], len(vocab_hash_map)), dtype=np.float32         )         aux_t = texts.cpu().numpy()          for post_idx, post in enumerate(aux_t):             for token in set(post):                 if token in vocab_hash_map:                     indices = np.where(post == token)                     # TODO estaba sum ahora mean                     chunk_mat[post_idx, vocab_hash_map[token]] = np.mean(                         sim_matrix[post_idx][indices]                     )          return chunk_mat In\u00a0[\u00a0]: Copied! <pre>class OffsetComponent(nn.Module):\n    def __init__(self):\n        super(OffsetComponent, self).__init__()\n        self.linear_1 = nn.Linear(768, 768)\n        self.linear_2 = nn.Linear(768, 768)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, embs):\n        h = self.dropout(torch.tanh(self.linear_1(embs)))\n        offset = self.linear_2(h).unsqueeze(0)\n        return offset\n</pre> class OffsetComponent(nn.Module):     def __init__(self):         super(OffsetComponent, self).__init__()         self.linear_1 = nn.Linear(768, 768)         self.linear_2 = nn.Linear(768, 768)         self.dropout = nn.Dropout(0.2)      def forward(self, embs):         h = self.dropout(torch.tanh(self.linear_1(embs)))         offset = self.linear_2(h).unsqueeze(0)         return offset In\u00a0[\u00a0]: Copied! <pre>class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.linear_1 = nn.Linear(768, 100)\n        self.linear_2 = nn.Linear(100, 1)\n        self.dropout = nn.Dropout(0.2)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, sims):\n        proj_1 = self.dropout(self.linear_1(sims))\n        return torch.sigmoid(self.linear_2(proj_1))\n</pre> class Classifier(nn.Module):     def __init__(self):         super(Classifier, self).__init__()         self.linear_1 = nn.Linear(768, 100)         self.linear_2 = nn.Linear(100, 1)         self.dropout = nn.Dropout(0.2)         self.softmax = nn.Softmax(dim=1)      def forward(self, sims):         proj_1 = self.dropout(self.linear_1(sims))         return torch.sigmoid(self.linear_2(proj_1))"},{"location":"examples/notebooks/models/deltas/","title":"Deltas","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.nn import functional as F\n</pre> import torch from torch.nn import functional as F In\u00a0[\u00a0]: Copied! <pre>def jensen_shannon_divergence(p, q):\n    m = 0.5 * (p + q)\n    return 0.5 * (\n        F.kl_div(m.log(), p, reduction=\"none\").sum(-1)\n        + F.kl_div(m.log(), q, reduction=\"none\").sum(-1)\n    )\n</pre> def jensen_shannon_divergence(p, q):     m = 0.5 * (p + q)     return 0.5 * (         F.kl_div(m.log(), p, reduction=\"none\").sum(-1)         + F.kl_div(m.log(), q, reduction=\"none\").sum(-1)     ) In\u00a0[\u00a0]: Copied! <pre>def wasserstein_distance(u_values, v_values):\n    def compute_wasserstein(u, v):\n        u_sorter = torch.argsort(u, dim=-1)\n        v_sorter = torch.argsort(v, dim=-1)\n\n        all_values = torch.cat([u, v], dim=-1)\n        all_values, _ = torch.sort(all_values, dim=-1)\n\n        deltas = torch.diff(all_values, dim=-1)\n\n        u_cdf = torch.searchsorted(\n            torch.gather(u, -1, u_sorter), all_values[..., :-1], right=True\n        )\n        v_cdf = torch.searchsorted(\n            torch.gather(v, -1, v_sorter), all_values[..., :-1], right=True\n        )\n\n        return torch.sum(torch.abs(u_cdf - v_cdf) * deltas, dim=-1)\n\n    distances = torch.stack(\n        [\n            compute_wasserstein(u_values[:, i, :], v_values[:, i, :])\n            for i in range(u_values.shape[1])\n        ],\n        dim=1,\n    )\n\n    return distances\n</pre> def wasserstein_distance(u_values, v_values):     def compute_wasserstein(u, v):         u_sorter = torch.argsort(u, dim=-1)         v_sorter = torch.argsort(v, dim=-1)          all_values = torch.cat([u, v], dim=-1)         all_values, _ = torch.sort(all_values, dim=-1)          deltas = torch.diff(all_values, dim=-1)          u_cdf = torch.searchsorted(             torch.gather(u, -1, u_sorter), all_values[..., :-1], right=True         )         v_cdf = torch.searchsorted(             torch.gather(v, -1, v_sorter), all_values[..., :-1], right=True         )          return torch.sum(torch.abs(u_cdf - v_cdf) * deltas, dim=-1)      distances = torch.stack(         [             compute_wasserstein(u_values[:, i, :], v_values[:, i, :])             for i in range(u_values.shape[1])         ],         dim=1,     )      return distances In\u00a0[\u00a0]: Copied! <pre>def pairwise_distance(input_embs, output_embs, p=2):\n    diff = input_embs - output_embs\n    distances = torch.norm(diff, p=p, dim=-1)\n\n    return distances\n</pre> def pairwise_distance(input_embs, output_embs, p=2):     diff = input_embs - output_embs     distances = torch.norm(diff, p=p, dim=-1)      return distances In\u00a0[\u00a0]: Copied! <pre>DISTANCES = {\n    \"cosine\": lambda input_embs, output_embs: (\n        1 - F.cosine_similarity(input_embs, output_embs, dim=-1)\n    )\n    / 2,\n    \"euclidean\": lambda input_embs, output_embs: pairwise_distance(\n        input_embs, output_embs, p=2\n    ),\n    \"manhattan\": lambda input_embs, output_embs: pairwise_distance(\n        input_embs, output_embs, p=1\n    ),\n    \"jensen_shannon\": lambda input_embs, output_embs: jensen_shannon_divergence(\n        input_embs, output_embs\n    ),\n    \"wasserstein\": lambda input_embs, output_embs: wasserstein_distance(\n        input_embs, output_embs\n    ),\n    \"chebyshev\": lambda input_embs, output_embs: torch.max(\n        torch.abs(input_embs - output_embs), dim=-1\n    ).values,\n    \"minkowski\": lambda input_embs, output_embs: pairwise_distance(\n        input_embs, output_embs, p=3\n    ).pow(1 / 3),\n}\n</pre> DISTANCES = {     \"cosine\": lambda input_embs, output_embs: (         1 - F.cosine_similarity(input_embs, output_embs, dim=-1)     )     / 2,     \"euclidean\": lambda input_embs, output_embs: pairwise_distance(         input_embs, output_embs, p=2     ),     \"manhattan\": lambda input_embs, output_embs: pairwise_distance(         input_embs, output_embs, p=1     ),     \"jensen_shannon\": lambda input_embs, output_embs: jensen_shannon_divergence(         input_embs, output_embs     ),     \"wasserstein\": lambda input_embs, output_embs: wasserstein_distance(         input_embs, output_embs     ),     \"chebyshev\": lambda input_embs, output_embs: torch.max(         torch.abs(input_embs - output_embs), dim=-1     ).values,     \"minkowski\": lambda input_embs, output_embs: pairwise_distance(         input_embs, output_embs, p=3     ).pow(1 / 3), } In\u00a0[\u00a0]: Copied! <pre>def f_generate_deltas(input_embs, output_embs, distance_metric=\"cosine\"):\n    if distance_metric in DISTANCES:\n        sim_matrix = DISTANCES[distance_metric](input_embs, output_embs)\n    else:\n        raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n\n    return sim_matrix\n</pre> def f_generate_deltas(input_embs, output_embs, distance_metric=\"cosine\"):     if distance_metric in DISTANCES:         sim_matrix = DISTANCES[distance_metric](input_embs, output_embs)     else:         raise ValueError(f\"Unsupported distance metric: {distance_metric}\")      return sim_matrix"},{"location":"examples/notebooks/models/random_forest/","title":"Random forest","text":"In\u00a0[\u00a0]: Copied! <pre>from typing import Literal\nfrom sklearn.ensemble import RandomForestClassifier\n</pre> from typing import Literal from sklearn.ensemble import RandomForestClassifier In\u00a0[\u00a0]: Copied! <pre>from framework3 import Container, XYData\nfrom framework3.base import BaseFilter\n</pre> from framework3 import Container, XYData from framework3.base import BaseFilter In\u00a0[\u00a0]: Copied! <pre>@Container.bind()\nclass GaussianNaiveBayes(BaseFilter):\n    def __init__(\n        self,\n        n_estimators=100,\n        criterion: Literal[\"gini\", \"entropy\", \"log_loss\"] = \"gini\",\n        max_depth=2,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        max_features: float | Literal[\"sqrt\", \"log2\"] = \"sqrt\",\n        class_weight=None,\n        proba=False,\n    ):\n        super().__init__()\n        self.proba = proba\n        self._model = RandomForestClassifier(\n            n_estimators=n_estimators,\n            criterion=criterion,\n            max_depth=max_depth,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            max_features=max_features,\n            class_weight=class_weight,\n            random_state=0,\n        )\n\n    def fit(self, x: XYData, y: XYData | None):\n        if y is None:\n            raise ValueError(\"y must be provided for training\")\n        self._model.fit(x.value, y.value)\n\n    def predict(self, x: XYData) -&gt; XYData:\n        if self.proba:\n            result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))\n        else:\n            result = self._model.predict(x.value)\n        return XYData.mock(result)\n</pre> @Container.bind() class GaussianNaiveBayes(BaseFilter):     def __init__(         self,         n_estimators=100,         criterion: Literal[\"gini\", \"entropy\", \"log_loss\"] = \"gini\",         max_depth=2,         min_samples_split=2,         min_samples_leaf=1,         max_features: float | Literal[\"sqrt\", \"log2\"] = \"sqrt\",         class_weight=None,         proba=False,     ):         super().__init__()         self.proba = proba         self._model = RandomForestClassifier(             n_estimators=n_estimators,             criterion=criterion,             max_depth=max_depth,             min_samples_split=min_samples_split,             min_samples_leaf=min_samples_leaf,             max_features=max_features,             class_weight=class_weight,             random_state=0,         )      def fit(self, x: XYData, y: XYData | None):         if y is None:             raise ValueError(\"y must be provided for training\")         self._model.fit(x.value, y.value)      def predict(self, x: XYData) -&gt; XYData:         if self.proba:             result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))         else:             result = self._model.predict(x.value)         return XYData.mock(result)"},{"location":"examples/notebooks/models/svm/","title":"Svm","text":"In\u00a0[\u00a0]: Copied! <pre>from typing import Any, Callable, Literal, Mapping\nfrom sklearn.svm import SVC\n</pre> from typing import Any, Callable, Literal, Mapping from sklearn.svm import SVC In\u00a0[\u00a0]: Copied! <pre>from framework3 import Container, XYData\nfrom framework3.base import BaseFilter\n</pre> from framework3 import Container, XYData from framework3.base import BaseFilter In\u00a0[\u00a0]: Copied! <pre>@Container.bind()\nclass ClassifierSVM(BaseFilter):\n    def __init__(\n        self,\n        C: float = 1,\n        kernel: Callable\n        | Literal[\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"] = \"rbf\",\n        gamma: float | Literal[\"scale\", \"auto\"] = \"scale\",\n        coef0: float = 0.0,\n        tol: float = 0.001,\n        decision_function_shape: Literal[\"ovo\", \"ovr\"] = \"ovr\",\n        class_weight_1: Mapping[Any, Any] | str | None = None,\n        probability: bool = False,\n    ):\n        super().__init__()\n        self.proba = probability\n        self._model = SVC(\n            C=C,\n            kernel=kernel,\n            gamma=gamma,\n            coef0=coef0,\n            tol=tol,\n            decision_function_shape=decision_function_shape,\n            class_weight=class_weight_1,\n            probability=probability,\n            random_state=43,\n        )\n\n    def fit(self, x: XYData, y: XYData | None):\n        if y is None:\n            raise ValueError(\"y must be provided for training\")\n        self._model.fit(x.value, y.value)\n\n    def predict(self, x: XYData) -&gt; XYData:\n        if self.proba:\n            result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))\n            return XYData.mock(result)\n        else:\n            result = self._model.predict(x.value)\n            return XYData.mock(result)\n</pre> @Container.bind() class ClassifierSVM(BaseFilter):     def __init__(         self,         C: float = 1,         kernel: Callable         | Literal[\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"] = \"rbf\",         gamma: float | Literal[\"scale\", \"auto\"] = \"scale\",         coef0: float = 0.0,         tol: float = 0.001,         decision_function_shape: Literal[\"ovo\", \"ovr\"] = \"ovr\",         class_weight_1: Mapping[Any, Any] | str | None = None,         probability: bool = False,     ):         super().__init__()         self.proba = probability         self._model = SVC(             C=C,             kernel=kernel,             gamma=gamma,             coef0=coef0,             tol=tol,             decision_function_shape=decision_function_shape,             class_weight=class_weight_1,             probability=probability,             random_state=43,         )      def fit(self, x: XYData, y: XYData | None):         if y is None:             raise ValueError(\"y must be provided for training\")         self._model.fit(x.value, y.value)      def predict(self, x: XYData) -&gt; XYData:         if self.proba:             result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))             return XYData.mock(result)         else:             result = self._model.predict(x.value)             return XYData.mock(result)"},{"location":"examples/notebooks/models/twec/","title":"Twec","text":"In\u00a0[\u00a0]: Copied! <pre>from gensim.models.word2vec import Word2Vec, LineSentence, PathLineSentences\n</pre> from gensim.models.word2vec import Word2Vec, LineSentence, PathLineSentences In\u00a0[\u00a0]: Copied! <pre>from gensim import utils\nimport os\nimport numpy as np\nimport logging\nimport copy\nfrom gensim.utils import tokenize\nimport multiprocessing\nfrom tqdm import tqdm\nfrom gensim.models import callbacks\nfrom itertools import chain\n</pre> from gensim import utils import os import numpy as np import logging import copy from gensim.utils import tokenize import multiprocessing from tqdm import tqdm from gensim.models import callbacks from itertools import chain In\u00a0[\u00a0]: Copied! <pre>class MyCallback(callbacks.CallbackAny2Vec):\n    def __init__(self):\n        self.epoch = 0\n\n    def on_epoch_end(self, model):\n        loss = model.get_latest_training_loss()\n        if self.epoch == 0:\n            print(\"P\u00e9rdida despu\u00e9s de la \u00e9poca {}: {}\".format(self.epoch, loss))\n        else:\n            print(\n                \"P\u00e9rdida despu\u00e9s de la \u00e9poca {}: {}\".format(\n                    self.epoch, loss - self.loss_previous_step\n                )\n            )\n        self.epoch += 1\n        self.loss_previous_step = loss\n</pre> class MyCallback(callbacks.CallbackAny2Vec):     def __init__(self):         self.epoch = 0      def on_epoch_end(self, model):         loss = model.get_latest_training_loss()         if self.epoch == 0:             print(\"P\u00e9rdida despu\u00e9s de la \u00e9poca {}: {}\".format(self.epoch, loss))         else:             print(                 \"P\u00e9rdida despu\u00e9s de la \u00e9poca {}: {}\".format(                     self.epoch, loss - self.loss_previous_step                 )             )         self.epoch += 1         self.loss_previous_step = loss In\u00a0[\u00a0]: Copied! <pre>class TWEC:\n    \"\"\"\n    Handles alignment between multiple slices of temporal text\n    \"\"\"\n\n    def __init__(\n        self,\n        size=100,\n        sg=0,\n        siter=10,\n        ns=10,\n        window=5,\n        alpha=0.025,\n        min_count=5,\n        workers=2,\n        test=\"test\",\n        init_mode=\"hidden\",\n    ):\n        \"\"\"\n\n        :param size: Number of dimensions. Default is 100.\n        :param sg: Neural architecture of Word2vec. Default is CBOW (). If 1, Skip-gram is employed.\n        :param siter: Number of static iterations (epochs). Default is 5.\n        :param diter: Number of dynamic iterations (epochs). Default is 5.\n        :param ns: Number of negative sampling examples. Default is 10, min is 1.\n        :param window: Size of the context window (left and right). Default is 5 (5 left + 5 right).\n        :param alpha: Initial learning rate. Default is 0.025.\n        :param min_count: Min frequency for words over the entire corpus. Default is 5.\n        :param workers: Number of worker threads. Default is 2.\n        :param test: Folder name of the diachronic corpus files for testing.\n        :param init_mode: If \\\"hidden\\\" (default), initialize temporal models with hidden embeddings of the context;'\n                            'if \\\"both\\\", initilize also the word embeddings;'\n                            'if \\\"copy\\\", temporal models are initiliazed as a copy of the context model\n                            (same vocabulary)\n        \"\"\"\n        self.size = size\n        self.sg = sg\n        self.trained_slices = dict()\n        self.gvocab = []\n        self.epoch = siter\n        self.negative = ns\n        self.window = window\n        self.static_alpha = alpha\n        self.dynamic_alpha = alpha\n        self.min_count = min_count\n        self.workers = multiprocessing.cpu_count() - 3\n        self.test = test\n        self.init_mode = init_mode\n        self.compass: None | Word2Vec = None\n\n    def initialize_from_compass(self, model) -&gt; Word2Vec:\n        if self.compass is None:\n            raise Exception(\"Compass model is not initialized\")\n\n        if self.init_mode == \"copy\":\n            model = copy.deepcopy(self.compass)\n        else:\n            if self.compass.layer1_size != self.size:  # type: ignore\n                raise Exception(\"Compass and Slice have different vector sizes\")\n\n            if len(model.wv.index_to_key) == 0:\n                model.build_vocab(corpus_iterable=self.compass.wv.index_to_key)  # type: ignore\n\n            vocab_m = model.wv.index_to_key\n\n            indices = [\n                self.compass.wv.key_to_index[w]\n                for w in vocab_m\n                if w in self.compass.wv.key_to_index\n            ]\n            new_syn1neg = np.array([self.compass.syn1neg[index] for index in indices])\n            model.syn1neg = new_syn1neg\n\n            if self.init_mode == \"both\":\n                new_syn0 = np.array([self.compass.wv.syn0[index] for index in indices])  # type: ignore\n                model.wv.syn0 = new_syn0\n\n        model.learn_hidden = False  # type: ignore\n        model.alpha = self.dynamic_alpha\n        return model\n\n    def internal_trimming_rule(self, word, count, min_count):\n        \"\"\"\n        Internal rule used to trim words\n        :param word:\n        :return:\n        \"\"\"\n        if word in self.gvocab:\n            return utils.RULE_KEEP\n        else:\n            return utils.RULE_DISCARD\n\n    def train_model(self, sentences) -&gt; Word2Vec | None:\n        model = None\n        if self.compass is None or self.init_mode != \"copy\":\n            model = Word2Vec(\n                sg=self.sg,\n                vector_size=self.size,\n                alpha=self.static_alpha,\n                negative=self.negative,\n                window=self.window,\n                min_count=self.min_count,\n                workers=self.workers,\n            )\n            model.build_vocab(\n                corpus_iterable=sentences,\n                trim_rule=self.internal_trimming_rule\n                if self.compass is not None\n                else None,\n            )\n\n        if self.compass is not None:\n            model = self.initialize_from_compass(model)\n            model.train(\n                corpus_iterable=sentences,\n                total_words=sum([len(s) for s in sentences]),\n                epochs=self.epoch,\n                compute_loss=True,\n            )\n        else:\n            model.train(  # type: ignore\n                corpus_iterable=sentences,\n                total_words=sum([len(s) for s in sentences]),\n                epochs=self.epoch,\n                compute_loss=True,\n                callbacks=[MyCallback()],\n            )\n\n        return model\n\n    def train_compass(self, chunks):\n        texts = list(chain(*chunks))\n        sentences = [\n            list(tokenize(str(text), lowercase=True, deacc=True))\n            for text in tqdm(texts, desc=\"Preparing full corpus\")\n        ]\n        print(\"Training the compass.\")\n        self.compass = self.train_model(sentences)\n        self.gvocab = self.compass.wv.index_to_key  # type: ignore\n\n    def train_slice(self, chunks):\n        if self.compass is None:\n            return Exception(\"Missing Compass\")\n\n        sentences = [\n            list(tokenize(str(text), lowercase=True, deacc=True)) for text in chunks\n        ]\n        model = self.train_model(sentences)\n        return model\n\n    # FINE TUNNING VARIATION\n\n    def finetune_model(self, sentences, pretrained_path):\n        model = None\n        if self.compass is None or self.init_mode != \"copy\":\n            model = Word2Vec(\n                sg=self.sg,\n                vector_size=self.size,\n                alpha=self.static_alpha,\n                negative=self.negative,\n                window=self.window,\n                min_count=self.min_count,\n                workers=self.workers,\n            )\n            model.build_vocab(\n                sentences,\n                trim_rule=self.internal_trimming_rule\n                if self.compass is not None\n                else None,\n            )\n            # model.build_vocab(list(pretrained_model.vocab.keys()), update=True)\n            model.intersect_word2vec_format(pretrained_path, binary=True, lockf=1.0)  # type: ignore\n\n        if self.compass is not None:\n            model = self.initialize_from_compass(model)\n\n        model.train(  # type: ignore\n            sentences,\n            total_words=sum([len(s) for s in sentences]),\n            epochs=self.epoch,\n            compute_loss=True,\n        )\n\n        return model\n\n    def finetune_compass(self, compass_text, pre_path, overwrite=False, save=True):\n        sentences = PathLineSentences(compass_text)\n        sentences.input_files = [\n            s for s in sentences.input_files if not os.path.basename(s).startswith(\".\")\n        ]\n        logging.info(\"Finetunning the compass.\")\n        self.compass = self.finetune_model(sentences, pre_path)\n\n        self.gvocab = self.compass.wv.index_to_key  # type: ignore\n\n    def finetune_slice(self, slice_text, pretrained):\n        try:\n            if self.compass is None:\n                logging.info(\"Fuck where is the dam compass\")\n                return Exception(\"Missing Compass\")\n            logging.info(\n                \"Finetunning temporal embeddings: slice {}.\".format(slice_text)\n            )\n\n            sentences = LineSentence(slice_text)\n            model = self.finetune_model(sentences, pretrained)\n            return model\n        except Exception as fk:\n            logging.error(\"What da &gt; {}\".format(fk))\n</pre> class TWEC:     \"\"\"     Handles alignment between multiple slices of temporal text     \"\"\"      def __init__(         self,         size=100,         sg=0,         siter=10,         ns=10,         window=5,         alpha=0.025,         min_count=5,         workers=2,         test=\"test\",         init_mode=\"hidden\",     ):         \"\"\"          :param size: Number of dimensions. Default is 100.         :param sg: Neural architecture of Word2vec. Default is CBOW (). If 1, Skip-gram is employed.         :param siter: Number of static iterations (epochs). Default is 5.         :param diter: Number of dynamic iterations (epochs). Default is 5.         :param ns: Number of negative sampling examples. Default is 10, min is 1.         :param window: Size of the context window (left and right). Default is 5 (5 left + 5 right).         :param alpha: Initial learning rate. Default is 0.025.         :param min_count: Min frequency for words over the entire corpus. Default is 5.         :param workers: Number of worker threads. Default is 2.         :param test: Folder name of the diachronic corpus files for testing.         :param init_mode: If \\\"hidden\\\" (default), initialize temporal models with hidden embeddings of the context;'                             'if \\\"both\\\", initilize also the word embeddings;'                             'if \\\"copy\\\", temporal models are initiliazed as a copy of the context model                             (same vocabulary)         \"\"\"         self.size = size         self.sg = sg         self.trained_slices = dict()         self.gvocab = []         self.epoch = siter         self.negative = ns         self.window = window         self.static_alpha = alpha         self.dynamic_alpha = alpha         self.min_count = min_count         self.workers = multiprocessing.cpu_count() - 3         self.test = test         self.init_mode = init_mode         self.compass: None | Word2Vec = None      def initialize_from_compass(self, model) -&gt; Word2Vec:         if self.compass is None:             raise Exception(\"Compass model is not initialized\")          if self.init_mode == \"copy\":             model = copy.deepcopy(self.compass)         else:             if self.compass.layer1_size != self.size:  # type: ignore                 raise Exception(\"Compass and Slice have different vector sizes\")              if len(model.wv.index_to_key) == 0:                 model.build_vocab(corpus_iterable=self.compass.wv.index_to_key)  # type: ignore              vocab_m = model.wv.index_to_key              indices = [                 self.compass.wv.key_to_index[w]                 for w in vocab_m                 if w in self.compass.wv.key_to_index             ]             new_syn1neg = np.array([self.compass.syn1neg[index] for index in indices])             model.syn1neg = new_syn1neg              if self.init_mode == \"both\":                 new_syn0 = np.array([self.compass.wv.syn0[index] for index in indices])  # type: ignore                 model.wv.syn0 = new_syn0          model.learn_hidden = False  # type: ignore         model.alpha = self.dynamic_alpha         return model      def internal_trimming_rule(self, word, count, min_count):         \"\"\"         Internal rule used to trim words         :param word:         :return:         \"\"\"         if word in self.gvocab:             return utils.RULE_KEEP         else:             return utils.RULE_DISCARD      def train_model(self, sentences) -&gt; Word2Vec | None:         model = None         if self.compass is None or self.init_mode != \"copy\":             model = Word2Vec(                 sg=self.sg,                 vector_size=self.size,                 alpha=self.static_alpha,                 negative=self.negative,                 window=self.window,                 min_count=self.min_count,                 workers=self.workers,             )             model.build_vocab(                 corpus_iterable=sentences,                 trim_rule=self.internal_trimming_rule                 if self.compass is not None                 else None,             )          if self.compass is not None:             model = self.initialize_from_compass(model)             model.train(                 corpus_iterable=sentences,                 total_words=sum([len(s) for s in sentences]),                 epochs=self.epoch,                 compute_loss=True,             )         else:             model.train(  # type: ignore                 corpus_iterable=sentences,                 total_words=sum([len(s) for s in sentences]),                 epochs=self.epoch,                 compute_loss=True,                 callbacks=[MyCallback()],             )          return model      def train_compass(self, chunks):         texts = list(chain(*chunks))         sentences = [             list(tokenize(str(text), lowercase=True, deacc=True))             for text in tqdm(texts, desc=\"Preparing full corpus\")         ]         print(\"Training the compass.\")         self.compass = self.train_model(sentences)         self.gvocab = self.compass.wv.index_to_key  # type: ignore      def train_slice(self, chunks):         if self.compass is None:             return Exception(\"Missing Compass\")          sentences = [             list(tokenize(str(text), lowercase=True, deacc=True)) for text in chunks         ]         model = self.train_model(sentences)         return model      # FINE TUNNING VARIATION      def finetune_model(self, sentences, pretrained_path):         model = None         if self.compass is None or self.init_mode != \"copy\":             model = Word2Vec(                 sg=self.sg,                 vector_size=self.size,                 alpha=self.static_alpha,                 negative=self.negative,                 window=self.window,                 min_count=self.min_count,                 workers=self.workers,             )             model.build_vocab(                 sentences,                 trim_rule=self.internal_trimming_rule                 if self.compass is not None                 else None,             )             # model.build_vocab(list(pretrained_model.vocab.keys()), update=True)             model.intersect_word2vec_format(pretrained_path, binary=True, lockf=1.0)  # type: ignore          if self.compass is not None:             model = self.initialize_from_compass(model)          model.train(  # type: ignore             sentences,             total_words=sum([len(s) for s in sentences]),             epochs=self.epoch,             compute_loss=True,         )          return model      def finetune_compass(self, compass_text, pre_path, overwrite=False, save=True):         sentences = PathLineSentences(compass_text)         sentences.input_files = [             s for s in sentences.input_files if not os.path.basename(s).startswith(\".\")         ]         logging.info(\"Finetunning the compass.\")         self.compass = self.finetune_model(sentences, pre_path)          self.gvocab = self.compass.wv.index_to_key  # type: ignore      def finetune_slice(self, slice_text, pretrained):         try:             if self.compass is None:                 logging.info(\"Fuck where is the dam compass\")                 return Exception(\"Missing Compass\")             logging.info(                 \"Finetunning temporal embeddings: slice {}.\".format(slice_text)             )              sentences = LineSentence(slice_text)             model = self.finetune_model(sentences, pretrained)             return model         except Exception as fk:             logging.error(\"What da &gt; {}\".format(fk))"},{"location":"installation/","title":"Installation Guide for LabChain","text":"<p>This guide will walk you through the process of installing LabChain using pip.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing LabChain, ensure you have the following prerequisites:</p> <ol> <li>Python 3.11 or higher</li> <li>pip (Python package installer)</li> </ol>"},{"location":"installation/#installation","title":"Installation","text":"<p>Installing LabChain is straightforward using pip. Follow these steps:</p>"},{"location":"installation/#step-1-set-up-a-virtual-environment-recommended","title":"Step 1: Set Up a Virtual Environment (Recommended)","text":"<p>It's recommended to use a virtual environment to avoid conflicts with other Python projects:</p> <pre><code>python -m venv venv\n</code></pre> <p>Activate the virtual environment:</p> <ul> <li>On Windows:   <pre><code>venv\\Scripts\\activate\n</code></pre></li> <li>On macOS and Linux:   <pre><code>source venv/bin/activate\n</code></pre></li> </ul>"},{"location":"installation/#step-2-install-labchain","title":"Step 2: Install LabChain","text":"<p>Install LabChain directly from PyPI using pip:</p> <pre><code>pip install framework3\n</code></pre> <p>This command will install the latest stable version of LabChain and all its dependencies.</p>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>To verify that LabChain is installed correctly, you can run a simple Python script:</p> <pre><code>from framework3 import __version__\n\nprint(f\"LabChain version: {__version__}\")\n</code></pre> <p>If this runs without errors and prints the version number, the installation was successful.</p>"},{"location":"installation/#updating-labchain","title":"Updating LabChain","text":"<p>To update LabChain to the latest version, simply run:</p> <pre><code>pip install --upgrade framework3\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation:</p> <ol> <li>Ensure your Python version is 3.11 or higher.</li> <li>Make sure pip is up to date: <code>pip install --upgrade pip</code></li> <li>If you're using a virtual environment, ensure it's activated when installing and using LabChain.</li> </ol> <p>For more detailed error messages, you can use the verbose mode when installing:</p> <pre><code>pip install -v framework3\n</code></pre> <p>If problems persist, please check the project's issue tracker on GitHub or reach out to the maintainers for support.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Now that you have LabChain installed, you can start using it in your projects. Check out the Quick Start Guide for an introduction to using LabChain, or explore the API Documentation for more detailed information on available modules and functions.</p>"},{"location":"quick_start/","title":"Quick Start Guide for LabChain","text":"<p>This guide will help you get started with LabChain, demonstrating its basic usage and core concepts.</p>"},{"location":"quick_start/#1-installation","title":"1. Installation","text":"<p>Install LabChain using pip:</p> <pre><code>pip install framework3\n</code></pre>"},{"location":"quick_start/#2-basic-concepts","title":"2. Basic Concepts","text":"<ul> <li>LabChain is built around:<ul> <li>Pipelines: Orchestrate the flow of data through processing steps.</li> <li>Filters: Perform specific operations on data.</li> <li>Metrics: Evaluate model performance.</li> </ul> </li> </ul>"},{"location":"quick_start/#3-creating-your-first-pipeline","title":"3. Creating Your First Pipeline","text":"<p>Let's create a simple pipeline that preprocesses data and performs classification:</p> <pre><code>from framework3 import ClassifierSVMPlugin, F3Pipeline\nfrom framework3.plugins.filters import StandardScalerPlugin\nfrom framework3.plugins.metrics import F1\nfrom framework3.base import XYData\nfrom sklearn.datasets import load_iris\n\n# Load and prepare data\niris = load_iris()\nX, y = iris.data, iris.target # type: ignore\n\n# Split the data into training and test sets\n\nX_train, X_test, y_train, y_test = XYData(\"Iris\", \"/dataset\", [])\\\n    .train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Make predictions and evaluate\npipeline = F3Pipeline(\n    filters=[\n        StandardScalerPlugin(),\n        ClassifierSVMPlugin()\n    ],\n    metrics=[F1()]\n)\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\nevaluation = pipeline.evaluate(X_test, y_test, predictions)\n</code></pre>"},{"location":"quick_start/#next-steps","title":"Next Steps","text":"<ul> <li>For more advanced usage and detailed API documentation, refer to:<ul> <li>API Documentation</li> <li>Examples</li> <li>Best Practices</li> </ul> </li> </ul> <p>Happy coding with LabChain!</p>"}]}